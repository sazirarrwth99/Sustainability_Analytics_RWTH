{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preparation"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Important!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#This code works only on pandas 1.X\n",
    "!pip install pandas==1.5.3"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Module imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import dropbox\n",
    "import config\n",
    "import requests\n",
    "import os\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "import pandas as pd\n",
    "import geopandas as gpd\n",
    "import plotly.express as px\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.cluster import KMeans\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.metrics import silhouette_score, calinski_harabasz_score, davies_bouldin_score\n",
    "import scipy\n",
    "import sklearn\n",
    "from sklearn import metrics\n",
    "import pandas as pd\n",
    "from scipy.stats import pearsonr,f\n",
    "import warnings\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.metrics import r2_score"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#categories on a high level\n",
    "healthcare_categories = ['Pharmaceuticals', 'Health Care Equipment & Supplies', 'Biotechnology', 'Life Sciences Tools & Services', 'Health Care Providers & Services', 'Health Care Technology', 'Tobacco', 'Marine']\n",
    "consumer_categories = ['Food Products', 'Specialty Retail', 'Household Durables', 'Food & Staples Retailing', 'Beverages', 'Household Products', 'Textiles, Apparel & Luxury Goods', 'Multiline Retail', 'Personal Products', 'Hotels, Restaurants & Leisure', 'Building Products', 'Airlines', 'Containers & Packaging', 'Automobiles', 'Auto Components', 'Entertainment', 'Leisure Products', 'Transportation Infrastructure', 'Diversified Consumer Services', 'Distributors']\n",
    "tech_categories = ['Semiconductors & Semiconductor Equipment', 'IT Services', 'Software', 'Technology Hardware, Storage & Peripherals', 'Interactive Media & Services', 'Software', 'Technology Hardware, Storage & Peripherals', 'Health Care Technology', 'Machinery', 'Media', 'Aerospace & Defense', 'Communications Equipment', 'Electrical Equipment', 'Electronic Equipment, Instruments & Components', 'Wireless Telecommunication Services', 'Internet & Direct Marketing Retail']\n",
    "finance_categories = ['Banks', 'Capital Markets', 'Insurance', 'Consumer Finance', 'Thrifts & Mortgage Finance', 'Equity Real Estate Investment Trusts (REITs)', 'Commercial Services & Supplies', 'Diversified Telecommunication Services', 'Professional Services', 'Real Estate Management & Development', 'Industrial Conglomerates', 'Paper & Forest Products', 'Diversified Financial Services', 'Construction & Engineering', 'Trading Companies & Distributors']\n",
    "energy_categories = ['Oil, Gas & Consumable Fuels', 'Electric Utilities', 'Energy Equipment & Services', 'Independent Power and Renewable Electricity Producers', 'Gas Utilities', 'Metals & Mining', 'Multi-Utilities', 'Construction Materials', 'Road & Rail', 'Air Freight & Logistics', 'Water Utilities', 'Chemicals']\n",
    "\n",
    "#parameters for models\n",
    "FLOAT_LEARNING_RATE = 0.0001\n",
    "LAMBDA = 0.2\n",
    "P_LAGS = 2"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Visual setting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#set visible columns to max\n",
    "pd.set_option('display.max_columns', None)\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Methods"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Lets gather the data"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First we import the data form our local folder or from dropbox"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def download_excel_file(url, output_file):\n",
    "    \"\"\"\n",
    "    Download an Excel file from a URL and save it locally.\n",
    "    \"\"\"\n",
    "    # Modify the Dropbox URL to force download\n",
    "    download_url = url.replace(\"?dl=0\", \"?dl=1\")\n",
    "\n",
    "    # Send an HTTP GET request to download the file\n",
    "    response = requests.get(download_url)\n",
    "\n",
    "    # Check if the request was successful (HTTP status code 200)\n",
    "    if response.status_code == 200:\n",
    "        # Create the output folder if it does not exist\n",
    "        output_folder = os.path.dirname(output_file)\n",
    "        if output_folder and not os.path.exists(output_folder):\n",
    "            os.makedirs(output_folder)\n",
    "\n",
    "        # Save the file in the output folder\n",
    "        with open(output_file, \"wb\") as file:\n",
    "            file.write(response.content)\n",
    "    else:\n",
    "        print(\"Error: Unable to download the file\")\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now lets define the categories on the high level"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the data generation via yahoo please see the Deprecated folder\n",
    "!Warning! it is highly unstructured code, since it was not used for any insights"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets import some Data from refinitive. Since the data validity is not given with the beta version of the refinitive Python Api. Anyways, hence it may help some people, here is the function (to get data without the API-KEY). It is not called, but the data can be downloaded with the function call. It is important to have a open session of refinitive workspace running on the PC. More methods can be found in the Deprecated folder. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_how_to_get_data_from_refinitive():\n",
    "    \"\"\"\n",
    "    example function to show how to get data from refinitive. Just to help if needed, absolutely no guarantee that it still works\n",
    "    \"\"\"\n",
    "    \n",
    "    #wee need this module to get the data from refinitive (still, eikon is also possible, but does not work well)\n",
    "    import refinitiv.data as rd\n",
    "    #if you want to use eikon, import it like this\n",
    "    #import refinitiv.data.eikon as ek\n",
    "    \n",
    "    import Deprecated.refinitive_fields\n",
    "    #in the ressource_use_fields.py file, we lists with all the fields we can use\n",
    "    #each key represents the categorie\n",
    "    import Deprecated.refinitive_fields as refinitive_fields\n",
    "    #let us import all the reccources use fields of the ESG\n",
    "    list_ressource_use_fields = refinitive_fields.ressource_use_fields\n",
    "    \n",
    "    #first we have to open a session\n",
    "    rd.open_session()\n",
    "    #now we just makeour query\n",
    "    #universe is the ticker of the company(ies), fields are the fields we want to get, start and end are the start and end date of the data, interval is the interval of the data\n",
    "    #to use eikon\n",
    "    df = rd.get_history(\n",
    "        universe = ['MBGn.DE'],\n",
    "        fields = list_ressource_use_fields,\n",
    "        start = \"2000-01-01\",\n",
    "        end = \"2022-12-31\",\n",
    "        interval='yearly',\n",
    "    )\n",
    "    return df"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### LetÂ´s clean the Data"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Make the data readable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def refinite_to_python(file_path):\n",
    "    \"\"\"\n",
    "    Make a file from the refinitive screener readable for python and useable for the analysis.\n",
    "    input: file_path\n",
    "    output: cleaned dataframe\n",
    "    \"\"\"\n",
    "    \n",
    "    # Read data from excel file\n",
    "    df_all_comp_all = pd.read_excel(file_path, header=[0, 1])\n",
    "\n",
    "    # Combine multilevel columns into a single level\n",
    "    cols = df_all_comp_all.columns\n",
    "    df_all_comp_all.columns = ['_'.join(col).strip() for col in cols]\n",
    "\n",
    "    # Rename columns for better understanding\n",
    "    if \"Company Name_Unnamed: 1_level_1\" in df_all_comp_all.columns:\n",
    "        df_all_comp_all.rename(columns={\"Company Name_Unnamed: 1_level_1\": \"Company Name\"}, inplace=True)\n",
    "    if \"Identifier (RIC)_Unnamed: 0_level_1\" in df_all_comp_all.columns:\n",
    "        df_all_comp_all.rename(columns={\"Identifier (RIC)_Unnamed: 0_level_1\": \"RIC\"}, inplace=True)\n",
    "    if \"Country of Headquarters_Unnamed: 2_level_1\" in df_all_comp_all.columns:\n",
    "        df_all_comp_all.rename(columns={\"Country of Headquarters_Unnamed: 2_level_1\": \"Country of Headquarters\"}, inplace=True)\n",
    "    if \"NAICS Subsector Name_Unnamed: 5_level_1\" in df_all_comp_all.columns:\n",
    "        df_all_comp_all.rename(columns={\"NAICS Subsector Name_Unnamed: 5_level_1\": \"NAICS Subsector Name\"}, inplace=True)\n",
    "    if \"GICS Industry Name_GICS Industry Name\" in df_all_comp_all.columns:\n",
    "        df_all_comp_all.rename(columns={\"GICS Industry Name_GICS Industry Name\": \"GICS Industry Name\"}, inplace=True)\n",
    "\n",
    "    # Remove unwanted characters from column names\n",
    "    df_all_comp_all.columns = df_all_comp_all.columns.str.replace('\\nIn the last 10 FY_FY', ' ')\n",
    "    df_all_comp_all.columns = df_all_comp_all.columns.str.replace(\"\\nIn the last 15 Y_Y\",\" \")\n",
    "    df_all_comp_all.columns = df_all_comp_all.columns.str.replace('-', '')\n",
    "    \n",
    "\n",
    "    # Create a copy of the DataFrame\n",
    "    df_all_comp = df_all_comp_all.copy()\n",
    "    #return df_all_comp\n",
    "    #check if the YTD Total Return 11, 12, 13, 14 are in the dataframe\n",
    "    if \"YTD Total Return 11\" in df_all_comp.columns:  \n",
    "        df_all_comp.drop(columns=[\"YTD Total Return 11\"], inplace=True)\n",
    "    if \"YTD Total Return 12\" in df_all_comp.columns:\n",
    "        df_all_comp.drop(columns=[\"YTD Total Return 12\"], inplace=True)  \n",
    "    if \"YTD Total Return 13\" in df_all_comp.columns:\n",
    "        df_all_comp.drop(columns=[\"YTD Total Return 13\"], inplace=True)    \n",
    "    if \"YTD Total Return 14\" in df_all_comp.columns:\n",
    "        df_all_comp.drop(columns=[\"YTD Total Return 14\"], inplace=True)\n",
    "\n",
    "\n",
    "    # Drop rows with all NaN values and fill remaining NaNs with 0\n",
    "    df_all_comp.dropna(inplace=True, how=\"all\")\n",
    "    df_all_comp.fillna(0, inplace=True)\n",
    "\n",
    "\n",
    "    # Extract unique column prefixes\n",
    "    list_columns = [col for col in df_all_comp.columns if col[-1].isdigit()]\n",
    "    list_columns = [col[:-1] for col in list_columns]\n",
    "    list_columns = [col[:-1] if col[-1].isdigit() else col for col in list_columns]\n",
    "    list_columns = list(set(list_columns))\n",
    "    list_columns = [col.replace(\"\\n\", \"\") for col in list_columns]\n",
    "\n",
    "    # Initialize an empty list to store DataFrames\n",
    "    dfs = []\n",
    "\n",
    "    # Iterate over unique column prefixes\n",
    "    for colum in tqdm(list_columns):\n",
    "        esg_cols = [col for col in df_all_comp.columns if col.startswith(colum)]\n",
    "        df = df_all_comp[[\"Company Name\"] + esg_cols].copy()\n",
    "\n",
    "        # Melt the DataFrame to transform it into the desired format\n",
    "        melted_df = df.melt(\n",
    "            id_vars=[\"Company Name\"],\n",
    "            var_name=\"Period\",\n",
    "            value_name=colum\n",
    "        )\n",
    "        melted_df[\"Period\"] = melted_df[\"Period\"].apply(lambda x: 1 if x[-1].isdigit() == False else 10 if x[-2:-1] == \"10\" else int(x[-1]))\n",
    "\n",
    "        dfs.append(melted_df)\n",
    "\n",
    "    # Concatenate the melted DataFrames\n",
    "    melted_df_all = pd.concat(dfs, axis=1)\n",
    "\n",
    "    # Remove duplicate columns\n",
    "    melted_df_all = melted_df_all.loc[:, ~melted_df_all.columns.duplicated()]\n",
    "\n",
    "    # Sort the DataFrame by Company Name and Period\n",
    "    melted_df_all = melted_df_all.sort_values(by=[\"Company Name\", \"Period\"])\n",
    "\n",
    "    # Create a copy of the final DataFrame\n",
    "    df_data_ = melted_df_all.copy()\n",
    "    \n",
    "    #replace all 0 with nan, except for the period column\n",
    "    #it is important to diffirentiate between 0 and nan, for example for the correlation\n",
    "    df_data_ = df_data_.replace(0, np.nan)\n",
    "    df_data_ = df_data_.replace(\"0\", np.nan)\n",
    "    df_data_[\"Period\"] = df_data_[\"Period\"].fillna(0)\n",
    "\n",
    "    #add RIC, Country of Headquarters and NAICS Subsector Name to the dataframe\n",
    "    #fill nan of RIC, Country of Headquarters and NAICS Subsector Name with the last value that is not nan\n",
    "    if \"RIC\" in df_all_comp_all.columns:\n",
    "        df_data_[\"RIC\"] = df_all_comp_all[\"RIC\"]\n",
    "        df_data_[\"RIC\"] = df_data_.groupby(\"Company Name\")[\"RIC\"].ffill()   \n",
    "    if \"Country of Headquarters\" in df_all_comp_all.columns:\n",
    "        df_data_[\"Country of Headquarters\"] = df_all_comp_all[\"Country of Headquarters\"]\n",
    "        df_data_[\"Country of Headquarters\"] = df_data_.groupby(\"Company Name\")[\"Country of Headquarters\"].ffill()            \n",
    "    if \"NAICS Subsector Name\" in df_all_comp_all.columns:\n",
    "        df_data_[\"NAICS Subsector Name\"] = df_all_comp_all[\"NAICS Subsector Name\"]\n",
    "        df_data_[\"NAICS Subsector Name\"] = df_data_.groupby(\"Company Name\")[\"NAICS Subsector Name\"].ffill()   \n",
    "    if \"GICS Industry Name\" in df_all_comp_all.columns:\n",
    "        df_data_[\"GICS Industry Name\"] = df_all_comp_all[\"GICS Industry Name\"]\n",
    "        df_data_[\"GICS Industry Name\"] = df_data_.groupby(\"Company Name\")[\"GICS Industry Name\"].ffill()\n",
    "\n",
    "    #remove all the whitespaces in the end of a column name\n",
    "    df_data_.columns = df_data_.columns.str.rstrip()\n",
    "        \n",
    "    if \"ESG Score \" in df_data_.columns:\n",
    "        #make sure only one ESG Score column is in the dataframe\n",
    "        df_data_[\"ESG Score\"] = df_data_[\"ESG Score \"]\n",
    "\n",
    "    \n",
    "    return df_data_\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def refinite_to_python_EBITDA(filepath):\n",
    "    \"\"\" \n",
    "    This function reads the EBITDA data from Refinitiv and returns a DataFrame with the data.\n",
    "    Works as a workaround for the Bugs occuring in the normal function\n",
    "    \"\"\"\n",
    "\n",
    "    df_ebitda = pd.read_excel(filepath, header=[0, 1])\n",
    "    # Combine multilevel columns into a single level\n",
    "    cols = df_ebitda.columns\n",
    "    df_ebitda.columns = ['_'.join(col).strip() for col in cols]\n",
    "    # Remove unwanted characters from column names\n",
    "    df_ebitda.columns = df_ebitda.columns.str.replace('\\nIn the last 10 FY_FY', ' ')\n",
    "    df_ebitda.columns = df_ebitda.columns.str.replace(\"\\nIn the last 15 Y_Y\",\" \")\n",
    "    df_ebitda.columns = df_ebitda.columns.str.replace('-', '')\n",
    "\n",
    "    # Drop rows with all NaN values and fill remaining NaNs with 0\n",
    "    df_ebitda.dropna(inplace=True, how=\"all\")\n",
    "    df_ebitda.fillna(0, inplace=True)\n",
    "\n",
    "    # Rename columns for better understanding\n",
    "    if \"Company Name_Unnamed: 1_level_1\" in df_ebitda.columns:\n",
    "        df_ebitda.rename(columns={\"Company Name_Unnamed: 1_level_1\": \"Company Name\"}, inplace=True)\n",
    "    if \"Identifier (RIC)_Unnamed: 0_level_1\" in df_ebitda.columns:\n",
    "        df_ebitda.rename(columns={\"Identifier (RIC)_Unnamed: 0_level_1\": \"RIC\"}, inplace=True)\n",
    "    if \"Country of Headquarters_Unnamed: 2_level_1\" in df_ebitda.columns:\n",
    "        df_ebitda.rename(columns={\"Country of Headquarters_Unnamed: 2_level_1\": \"Country of Headquarters\"}, inplace=True)\n",
    "    if \"NAICS Subsector Name_Unnamed: 5_level_1\" in df_ebitda.columns:\n",
    "        df_ebitda.rename(columns={\"NAICS Subsector Name_Unnamed: 5_level_1\": \"NAICS Subsector Name\"}, inplace=True)\n",
    "    if \"GICS Industry Name_GICS Industry Name\" in df_ebitda.columns:\n",
    "        df_ebitda.rename(columns={\"GICS Industry Name_GICS Industry Name\": \"GICS Industry Name\"}, inplace=True)\n",
    "\n",
    "\n",
    "    # Extract unique column prefixes\n",
    "    list_columns = [col for col in df_ebitda.columns if col[-1].isdigit()]\n",
    "    list_columns = [col[:-1] for col in list_columns]\n",
    "    list_columns = [col[:-1] if col[-1].isdigit() else col for col in list_columns]\n",
    "    list_columns = list(set(list_columns))\n",
    "    list_columns = [col.replace(\"\\n\", \"\") for col in list_columns]\n",
    "\n",
    "    # Initialize an empty list to store DataFrames\n",
    "    dfs = []\n",
    "\n",
    "    # Iterate over unique column prefixes\n",
    "    for colum in tqdm(list_columns):\n",
    "        esg_cols = [col for col in df_ebitda.columns if col.startswith(colum)]\n",
    "        df = df_ebitda[[\"Company Name\"] + esg_cols].copy()\n",
    "\n",
    "        # Melt the DataFrame to transform it into the desired format\n",
    "        melted_df = df.melt(\n",
    "            id_vars=[\"Company Name\"],\n",
    "            var_name=\"Period\",\n",
    "            value_name=colum\n",
    "        )\n",
    "        melted_df[\"Period\"] = melted_df[\"Period\"].apply(lambda x: 1 if x[-1].isdigit() == False else 10 if x[-2:-1] == \"10\" else int(x[-1]))\n",
    "\n",
    "        dfs.append(melted_df)\n",
    "\n",
    "    # Concatenate the melted DataFrames\n",
    "    melted_df_all = pd.concat(dfs, axis=1)\n",
    "\n",
    "    # Remove duplicate columns\n",
    "    #melted_df_all = melted_df_all.loc[:, ~melted_df_all.columns.duplicated()]\n",
    "\n",
    "    melted_df_all.fillna(0, inplace=True)\n",
    "    df_new = pd.DataFrame()\n",
    "    for column_ in melted_df_all.columns:\n",
    "        #check if column is duplicate\n",
    "        if column_ == \"Company Name\":\n",
    "                #rename the column, cal\n",
    "                X = pd.DataFrame(melted_df_all[column_])\n",
    "                X.columns = [str(i) for i in range(X.shape[1])]\n",
    "                #calculate the numbers of 0 in every column\n",
    "                if 0 in X[\"0\"]:\n",
    "                    col_1 = X[\"0\"].value_counts().loc[0]\n",
    "                else:  \n",
    "                    col_1 = 0\n",
    "                if 0 in X[\"1\"].values:\n",
    "                    col_2 = X[\"1\"].value_counts().loc[0]\n",
    "                else:\n",
    "                    col_2 = 0\n",
    "                if  col_1 < col_2:\n",
    "                    df_new[column_] = X[\"0\"]\n",
    "                else:\n",
    "                    df_new[column_] = X[\"1\"]\n",
    "        else:\n",
    "                try:\n",
    "                    if melted_df_all[column_].shape[1]>1:\n",
    "                        print(column_)\n",
    "                        #rename the column, cal\n",
    "                        X = pd.DataFrame(melted_df_all[column_])\n",
    "                        X.columns = [str(i) for i in range(X.shape[1])]\n",
    "                        #calculate the numbers of 0 in every column\n",
    "                        col_1 = X[\"0\"].sum()\n",
    "                        col_2 = X[\"1\"].sum()\n",
    "                        if  col_1 > col_2:\n",
    "                            df_new[column_] = X[\"0\"]\n",
    "                        else:\n",
    "                            df_new[column_] = X[\"1\"]\n",
    "                        \n",
    "                except KeyboardInterrupt:\n",
    "                    raise\n",
    "                except Exception as e:\n",
    "                    print(column_)\n",
    "                    df_new[column_] = melted_df_all[column_]\n",
    "                    print(column_)\n",
    "                    print(e)\n",
    "                    pass\n",
    "\n",
    "    melted_df_all = df_new.copy()\n",
    "    # Sort the DataFrame by Company Name and Period\n",
    "    melted_df_all = melted_df_all.sort_values(by=[\"Company Name\", \"Period\"])\n",
    "\n",
    "    # Create a copy of the final DataFrame\n",
    "    df_data_ = melted_df_all.copy()\n",
    "\n",
    "    #replace all 0 with nan, except for the period column\n",
    "    #it is important to diffirentiate between 0 and nan, for example for the correlation\n",
    "    df_data_ = df_data_.replace(0, np.nan)\n",
    "    df_data_ = df_data_.replace(\"0\", np.nan)\n",
    "    df_data_[\"Period\"] = df_data_[\"Period\"].fillna(0)\n",
    "    df_EBITDA_10Y_raw = df_data_.copy()\n",
    "    return df_EBITDA_10Y_raw"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Clean the data and add some clusters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_data(df, esg_col = \"ESG Score\", return_col = \"Return\"):\n",
    "    \"\"\"\n",
    "    Clean and preprocess a dataframe for clustering based on \"ESG Score\" and \"Return\" columns.\n",
    "\n",
    "    Args:\n",
    "        df (pd.DataFrame): The input dataframe to clean.\n",
    "        esg_col (str): The column name for the ESG Score.\n",
    "        return_col (str): The column name for the Return.\n",
    "\n",
    "    Returns:\n",
    "        pd.DataFrame: The cleaned and preprocessed dataframe.\n",
    "    \"\"\"\n",
    "\n",
    "    # Make a copy of the input dataframe to avoid modifying it\n",
    "    df_data = df.copy()\n",
    "    \n",
    "    #get only numeric columns\n",
    "    numeric_columns = df_data.select_dtypes(['int64', 'float64']).columns\n",
    "\n",
    "    # Drop rows with NaN values in \"Return\" or \"ESG Score\" columns\n",
    "    df_data.dropna(subset=[return_col, esg_col], inplace=True)\n",
    "\n",
    "    # Remove outliers in the \"Return\" column (more than 2 std away from the mean)\n",
    "    return_mean = df_data[return_col].mean()\n",
    "    return_std = df_data[return_col].std()\n",
    "    df_data = df_data[(df_data[return_col] < return_mean + 2 * return_std) & (df_data[return_col] > return_mean - 2 * return_std)]\n",
    "    #for clustering\n",
    "    df_data_ = df_data.copy()\n",
    "    df_data_.fillna(0, inplace=True)\n",
    "    #make mean of company \n",
    "    df_data_ = df_data_.groupby(\"Company Name\").mean()\n",
    "    # Cluster the companies columns using KMeans\n",
    "    model = KMeans(n_clusters=5)\n",
    "    model.fit(df_data_[numeric_columns])\n",
    "    labels = model.predict(df_data_[numeric_columns])\n",
    "    df_data_[\"Cluster\"] = labels\n",
    "    #map back to the original dataframe based on the company name  \n",
    "    df_data[\"Cluster\"] = df_data[\"Company Name\"].map(df_data_[\"Cluster\"])\n",
    "    # Return the cleaned dataframe\n",
    "    \n",
    "    return df_data"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Add Transformation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def transform_columns(df,column):\n",
    "    \"\"\"\n",
    "    takes a dataframe and a column name and transforms the column into:\n",
    "    log\n",
    "    log_2\n",
    "    sqrt\n",
    "    sqrt_2\n",
    "    _2\n",
    "    _3\n",
    "    _4\n",
    "    \"\"\"\n",
    "    \n",
    "    df[column+\"_log\"] = np.log(df[column])\n",
    "    df[column+\"_log_2\"] = np.log(df[column]**2)\n",
    "    df[column+\"_sqrt\"] = np.sqrt(df[column])\n",
    "    df[column+\"_sqrt_2\"] = np.sqrt(df[column]**2)\n",
    "    df[column+\"_2\"] = df[column]**2\n",
    "    df[column+\"_3\"] = df[column]**3\n",
    "    df[column+\"_4\"] = df[column]**4\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Scale the Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "def scale_data(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"Scale the numerical columns of the DataFrame.\n",
    "\n",
    "    Args:\n",
    "        df (pd.DataFrame): The input DataFrame.\n",
    "\n",
    "    Returns:\n",
    "        pd.DataFrame: The scaled DataFrame.\n",
    "    \"\"\"\n",
    "    numerical_columns = df.select_dtypes(include=['float64', 'int64']).columns\n",
    "    if len(numerical_columns) == 0:\n",
    "        raise ValueError(\"No numerical columns in DataFrame\")\n",
    "    scaler = StandardScaler()\n",
    "    df_scaled = scaler.fit_transform(df[numerical_columns])\n",
    "    df_scaled = pd.DataFrame(df_scaled, columns=numerical_columns, index=df.index)\n",
    "    return pd.concat([df.drop(columns=numerical_columns), df_scaled], axis=1)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lets dive deep in the data"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MATH Formulas"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### LetÂ´s define our basic math functions \n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Gradient Descent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def initialize_coefficients(n_features, n_targets):\n",
    "    \"\"\"\n",
    "    Initializes the coefficients for VAR.\n",
    "    \n",
    "    Parameters:\n",
    "        n_features (int): Number of input features.\n",
    "        n_targets (int): Number of output targets.\n",
    "        \n",
    "    Returns:\n",
    "        numpy.ndarray: An (n_features x n_targets) matrix of initialized coefficients.\n",
    "    \"\"\"\n",
    "    return np.random.rand(n_features, n_targets)\n",
    "\n",
    "def calculate_loss(X, Y, coefficients):\n",
    "    \"\"\"\n",
    "    Calculates the mean squared error (MSE) for the current coefficients.\n",
    "    \n",
    "    Parameters:\n",
    "        X (numpy.ndarray): An (n_samples x n_features) array of input data.\n",
    "        Y (numpy.ndarray): An (n_samples x n_targets) array of output data.\n",
    "        coefficients (numpy.ndarray): An (n_features x n_targets) matrix of current coefficients.\n",
    "        \n",
    "    Returns:\n",
    "        float: The mean squared error (MSE) for the current coefficients.\n",
    "    \"\"\"\n",
    "    predicted = np.dot(X, coefficients)\n",
    "    errors = Y - predicted\n",
    "    return np.sum(errors**2) / (2 * X.shape[0])\n",
    "\n",
    "def calculate_lasso_loss(_X, _Y, coefficients, lambda_):\n",
    "    \"\"\"\n",
    "    Calculates the LASSO loss for the current coefficients.\n",
    "\n",
    "    Parameters:\n",
    "        X (numpy.ndarray): An (n_samples x n_features) array of input data.\n",
    "        Y (numpy.ndarray): An (n_samples x n_targets) array of output data.\n",
    "        coefficients (numpy.ndarray): An (n_features x n_targets) matrix of current coefficients.\n",
    "        lambda_ (float): The regularization parameter.\n",
    "\n",
    "    Returns:\n",
    "        float: The LASSO loss for the current coefficients.\n",
    "    \"\"\"\n",
    "    errors = _Y - np.dot(_X, coefficients)\n",
    "    mse = np.sum(errors**2) / (2 * _X.shape[0])\n",
    "    l1_norm = np.sum(np.abs(coefficients))\n",
    "    lasso_loss = mse + lambda_ * l1_norm\n",
    "    return lasso_loss\n",
    "\n",
    "\n",
    "def outer(a, b):\n",
    "    \"\"\"\n",
    "    Computes the outer product of two 1-dimensional arrays.\n",
    "\n",
    "    Parameters:\n",
    "        a (array-like): 1-dimensional array.\n",
    "        b (array-like): 1-dimensional array.\n",
    "\n",
    "    Returns:\n",
    "        2-dimensional array where the element at position (i, j) is the\n",
    "        product of the i-th element of `a` and the j-th element of `b`.\n",
    "    \"\"\"\n",
    "    outer_product = np.zeros((len(a), len(b)))  # initialize the result with zeros\n",
    "    for i in range(len(a)):\n",
    "        for j in range(len(b)):\n",
    "            outer_product[i, j] = a[i] * b[j]  # compute the product of the i-th element of a and the j-th element of b\n",
    "    return outer_product\n",
    "\n",
    "\n",
    "\n",
    "def calculate_gradient(X, Y, coefficients):\n",
    "    \"\"\"\n",
    "    Calculates the gradient of the loss function with respect to the coefficients.\n",
    "    \n",
    "    Parameters:\n",
    "        X (numpy.ndarray): An (n_samples x n_features) array of input data.\n",
    "        Y (numpy.ndarray): An (n_samples x n_targets) array of output data.\n",
    "        coefficients (numpy.ndarray): An (n_features x n_targets) matrix of current coefficients.\n",
    "        \n",
    "    Returns:\n",
    "        numpy.ndarray: An (n_features x n_targets) matrix of gradient values.\n",
    "    \"\"\"\n",
    "    n_samples = X.shape[0]\n",
    "    #without Lasso\n",
    "    #grad = np.dot(X.T, np.dot(X, coefficients) - Y) / n_samples\n",
    "    global LAMBDA\n",
    "    grad = (1/n_samples) * np.dot(X.T, np.dot(X, coefficients) - Y) + LAMBDA * np.sign(coefficients)  \n",
    "    return grad\n",
    "\n",
    "def update_coefficients(coefficients, gradients, lr):\n",
    "    \"\"\"\n",
    "    Updates the coefficients using the gradient and the learning rate.\n",
    "    \n",
    "    Parameters:\n",
    "        coefficients (numpy.ndarray): An (n_features x n_targets) matrix of current coefficients.\n",
    "        gradients (numpy.ndarray): An (n_features x n_targets) matrix of gradient values.\n",
    "        lr (float): Learning rate for the optimization algorithm.\n",
    "        \n",
    "    Returns:\n",
    "        numpy.ndarray: An (n_features x n_targets) matrix of updated coefficients.\n",
    "    \"\"\"\n",
    "    return coefficients - lr * gradients\n",
    "\n",
    "def gradient_descent_step(X__, Y__, coefficients__, lr):\n",
    "    \"\"\"\n",
    "    Performs a single step of gradient descent for VAR.\n",
    "    \n",
    "    Parameters:\n",
    "        X (numpy.ndarray): An (n_samples x n_features) array of input data.\n",
    "        Y (numpy.ndarray): An (n_samples x n_targets) array of output data.\n",
    "        coefficients (numpy.ndarray): An (n_features x n_targets) matrix of current coefficients.\n",
    "        lr (float): Learning rate for the optimization algorithm.\n",
    "        \n",
    "    Returns:\n",
    "        tuple: A tuple containing the updated coefficients and the loss for the current iteration.\n",
    "    \"\"\"\n",
    "    global LAMBDA\n",
    "    lambda_=1 + LAMBDA #variieren mit cosine similarity\n",
    "    gradients = calculate_gradient(X__, Y__, coefficients__)\n",
    "    updated_coefficients = update_coefficients(coefficients__, gradients, lr)\n",
    "    loss = calculate_lasso_loss(X__, Y__, coefficients__, lambda_)\n",
    "    return updated_coefficients, loss\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "F-Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_f_statistic(sample1, sample2):\n",
    "    \"\"\"\n",
    "    Calculates the F-statistic for two samples.\n",
    "\n",
    "    Parameters:\n",
    "        sample1 (numpy.ndarray): An array of values for the first sample.\n",
    "        sample2 (numpy.ndarray): An array of values for the second sample.\n",
    "\n",
    "    Returns:\n",
    "        float: The F-statistic for the two samples.\n",
    "    \"\"\"\n",
    "    var1 = np.var(sample1)\n",
    "    var2 = np.var(sample2)\n",
    "    return var1 / var2\n",
    "\n",
    "\n",
    "def calculate_critical_value(sample1, sample2, alpha):\n",
    "    \"\"\"\n",
    "    Calculates the critical value of the F-distribution for two samples.\n",
    "\n",
    "    Parameters:\n",
    "        sample1 (numpy.ndarray): An array of values for the first sample.\n",
    "        sample2 (numpy.ndarray): An array of values for the second sample.\n",
    "        alpha (float): The significance level.\n",
    "\n",
    "    Returns:\n",
    "        float: The critical value of the F-distribution for the two samples.\n",
    "    \"\"\"\n",
    "    df1 = len(sample1) - 1\n",
    "    df2 = len(sample2) - 1\n",
    "    return f.ppf(q=1 - alpha, dfn=df1, dfd=df2)\n",
    "\n",
    "\n",
    "def compare_f_statistic_to_critical_value(f_statistic, critical_value):\n",
    "    \"\"\"\n",
    "    Compares the F-statistic to the critical value of the F-distribution.\n",
    "\n",
    "    Parameters:\n",
    "        f_statistic (float): The F-statistic for the two samples.\n",
    "        critical_value (float): The critical value of the F-distribution for the two samples.\n",
    "    \"\"\"\n",
    "    if f_statistic > critical_value:\n",
    "        print(\"Reject the null hypothesis that the variances are equal\")\n",
    "        return False\n",
    "    else:\n",
    "        print(\"Accept the null hypothesis that the variances are equal\")\n",
    "        return True"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cosine similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cosine_similarity(v1, v2):\n",
    "    \"\"\"\n",
    "    calculates the cosine similarity between two vectors\n",
    "    input: v1, v2: numpy arrays\n",
    "    output: cosine similarity (float) \n",
    "    \"\"\"\n",
    "    return np.dot(v1, v2) / (np.linalg.norm(v1) * np.linalg.norm(v2))\n",
    "\n",
    "def cosine_similarity_matrix(A,B):\n",
    "    \"\"\"\n",
    "    calculates the cosine similarity similarity between the rows of A and B\n",
    "    \"\"\"\n",
    "    return np.array([cosine_similarity(A[i,:],B[i,:]) for i in range(A.shape[0])])\n",
    "\n",
    "#calculate the cosine similarity for every company to every other company\n",
    "#select only the numerical columns\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lag Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lag_matrix(X,p):\n",
    "    \"\"\"\n",
    "    lags matrix X by p\n",
    "    \"\"\"\n",
    "    X_lagged = np.zeros((X.shape[0]-p,X.shape[1]*p))\n",
    "    for i in range(p):\n",
    "        X_lagged[:,i*X.shape[1]:(i+1)*X.shape[1]] = X[p-i-1:-i-1,:]\n",
    "    return X_lagged"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "calculate optimal number cluster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_optimal_clusters(df_scaled: pd.DataFrame) -> int:\n",
    "    \"\"\"Find the optimal number of clusters using silhouette, calinski_harabasz,\n",
    "    and davies_bouldin scores.\n",
    "\n",
    "    Args:\n",
    "        df_scaled (pd.DataFrame): The scaled DataFrame.\n",
    "\n",
    "    Returns:\n",
    "        int: The optimal number of clusters.\n",
    "    \"\"\"\n",
    "    scores = {}\n",
    "    #fill missing values with 0\n",
    "    df_scaled = df_scaled.fillna(0)\n",
    "    #make sure only numeric columns are used\n",
    "    list_numeric_columns = df_scaled.select_dtypes(include=['float64', 'int64']).columns\n",
    "    df_scaled_num = df_scaled[list_numeric_columns]\n",
    "    \n",
    "    for n_clusters in tqdm(range(3, 100, 3)):\n",
    "        kmeans = KMeans(n_clusters=n_clusters, random_state=0).fit(df_scaled_num)\n",
    "        scores[n_clusters] = sum((\n",
    "            silhouette_score(df_scaled_num, kmeans.labels_),\n",
    "            calinski_harabasz_score(df_scaled_num, kmeans.labels_),\n",
    "            davies_bouldin_score(df_scaled_num, kmeans.labels_)\n",
    "        ))\n",
    "    return max(scores, key=scores.get)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cluster the Companies (KMeans)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cluster_companies(df_data: pd.DataFrame, min_size: int = 15,\n",
    "                      cluster_size: int = 0\n",
    "                      ) -> pd.DataFrame:\n",
    "    \"\"\"Cluster the companies based on all the data into the optimal number of\n",
    "    clusters, but every cluster has at least `min_size` companies.\n",
    "\n",
    "    Args:\n",
    "        df_data (pd.DataFrame): The input DataFrame.\n",
    "        min_size (int, optional): The minimum number of companies per cluster.\n",
    "            Defaults to 15.\n",
    "        cluster_size (int, optional): The number of clusters to use. If 0, the\n",
    "            \n",
    "\n",
    "    Returns:\n",
    "        pd.DataFrame: The input DataFrame with an additional column 'cluster'\n",
    "        indicating the cluster number for each company.\n",
    "    \"\"\"\n",
    "    # Scale the data\n",
    "    df_scaled = scale_data(df_data)\n",
    "\n",
    "    # Find the optimal number of clusters if not provided\n",
    "    if cluster_size == 0:\n",
    "        optimal_clusters = find_optimal_clusters(df_scaled)\n",
    "    else:\n",
    "        optimal_clusters = cluster_size\n",
    "\n",
    "    # Perform the clustering\n",
    "    kmeans = KMeans(n_clusters=optimal_clusters, random_state=0).fit(df_scaled)\n",
    "    df_data['cluster'] = kmeans.labels_\n",
    "\n",
    "    # Filter out clusters with less than `min_size` companies\n",
    "    df_data['cluster_size'] = df_data.groupby('cluster').transform('count')['id']\n",
    "    df_data = df_data[df_data['cluster_size'] >= min_size]\n",
    "    return df_data\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Perform correlation analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def group_corr(df_data, column_corr_a, column_corr_b, column_grop=\"Company Name\"):\n",
    "    \"\"\"\n",
    "    This function calculates the correlation between two columns, grouped by a third column,\n",
    "    and then visualizes the distribution of the correlations using a distribution plot.\n",
    "    \n",
    "    Args:\n",
    "        df_data (pd.DataFrame): The input dataframe containing the data.\n",
    "        column_corr_a (str): The first column to calculate the correlation.\n",
    "        column_corr_b (str): The second column to calculate the correlation.\n",
    "        column_grop (str, optional): The column to group by. Defaults to \"Company Name\".\n",
    "\n",
    "    Returns:\n",
    "        df_corr (pd.DataFrame): A dataframe containing the grouped correlation values.\n",
    "    \"\"\"\n",
    "    \n",
    "    df_corr = df_data.groupby(column_grop).apply(lambda x: x[[column_corr_a, column_corr_b]].corr().iloc[0, 1])\n",
    "    df_corr = df_corr.reset_index()\n",
    "    df_corr.columns = [column_grop, \"corr\"]\n",
    "\n",
    "    # Plot the distribution of correlations\n",
    "    sns.distplot(df_corr[\"corr\"])\n",
    "    plt.title(f\"Distribution of correlations between {column_corr_a} and {column_corr_b}\")\n",
    "\n",
    "    # Add mean and median lines\n",
    "    plt.axvline(df_corr[\"corr\"].mean(), color=\"red\", label=\"mean\")\n",
    "    plt.axvline(df_corr[\"corr\"].median(), color=\"green\", label=\"median\")\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "\n",
    "    return df_corr\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lagged_corr(df_data, column_corr_a, column_corr_b, column_grop=\"Company Name\", p=1, time_column=\"Period\"):\n",
    "    \"\"\"\n",
    "    Calculates lagged correlation between two columns, grouped by a third column\n",
    "    e.g. if group is company name, it calculates the correlation for column_corr_a to the column_corr_b of last period of the company.\n",
    "    For the oldest period of the company the correlation can't be determined, so it is set to nan.\n",
    "    \n",
    "    Args:\n",
    "        df_data (pd.DataFrame): Input data frame with the data\n",
    "        column_corr_a (str): First column for correlation calculation\n",
    "        column_corr_b (str): Second column for correlation calculation\n",
    "        column_grop (str, optional): Column to group data by. Defaults to \"Company Name\".\n",
    "        p (int, optional): Number of periods to lag. Defaults to 1.\n",
    "        time_column (str, optional): Column containing time periods. Defaults to \"Period\".\n",
    "    \n",
    "    Returns:\n",
    "        pd.DataFrame: Data frame with calculated lagged correlations\n",
    "    \"\"\"\n",
    "\n",
    "    # Create a copy of the input dataframe to avoid modifying the original dataframe\n",
    "    df = df_data.copy()\n",
    "\n",
    "    # Shift column_corr_b by p periods\n",
    "    df['shifted_column_corr_b'] = df.groupby(column_grop)[column_corr_b].shift(p)\n",
    "\n",
    "    # Calculate correlation for each group\n",
    "    df_corr = df.groupby(column_grop).apply(lambda x: x[[column_corr_a, 'shifted_column_corr_b']].corr().iloc[0, 1])\n",
    "\n",
    "    # Create a new dataframe with the correlation results\n",
    "    df_result = pd.DataFrame(df_corr).reset_index()\n",
    "    df_result.columns = [column_grop, 'lagged_correlation']\n",
    "\n",
    "    return df_result\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### LetÂ´s define our main models\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "VAR (Vektor Auto Regression)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def VAR_model(X, p):\n",
    "    \"\"\"\n",
    "    This function takes a matrix X and a number of lags p and performs a VAR(p) model on the data.\n",
    "    It returns the mean squared error and the R2 score, the matrix anf the coefficients.\n",
    "    \n",
    "    \"\"\"\n",
    "    n_samples, n_features = X.shape\n",
    "\n",
    "    X_lagged = np.zeros((n_samples - p, p * n_features))\n",
    "    for i in range(p):\n",
    "        X_lagged[:, i*n_features:(i+1)*n_features] = X[p-i-1:-i-1, :]\n",
    "\n",
    "    # Split the data into training and testing sets\n",
    "    train_size = int(0.8 * n_samples)\n",
    "    X_train = X_lagged[:train_size, :]\n",
    "    Y_train = X[p:train_size+p, :]\n",
    "    X_test = X_lagged[train_size-p:-p, :]\n",
    "    Y_test = X[train_size+p:, :]\n",
    "    # Compute the coefficients using the training set\n",
    "    coeffs = np.linalg.inv(X_train.T @ X_train) @ X_train.T @ Y_train\n",
    "\n",
    "    # make predictions for the test set\n",
    "    Y_pred = X_test @ coeffs\n",
    "\n",
    "    # calculate the mean squared error\n",
    "    mse = np.mean((Y_test - Y_pred)**2)\n",
    "    #calculate the R2 score\n",
    "    r2 = 1 - np.sum((Y_test - Y_pred)**2) / np.sum((Y_test - np.mean(Y_test))**2)\n",
    "\n",
    "    \n",
    "    #return the mean squared error and the R2 score, the matrix anf the coefficients\n",
    "    return mse, r2, X_lagged, coeffs"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Industry beta on VAR Model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def industry_beta(X_industry, y, coeffs_industry):\n",
    "    # Add a column of ones to the left of X_industry for the intercept\n",
    "\n",
    "    # Create a new matrix W where W = X @ coeffs_industry_with_intercept\n",
    "    W = X_industry @ coeffs_industry\n",
    "    #add a column of ones to the left of W\n",
    "    W = np.hstack((np.ones((W.shape[0], 1)), W))\n",
    "    \n",
    "\n",
    "\n",
    "    # Use the normal equation to find the optimal beta and intercept\n",
    "    # (W.T @ W)^(-1) @ W.T @ y\n",
    "    optimal_beta_intercept = np.linalg.inv(W.T @ W) @ W.T @ y\n",
    "\n",
    "    return float(optimal_beta_intercept[1]), float(optimal_beta_intercept[0])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Granger Test"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Granger test based on VAR (simple)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_granger_simple(df_data: pd.DataFrame, p: int, column_A: str, column_B: str) -> tuple:\n",
    "    \"\"\"\n",
    "    Check for Granger causality between two variables, given a pandas DataFrame with columns 'Company Name', 'Period',\n",
    "    'Return', and 'ESG Score'.\n",
    "\n",
    "    Parameters:\n",
    "    df_data (pd.DataFrame): A pandas DataFrame with columns 'Company Name', 'Period', 'Return', and 'ESG Score'.\n",
    "    p (int): The number of lags to include in the VAR models.\n",
    "    column_A (str): The name of the first column to test for Granger causality. Caused by column_B.\n",
    "    column_B (str): The name of the second column to test for Granger causality. Causes column_A.\n",
    "\n",
    "    Returns:\n",
    "    A tuple containing the residuals, y, and coefficients for each of the three VAR models, as well as the R-squared\n",
    "    values, F-statistics, and critical values for the Granger causality tests.\n",
    "    \"\"\"\n",
    "\n",
    "    # Create a copy of the input dataframe to avoid modifying the original\n",
    "    df_data_old = df_data.copy()\n",
    "\n",
    "    # Initialize an empty dataframe with the same columns as the input dataframe\n",
    "    df_data = pd.DataFrame(columns=df_data.columns)\n",
    "\n",
    "    # Iterate through all unique company names in the input dataframe\n",
    "    for company in df_data_old[\"Company Name\"].unique():\n",
    "        X_company = df_data_old[df_data_old[\"Company Name\"] == company]\n",
    "\n",
    "        # Only process companies with more than 5 data points\n",
    "        if X_company.shape[0] > 5:\n",
    "            # Add 'p' rows of zeros to the beginning of each company's data\n",
    "            X_c = pd.DataFrame(columns=df_data.columns)\n",
    "            X_c = X_c.append([X_company.iloc[0]] * p, ignore_index=True)\n",
    "            X_c = X_c * 0\n",
    "            X_c[\"Period\"] = list(range(-p, 0))\n",
    "            X_c = X_c.append(X_company, ignore_index=True)\n",
    "            df_data = df_data.append(X_c, ignore_index=True)\n",
    "\n",
    "    # Initialize the VAR models using different predictors\n",
    "    # Model 1: column_A\n",
    "    # Model 2: column_A, random noise\n",
    "    # Model 3: column_A, column_B\n",
    "    X1 = df_data[[column_A]].copy()\n",
    "    X2 = df_data[[column_A]].copy()\n",
    "    X2[\"random\"] = np.random.rand(len(X2))\n",
    "    X3 = df_data[[column_A, column_B]].copy()\n",
    "\n",
    "    # Convert dataframes to numpy arrays\n",
    "    X1, X2, X3 = X1.values, X2.values, X3.values\n",
    "\n",
    "    # Fit the VAR models with 'p' lags\n",
    "    mse1, r21, X_lagged1, coeffs_1 = VAR_model(X1, p)\n",
    "    mse2, r22, X_lagged2, coeffs_2 = VAR_model(X2, p)\n",
    "    mse3, r23, X_lagged3, coeffs_3 = VAR_model(X3, p)\n",
    "\n",
    "    # Calculate the residuals for each model\n",
    "    y = df_data[column_A].values[p:]\n",
    "    residuals_1 = y - (X_lagged1 @ coeffs_1)[:, 0]\n",
    "    residuals_2 = y - (X_lagged2 @ coeffs_2)[:, 0]\n",
    "    residuals_3 = y - (X_lagged3 @ coeffs_3)[:, 0]\n",
    "\n",
    "    # Define a function to calculate R-squared from residuals\n",
    "    def r2_from_residuals(y,residuals):\n",
    "        \"\"\"\n",
    "        Calculate the R-squared value from the residuals of a model.\n",
    "\n",
    "        Parameters:\n",
    "        y (numpy.ndarray): The dependent variable.\n",
    "        residuals (numpy.ndarray): The residuals of the model.\n",
    "\n",
    "        Returns:\n",
    "        The R-squared value.\n",
    "        \"\"\"\n",
    "        SST = sum((y - np.mean(y)) ** 2)\n",
    "        SSR = sum(residuals ** 2)\n",
    "        R2 = 1 - (SSR / SST)\n",
    "        return R2\n",
    "\n",
    "    # Print R-squared values for each model\n",
    "    print(f\"R-squared (Model 1): {r2_from_residuals(y, residuals_1)}\")\n",
    "    print(f\"R-squared (Model 2): {r2_from_residuals(y, residuals_2)}\")\n",
    "    print(f\"R-squared (Model 3): {r2_from_residuals(y, residuals_3)}\")\n",
    "\n",
    "    # Perform F-tests to compare residuals between models\n",
    "    F12 = calculate_f_statistic(residuals_1, residuals_2)\n",
    "    F13 = calculate_f_statistic(residuals_1, residuals_3)\n",
    "    print(f\"F-statistic (Model 1 vs Model 2): {F12}\")\n",
    "    print(f\"F-statistic (Model 1 vs Model 3): {F13}\")\n",
    "\n",
    "    critical_value12 = calculate_critical_value(residuals_1, residuals_2, alpha=0.05)\n",
    "    critical_value13 = calculate_critical_value(residuals_1, residuals_3, alpha=0.05)\n",
    "    print(f\"Critical value (Model 1 vs Model 2): {critical_value12}\")\n",
    "    print(f\"Critical value (Model 1 vs Model 3): {critical_value13}\")\n",
    "\n",
    "    # Compare F-statistics to critical values\n",
    "    print(\"Test whether random noise improves Model 1:\")\n",
    "    compare_f_statistic_to_critical_value(F12, critical_value12)\n",
    "    print(\"Test whether column_B improves Model 1:\")\n",
    "    compare_f_statistic_to_critical_value(F13, critical_value13)\n",
    "\n",
    "    # Return the residuals, y, and coefficients for each model\n",
    "    return (residuals_1, residuals_2, residuals_3, y,\n",
    "            coeffs_1, coeffs_2, coeffs_3)\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Granger with industry beta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_granger_industry_beta(df_data: pd.DataFrame, p: int, column_A: str, column_B: str, column_industry: str, industry_name: str) -> tuple:\n",
    "    \"\"\"\n",
    "    Check for Granger causality between two variables, given a pandas DataFrame with columns 'Company Name', 'Period',\n",
    "    'Return', 'ESG Score', and the specified 'column_industry'.\n",
    "\n",
    "    Parameters:\n",
    "    df_data (pd.DataFrame): A pandas DataFrame with columns 'Company Name', 'Period', 'Return', 'ESG Score', and the specified 'column_industry'.\n",
    "    p (int): The number of lags to include in the VAR models.\n",
    "    column_A (str): The name of the first column to test for Granger causality.\n",
    "    column_B (str): The name of the second column to test for Granger causality.\n",
    "    column_industry (str): The name of the column containing industry information.\n",
    "    industry_name (str): The name of the industry to filter by when performing the Granger causality test.\n",
    "\n",
    "    Returns:\n",
    "    A tuple containing the residuals for each of the three VAR models applied to the specified industry, the dependent variable (y),\n",
    "    and the coefficients for each of the three VAR models.\n",
    "    \"\"\"\n",
    "\n",
    "    # Create a copy of the input dataframe to avoid modifying the original\n",
    "    df_data_old = df_data.copy()\n",
    "\n",
    "    # Initialize an empty dataframe with the same columns as the input dataframe\n",
    "    df_data = pd.DataFrame(columns=df_data.columns)\n",
    "\n",
    "    # Iterate through all unique company names in the input dataframe\n",
    "    for company in df_data_old[\"Company Name\"].unique():\n",
    "        X_company = df_data_old[df_data_old[\"Company Name\"] == company]\n",
    "\n",
    "        # Only process companies with more than 5 data points\n",
    "        if X_company.shape[0] > 5:\n",
    "            # Add 'p' rows of zeros to the beginning of each company's data\n",
    "            X_c = pd.DataFrame(columns=df_data.columns)\n",
    "            X_c = X_c.append([X_company.iloc[0]] * p, ignore_index=True)\n",
    "            X_c = X_c * 0\n",
    "            X_c[\"Period\"] = list(range(-p, 0))\n",
    "            X_c = X_c.append(X_company, ignore_index=True)\n",
    "            df_data = df_data.append(X_c, ignore_index=True)\n",
    "\n",
    "    # Initialize the VAR models using different predictors\n",
    "    # Model 1: column_A\n",
    "    # Model 2: column_A, random noise\n",
    "    # Model 3: column_A, column_B\n",
    "    X1 = df_data[[column_A]].copy()\n",
    "    X2 = df_data[[column_A]].copy()\n",
    "    X2[\"random\"] = np.random.rand(len(X2))\n",
    "    X3 = df_data[[column_A, column_B]].copy()\n",
    "\n",
    "    # Convert dataframes to numpy arrays\n",
    "    X1, X2, X3 = X1.values, X2.values, X3.values\n",
    "\n",
    "    # Fit the VAR models with 'p' lags\n",
    "    mse1, r21, X_lagged1, coeffs_1 = VAR_model(X1, p)\n",
    "    mse2, r22, X_lagged2, coeffs_2 = VAR_model(X2, p)\n",
    "    mse3, r23, X_lagged3, coeffs_3 = VAR_model(X3, p)\n",
    "    \n",
    "\n",
    "    #select only the industry rows from the data\n",
    "    df_data_industry = df_data[df_data[column_industry] == industry_name]\n",
    "    X1_industry = df_data_industry[[column_A]].copy()\n",
    "    X2_industry = df_data_industry[[column_A]].copy()\n",
    "    X2_industry[\"random\"] = np.random.rand(len(X2_industry))\n",
    "    X3_industry = df_data_industry[[column_A, column_B]].copy()\n",
    "    \n",
    "    # Convert dataframes to numpy arrays\n",
    "    X1_industry, X2_industry, X3_industry = X1_industry.values, X2_industry.values, X3_industry.values\n",
    "\n",
    "    # Fit the VAR models with 'p' lags\n",
    "    mse1_industry, r21_industry, X_lagged1_industry,_= VAR_model(X1_industry, p)\n",
    "    mse2_industry, r22_industry, X_lagged2_industry, _ = VAR_model(X2_industry, p)\n",
    "    mse3_industry, r23_industry, X_lagged3_industry, _ = VAR_model(X3_industry, p)\n",
    "    \n",
    "    # Calculate the residuals for each model\n",
    "    y = df_data_industry[column_A].values[p:]\n",
    "    \n",
    "    #add the industry beta\n",
    "    \n",
    "    industry_beta_1,industry_intercept_1 = industry_beta(X_lagged1_industry, y, coeffs_1)\n",
    "    industry_beta_2,industry_intercept_2 = industry_beta(X_lagged2_industry, y, coeffs_2)\n",
    "    industry_beta_3,industry_intercept_3 = industry_beta(X_lagged3_industry, y, coeffs_3)\n",
    "    coeffs_1_industry = coeffs_1*industry_beta_1 \n",
    "    coeffs_2_industry = coeffs_2*industry_beta_2\n",
    "    coeffs_3_industry = coeffs_3*industry_beta_3\n",
    "\n",
    "    \n",
    "    \n",
    "    residuals_1 = y - ((X_lagged1_industry @ coeffs_1_industry)[:, 0] + industry_intercept_1)\n",
    "    residuals_2 = y - ((X_lagged2_industry @ coeffs_2_industry)[:, 0] + industry_intercept_2)\n",
    "    residuals_3 = y - ((X_lagged3_industry @ coeffs_3_industry)[:, 0] + industry_intercept_3)\n",
    "\n",
    "    # Define a function to calculate R-squared from residuals\n",
    "    def r2_from_residuals(y,residuals):\n",
    "        \"\"\"\n",
    "        Calculate the R-squared value from the residuals of a model.\n",
    "\n",
    "        Parameters:\n",
    "        y (numpy.ndarray): The dependent variable.\n",
    "        residuals (numpy.ndarray): The residuals of the model.\n",
    "\n",
    "        Returns:\n",
    "        The R-squared value.\n",
    "        \"\"\"\n",
    "        SST = sum((y - np.mean(y)) ** 2)\n",
    "        SSR = sum(residuals ** 2)\n",
    "        R2 = 1 - (SSR / SST)\n",
    "        return R2\n",
    "\n",
    "    # Print R-squared values for each model\n",
    "    print(f\"R-squared (Model 1): {r2_from_residuals(y, residuals_1)}\")\n",
    "    print(f\"R-squared (Model 2): {r2_from_residuals(y, residuals_2)}\")\n",
    "    print(f\"R-squared (Model 3): {r2_from_residuals(y, residuals_3)}\")\n",
    "\n",
    "    # Perform F-tests to compare residuals between models\n",
    "    F12 = calculate_f_statistic(residuals_1, residuals_2)\n",
    "    F13 = calculate_f_statistic(residuals_1, residuals_3)\n",
    "    print(f\"F-statistic (Model 1 vs Model 2): {F12}\")\n",
    "    print(f\"F-statistic (Model 1 vs Model 3): {F13}\")\n",
    "\n",
    "    critical_value12 = calculate_critical_value(residuals_1, residuals_2, alpha=0.05)\n",
    "    critical_value13 = calculate_critical_value(residuals_1, residuals_3, alpha=0.05)\n",
    "    print(f\"Critical value (Model 1 vs Model 2): {critical_value12}\")\n",
    "    print(f\"Critical value (Model 1 vs Model 3): {critical_value13}\")\n",
    "\n",
    "    # Compare F-statistics to critical values\n",
    "    compare_f_statistic_to_critical_value(F12, critical_value12)\n",
    "    compare_f_statistic_to_critical_value(F13, critical_value13)\n",
    "\n",
    "    # Return the residuals, y, and coefficients for each model\n",
    "    return (residuals_1, residuals_2, residuals_3, y,\n",
    "            coeffs_1_industry, coeffs_2_industry, coeffs_3_industry,\n",
    "            industry_intercept_1, industry_intercept_2, industry_intercept_3)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Granger test based on Multiple Linear Regression (Vanilla /Lasso /Ridge)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def granger_mlr(data, target, predictors, p, companys, alpha_lasso=0.05):\n",
    "    \"\"\"\n",
    "    This function performs Granger Causality tests using multivariate linear regression (MLR) models.\n",
    "    It fits three models:\n",
    "    1. Lasso Regression with the target and the lagged target as predictors\n",
    "    2. Lasso Regression with the target, the lagged target, and the lagged predictors as predictors\n",
    "    \n",
    "    Args:\n",
    "        data (DataFrame): The dataset containing the features and target.\n",
    "        target (str): The target variable.\n",
    "        predictors (list): The list of predictor variables.\n",
    "        p (int): The number of lags to include in the models.\n",
    "        companys (list): The list of companies to use for training, the rest will be used for testing.\n",
    "        alpha_lasso (float): The Lasso regularization parameter. Default is 0.05.\n",
    "        \n",
    "    Returns:\n",
    "        r2_residuals_1 (float): R-squared of the residuals of the first model.\n",
    "        r2_residuals_2 (float): R-squared of the residuals of the second model.\n",
    "        test_true (bool): The result of the F-test comparing the residuals of the two models.\n",
    "        data_shape (tuple): The shape of the processed data.\n",
    "    \"\"\"\n",
    "    # Preprocess data\n",
    "    data = data.copy()\n",
    "    data.dropna(inplace=True)\n",
    "    data = data[np.isfinite(data[target])]\n",
    "    data = data[np.isfinite(data[predictors])]\n",
    "\n",
    "    # Split data into training and testing sets\n",
    "    list_X_col = [target] + predictors + [\"Company Name\"]\n",
    "    X_train = data[~data[\"Company Name\"].isin(companys)][list_X_col].copy()\n",
    "    X_test = data[data[\"Company Name\"].isin(companys)][list_X_col].copy()\n",
    "    Y_train = data[~data[\"Company Name\"].isin(companys)][target].copy()\n",
    "    Y_test = data[data[\"Company Name\"].isin(companys)][target].copy()\n",
    "\n",
    "    # Add lagged predictors and target variables\n",
    "    for i in range(1, p + 1):\n",
    "        lag = -i\n",
    "        for pred in predictors:\n",
    "            col_name = f\"{pred}_shifted_{lag}\"\n",
    "            X_train[col_name] = X_train.groupby(\"Company Name\")[pred].shift(lag)\n",
    "            X_test[col_name] = X_test.groupby(\"Company Name\")[pred].shift(lag)\n",
    "        target_col_name = f\"{target}_shifted{lag}\"\n",
    "        X_train[target_col_name] = X_train.groupby(\"Company Name\")[target].shift(lag)\n",
    "        X_test[target_col_name] = X_test.groupby(\"Company Name\")[target].shift(lag)\n",
    "\n",
    "    # Remove the last p rows for every company and keep only shifted columns\n",
    "    X_train.dropna(inplace=True)\n",
    "    X_test.dropna(inplace=True)\n",
    "    Y_train = Y_train[X_train.index]\n",
    "    Y_test = Y_test[X_test.index]\n",
    "    X_train = X_train[[col for col in X_train.columns if \"shifted\" in col]]\n",
    "    X_test = X_test[[col for col in X_test.columns if \"shifted\" in col]]\n",
    "\n",
    "    # Train first model on the train data\n",
    "    model_1 = sklearn.linear_model.Lasso(alpha=alpha_lasso)\n",
    "    target_columns = [col for col in X_train.columns if str(target) in col]\n",
    "    model_1.fit(X_train[target_columns], Y_train)\n",
    "\n",
    "    # Train second model on the train data\n",
    "    model_2 = sklearn.linear_model.Lasso(alpha=alpha_lasso)\n",
    "    model_2.fit(X_train, Y_train)\n",
    "\n",
    "    # Calculate residuals\n",
    "    residuals_1 = Y_test - model_1.predict(X_test[target_columns])\n",
    "    residuals_2 = Y_test - model_2.predict(X_test)\n",
    "\n",
    "    # Calculate R-squared from residuals\n",
    "    def r2_from_residuals(y, residuals):\n",
    "        SST = sum((y - np.mean(y)) ** 2)  # Sum of Squares Total\n",
    "        SSR = sum(residuals ** 2)  # Sum of Squares Residuals\n",
    "        R2 = 1 - (SSR/SST)  # R-squared\n",
    "        return R2\n",
    "\n",
    "    r2_residuals_1 = r2_from_residuals(Y_test, residuals_1)\n",
    "    r2_residuals_2 = r2_from_residuals(Y_test, residuals_2)\n",
    "\n",
    "    # Perform F Test to check if the residuals are different\n",
    "    F = metrics.f_regression(residuals_1.reshape(-1, 1), residuals_2)[0][0]\n",
    "    critical_value = scipy.stats.f.ppf(1 - 0.05, len(residuals_1) - 1, len(residuals_2) - 1)\n",
    "    test_true = F > critical_value\n",
    "\n",
    "    # Print results\n",
    "    print(f\"F-statistic: {F}\")\n",
    "    print(f\"Critical value: {critical_value}\")\n",
    "    print(f\"Test result: {test_true}\")\n",
    "    print(\"R2 of the models\")\n",
    "    print(f\"R2 of the first model: {model_1.score(X_test[target_columns], Y_test)}\")\n",
    "    print(f\"R2 of the second model: {model_2.score(X_test, Y_test)}\")\n",
    "    print(\"R2 of the residuals\")\n",
    "\n",
    "    # Return R-squared of the residuals and the shape of the processed data\n",
    "    return r2_residuals_1, r2_residuals_2, test_true, data.shape\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Granger test with Gradient Descent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_gradient_descent_step_by_step(df_dataframe, companies, coef_matrix, learning_rate_c, feature_names, target_name):\n",
    "    \"\"\"\n",
    "    Makes a descent step by step.\n",
    "    Uses all rows besides the ones with the company names in the list companies.\n",
    "    \"\"\"\n",
    "    df_use = df_dataframe.copy()\n",
    "    df_use = df_use[~df_use[\"Company Name\"].isin(companies)]\n",
    "    df_comp = df_dataframe[df_dataframe[\"Company Name\"].isin(companies)]\n",
    "\n",
    "    # Select only the numerical columns\n",
    "    columns_num = df_use.select_dtypes(include=['float64', 'int64']).columns\n",
    "    df_use.dropna(subset=columns_num, inplace=True)\n",
    "    df_scaled = df_use[columns_num].copy()\n",
    "    df_comp = df_comp[columns_num].copy()\n",
    "\n",
    "    # Calculate the cosine similarity for every row\n",
    "    df_use[\"cosine_similarity\"] = df_scaled.apply(\n",
    "        lambda x: cosine_similarity_matrix(x.values.reshape(1, -1), df_comp.values), axis=1)\n",
    "\n",
    "    # Add the cosine similarity to the dataframe\n",
    "    df_use[\"cosine_similarity\"] = df_use[\"cosine_similarity\"].apply(lambda x: x[0])\n",
    "\n",
    "    # Sort the companies by the cosine similarity\n",
    "    dict_company_cosine_similarity = {}\n",
    "    for company in df_use[\"Company Name\"].unique():\n",
    "        dict_company_cosine_similarity[company] = df_use[df_use[\"Company Name\"] == company][\"cosine_similarity\"].mean()\n",
    "\n",
    "    companies_sorted = sorted(dict_company_cosine_similarity, key=dict_company_cosine_similarity.get, reverse=True)\n",
    "\n",
    "    # Perform the gradient descent on every row, use the formula learning rate * cosine similarity as the learning rate\n",
    "    df_use.dropna(subset=feature_names, inplace=True)\n",
    "    list_loss = []\n",
    "    for company_nam in tqdm(companies_sorted):\n",
    "        X = df_use[df_use[\"Company Name\"] == company_nam][feature_names].values\n",
    "        Y = df_use[df_use[\"Company Name\"] == company_nam][target_name].values\n",
    "        \n",
    "        #add p rows of zeros to the X matrix\n",
    "        global P_LAGS\n",
    "        for i in range(0,P_LAGS):\n",
    "            X = np.row_stack((np.zeros(X.shape[1]), X))\n",
    "        \n",
    "\n",
    "        if X.shape[0] < 5:\n",
    "            continue\n",
    "\n",
    "        X_lagged = lag_matrix(X, 2)\n",
    "        learning_rate = dict_company_cosine_similarity[company_nam] * learning_rate_c\n",
    "        coef_matrix_1 = coef_matrix\n",
    "        coef_matrix, _ = gradient_descent_step(X_lagged, Y, coef_matrix_1, learning_rate)\n",
    "        list_loss.append(_)\n",
    "\n",
    "        if np.isinf(coef_matrix).any():\n",
    "            print(\"INF\")\n",
    "            return coef_matrix, X_lagged, Y, coef_matrix, learning_rate, coef_matrix_1\n",
    "\n",
    "        if np.isnan(coef_matrix).any():\n",
    "            print(\"NAN\")\n",
    "            return coef_matrix, X_lagged, Y, coef_matrix, learning_rate, coef_matrix_1\n",
    "\n",
    "    X = df_comp[feature_names].values\n",
    "    Y = df_comp[target_name].values\n",
    "    X_lagged = lag_matrix(X, 2)\n",
    "    Y = Y[2:]\n",
    "\n",
    "    # Calculate the R2\n",
    "    Y_pred = X_lagged @ coef_matrix\n",
    "    R2 = r2_score(Y, Y_pred)\n",
    "\n",
    "    # Calculate the residuals\n",
    "    residuals = Y - Y_pred\n",
    "\n",
    "    return coef_matrix, list_loss, residuals, R2, y\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def granger_industry_gradient_descent(df_data_input: pd.DataFrame, p: int, column_A: str, column_B: str, column_industry: str, industry_names: list, learning_rate: float) -> tuple:\n",
    "    \"\"\"\n",
    "    Check for Granger causality between two variables, given a pandas DataFrame with columns 'Company Name', 'Period',\n",
    "    'Return', 'ESG Score', and the specified 'column_industry'.\n",
    "\n",
    "    Parameters:\n",
    "    df_data (pd.DataFrame): A pandas DataFrame with columns 'Company Name', 'Period', 'Return', 'ESG Score', and the specified 'column_industry'.\n",
    "    p (int): The number of lags to include in the VAR models.\n",
    "    column_A (str): The name of the first column to test for Granger causality.\n",
    "    column_B (str): The name of the second column to test for Granger causality.\n",
    "    column_industry (str): The name of the column containing industry information.\n",
    "    industry_name (str): The name of the industry to filter by when performing the Granger causality test.\n",
    "\n",
    "    Returns:\n",
    "    A tuple containing the residuals for each of the three VAR models applied to the specified industry, the dependent variable (y),\n",
    "    and the coefficients for each of the three VAR models.\n",
    "    \"\"\"\n",
    "\n",
    "    # Create a copy of the input dataframe to avoid modifying the original\n",
    "    df_data_old = df_data_input.copy()\n",
    "    df_data_old[\"random\"] = np.random.rand(df_data_old.shape[0])\n",
    "\n",
    "    # Initialize an empty dataframe with the same columns as the input dataframe\n",
    "    df_data = pd.DataFrame(columns=df_data_input.columns)\n",
    "\n",
    "    # Iterate through all unique company names in the input dataframe\n",
    "    for company in df_data_old[\"Company Name\"].unique():\n",
    "        X_company = df_data_old[df_data_old[\"Company Name\"] == company]\n",
    "\n",
    "        # Only process companies with more than 5 data points\n",
    "        if X_company.shape[0] > 5:\n",
    "            # Add 'p' rows of zeros to the beginning of each company's data\n",
    "            X_c = pd.DataFrame(columns=df_data.columns)\n",
    "            X_c = X_c.append([X_company.iloc[0]] * p, ignore_index=True)\n",
    "            X_c = X_c * 0\n",
    "            X_c[\"Period\"] = list(range(-p, 0))\n",
    "            X_c = X_c.append(X_company, ignore_index=True)\n",
    "            df_data = df_data.append(X_c, ignore_index=True)\n",
    "\n",
    "    # Initialize the VAR models using different predictors\n",
    "    # Model 1: column_A\n",
    "    # Model 2: column_A, random noise\n",
    "    # Model 3: column_A, column_B\n",
    "    X1 = df_data[[column_A]].copy()\n",
    "    X2 = df_data[[column_A]].copy()\n",
    "    X2[\"random\"] = np.random.rand(len(X2))\n",
    "    X3 = df_data[[column_A, column_B]].copy()\n",
    "\n",
    "    # Convert dataframes to numpy arrays\n",
    "    X1, X2, X3 = X1.values, X2.values, X3.values\n",
    "\n",
    "    # Fit the VAR models with 'p' lags\n",
    "    mse1, r21, X_lagged1, coeffs_1 = VAR_model(X1, p)\n",
    "    mse2, r22, X_lagged2, coeffs_2 = VAR_model(X2, p)\n",
    "    mse3, r23, X_lagged3, coeffs_3 = VAR_model(X3, p)\n",
    "    \n",
    "    #select only the industry rows from the data\n",
    "    df_data_industry = df_data[df_data[column_industry].isin(industry_names)]\n",
    "        \n",
    "    # Calculate the residuals for each model\n",
    "    companies_ = df_data_industry[\"Company Name\"].unique()\n",
    "\n",
    "    coeffs_1_industry,_,residuals_1,r2_1,y= make_gradient_descent_step_by_step(df_data,companies_,coeffs_1,learning_rate,[column_A],[column_A])\n",
    "    coeffs_2_industry,_,residuals_2,r2_2,y = make_gradient_descent_step_by_step(df_data,companies_,coeffs_2,learning_rate,[column_A,\"random\"],[column_A,\"random\"])\n",
    "    coeffs_3_industry,_,residuals_3,r2_3,y = make_gradient_descent_step_by_step(df_data,companies_,coeffs_3,learning_rate,[column_A,column_B],[column_A,column_B])\n",
    "\n",
    "    #print what model includes what variables\n",
    "    print(f\"Model 1: {column_A}\")\n",
    "    print(f\"Model 2: {column_A}, random\")\n",
    "    print(f\"Model 3: {column_A}, {column_B}\")\n",
    "        \n",
    "    # Print R-squared values for each model\n",
    "    print(f\"R-squared (Model 1): {r2_1}\")\n",
    "    print(f\"R-squared (Model 2): {r2_2}\")\n",
    "    print(f\"R-squared (Model 3): {r2_3}\")\n",
    "\n",
    "    # Perform F-tests to compare residuals between models\n",
    "    F12 = calculate_f_statistic(residuals_2, residuals_1)\n",
    "    F13 = calculate_f_statistic(residuals_3, residuals_1)\n",
    "    print(f\"F-statistic (Model 1 vs Model 2): {F12}\")\n",
    "    print(f\"F-statistic (Model 1 vs Model 3): {F13}\")\n",
    "\n",
    "    critical_value12 = calculate_critical_value(residuals_2, residuals_1, alpha=0.05)\n",
    "    critical_value13 = calculate_critical_value(residuals_3, residuals_1, alpha=0.05)\n",
    "    print(f\"Critical value (Model 1 vs Model 2): {critical_value12}\")\n",
    "    print(f\"Critical value (Model 1 vs Model 3): {critical_value13}\")\n",
    "\n",
    "    # Compare F-statistics to critical values\n",
    "    compare_f_statistic_to_critical_value(F12, critical_value12)\n",
    "    compare_f_statistic_to_critical_value(F13, critical_value13)\n",
    "\n",
    "    # Return the residuals, y, and coefficients for each model\n",
    "    return (residuals_1, residuals_2, residuals_3, y,\n",
    "            coeffs_1_industry, coeffs_2_industry, coeffs_3_industry\n",
    "            )"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analyze"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualize"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ESG by Country"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_esg_map(dataframe, country_col='Country of Headquarters', esg_score_col='ESG Score',download=True):\n",
    "    \"\"\"\n",
    "    Create a map of the world with ESG scores for each country. The ESG scores are standardized to have a mean of 0 and a standard deviation of 1.\n",
    "    input:\n",
    "        dataframe: A dataframe with a column containing the country names and a column containing the ESG scores.\n",
    "        country_col: The name of the column containing the country names.\n",
    "        esg_score_col: The name of the column containing the ESG scores.\n",
    "        download: A boolean indicating whether to download the map as an HTML file or to display it in the notebook.\n",
    "    output:\n",
    "        A map of the world with ESG scores for each country.(HTML file or displayed in the notebook)\n",
    "    \"\"\"\n",
    "    \n",
    "    world = gpd.read_file(gpd.datasets.get_path('naturalearth_lowres'))\n",
    "    merged_data = world.merge(dataframe, left_on='name', right_on=country_col, how='left')\n",
    "    merged_data = merged_data[merged_data[esg_score_col].notna()]\n",
    "\n",
    "    # Standardize the ESG scores\n",
    "    scaler = StandardScaler()\n",
    "    merged_data['Standardized ESG Score'] = scaler.fit_transform(merged_data[[esg_score_col]])\n",
    "\n",
    "    fig = px.choropleth(merged_data, geojson=merged_data.geometry, locations=merged_data.index,\n",
    "                        color='Standardized ESG Score', \n",
    "                        range_color=(merged_data['Standardized ESG Score'].min(), merged_data['Standardized ESG Score'].max()),\n",
    "                        projection='natural earth', hover_name='name', hover_data=[esg_score_col],\n",
    "                        labels={'Standardized ESG Score': 'Standardized ESG Score'})\n",
    "\n",
    "    fig.update_geos(showcountries=True, countrywidth=0.5)\n",
    "    fig.update_layout(title_text='Standardized ESG Score of Companies by Country', title_x=0.5)\n",
    "    if download:\n",
    "        fig.write_html(\"esg_map.html\")\n",
    "    else:\n",
    "        fig.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plot the Clusters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_clusters(df, labels, esg_col='ESG Score', return_col='Return'\n",
    "                  ):\n",
    "    \"\"\"\n",
    "    Plot the clusters of a dataframe based on cluster labels.\n",
    "\n",
    "    Args:\n",
    "        df (pd.DataFrame): The dataframe to plot.\n",
    "        labels (np.ndarray): The cluster labels of each row in the dataframe.\n",
    "    \"\"\"\n",
    "\n",
    "    # Create a scatter plot of \"ESG Score\" vs \"Return\" and use cluster labels to define the colors\n",
    "    sns.scatterplot(x=esg_col, y=return_col, hue=labels, data=df)\n",
    "    plt.legend(bbox_to_anchor=(1.05, 1), loc=2, borderaxespad=0.)\n",
    "    plt.show()\n",
    "\n",
    "    # Print the histogram of the cluster labels\n",
    "    sns.histplot(labels)\n",
    "    plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RUN CODE"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### First, we get the Data and clean it"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get the Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#let's download the all files we need \n",
    "for key, value in config.DICT_URL.items():\n",
    "    #check if the file is already downloaded\n",
    "    if not os.path.exists(config.LOCAL_FOLDER + key):\n",
    "        download_excel_file(value, config.LOCAL_FOLDER + key)\n",
    "        print(\"Downloaded file: \" + key)\n",
    "    \n",
    "#Scores of the ESG indicators on a company level\n",
    "df_individual_scores_10Y = refinite_to_python(config.LOCAL_FOLDER + \"ESG-individual_scores_10Y.xlsx\")\n",
    "#lets add the ESG Pillar scores\n",
    "df_pillar_scores_10Y_raw = refinite_to_python(config.LOCAL_FOLDER + \"ESG-pillar_scores_10Y.xlsx\")\n",
    "#lets read the logged volatility data to\n",
    "#since it is already prepared, we can just read it\n",
    "df_volatility_10Y_logged_raw = pd.read_excel(config.LOCAL_FOLDER + \"ESG-volatility_10Y_logged.xlsx\")\n",
    "#finally, lets add the EBIDTA data\n",
    "df_EBITDA_10Y_raw = refinite_to_python_EBITDA(config.LOCAL_FOLDER + \"ESG-EBITDA_10Y.xlsx\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Clean it and add clusters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#first, lets rename the Return column \n",
    "df_individual_scores_10Y.rename(columns={'YTD Total Return':'Return'}, inplace=True)\n",
    "df_individual_scores_10Y_clean = clean_data(df_individual_scores_10Y)\n",
    "\n",
    "df_pillar_scores_10Y = df_pillar_scores_10Y_raw.copy()\n",
    "#add the Return and clusters to df_pillar_scores_10Y based on Company Name and Period\n",
    "if \"YTD Total ReturnIn the last 10 D_D\" in df_pillar_scores_10Y.columns:\n",
    "    df_pillar_scores_10Y.drop(columns=['YTD Total ReturnIn the last 10 D_D'], inplace=True)\n",
    "df_merged_pillar = pd.merge(df_individual_scores_10Y_clean[['Company Name', 'Period', 'Cluster',\"Return\"]], df_pillar_scores_10Y, left_on=['Company Name', 'Period'], right_on=['Company Name', 'Period'], how='left')\n",
    "\n",
    "#format volatility data and add to ESG Pillar file\n",
    "#make a list from 2022 to 2012\n",
    "list_dates = [str(i) for i in range(2022, 2012, -1)]\n",
    "#melt, so the format is correct\n",
    "melted_df = df_volatility_10Y_logged_raw.melt(id_vars=['Company_Name'], value_vars=list_dates, var_name='Period', value_name='Volatility')\n",
    "#calculate the period\n",
    "melted_df[\"Period\"] = 2022 - melted_df[\"Period\"].astype(int)\n",
    "#rename the column\n",
    "melted_df.rename(columns={'Company_Name':'Company Name'}, inplace=True)\n",
    "df_volatility_10Y_logged_melt = melted_df.copy()\n",
    "#merge the volatility data with the ESG Pillar data\n",
    "df_merged_pillar_volatility = pd.merge(df_merged_pillar, df_volatility_10Y_logged_melt[['Company Name', 'Period',\"Volatility\"]], left_on=['Company Name', 'Period'], right_on=['Company Name', 'Period'], how='left')\n",
    "\n",
    "#format EBITDA data and add to ESG Pillar file\n",
    "df_merged_pillar_volatility_EBITDA = pd.merge(df_merged_pillar_volatility, df_EBITDA_10Y_raw[['Company Name', 'Period',\"EBITDA Margin, Percent\"]], left_on=['Company Name', 'Period'], right_on=['Company Name', 'Period'], how='left')\n",
    "\n",
    "#rename \n",
    "df_pillar_scores_10Y_EBITDA_volatility = df_merged_pillar_volatility_EBITDA.copy()\n",
    "#lets remove if column has only NaN values\n",
    "df_pillar_scores_10Y_EBITDA_volatility.dropna(axis=1, how='all', inplace=True)\n",
    "df_pillar_scores_10Y_EBITDA_volatility.dropna(axis=0, subset='Period', inplace=True)\n",
    "df_pillar_scores_10Y_EBITDA_volatility.dropna(axis=0, subset='Cluster', inplace=True)\n",
    "\n",
    "#remove all rows where the volatility, the return or the EBITDA Margin is lower than -1 or higher than 1\n",
    "df_pillar_scores_10Y_EBITDA_volatility = df_pillar_scores_10Y_EBITDA_volatility[(df_pillar_scores_10Y_EBITDA_volatility['Volatility'] >= -1) & (df_pillar_scores_10Y_EBITDA_volatility['Volatility'] <= 1)]\n",
    "df_pillar_scores_10Y_EBITDA_volatility.fillna(0, inplace=True)\n",
    "df_pillar_scores_10Y_EBITDA_volatility_clean = df_merged_pillar_volatility_EBITDA.fillna(0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#lets calculate the optimal number of clusters\n",
    "df_individual_scores_10Y_clean_scaled = scale_data(df_individual_scores_10Y_clean)\n",
    "#takes a bit time, if interested, remove the # sign\n",
    "#optimal_n_clusters = find_optimal_clusters(df_individual_scores_10Y_clean_scaled)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets count the Nans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#percentage of non-null values in each column\n",
    "pd.set_option('display.max_rows', None)\n",
    "display(1 - df_individual_scores_10Y_clean.isna().sum()/len(df_individual_scores_10Y_clean))\n",
    "pd.reset_option('display.max_rows', 15)\n",
    "#we leave the Nans so we can distinguish between the companies that have scores and the ones that don't"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets add the transformed values as returns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_individual_scores_10Y_clean_transformed = transform_columns(df_individual_scores_10Y_clean,\"Return\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Now we start with some basic data exploration and visualization "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ESG by country and Correlation analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#takes some time, if interested, remove the # sign\n",
    "#create_esg_map(df_individual_scores_10Y_clean)\n",
    "#group_corr(df_individual_scores_10Y,\"Return\",\"ESG Score\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ESG by Cluster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#get the mean ESG Score for each cluster\n",
    "df_individual_scores_10Y_clean_transformed.groupby('Cluster').mean()[\"ESG Score\"]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Correlation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#get non numeric columns\n",
    "df_individual_scores_10Y_clean.corr()[\"Return\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "group_corr(df_individual_scores_10Y_clean, \"ESG Score\", \"Return\", column_grop=\"Company Name\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#lagged correlation\n",
    "for col_ in df_pillar_scores_10Y_EBITDA_volatility.columns:\n",
    "    if col_ != \"Period\" and col_ != \"Cluster\" and col_ != \"Company Name\" and col_ != \"Return\" and col_ != \"Volatility\" and col_ != \"EBITDA Margin, Percent\" and col_ != \"RIC\":\n",
    "        print(\"Correlation of \" + col_ + \" with:\")\n",
    "        print(\"Return\")\n",
    "        print(lagged_corr(df_pillar_scores_10Y_EBITDA_volatility, col_, \"Return\", column_grop=\"Company Name\", p=1, time_column=\"Period\")[\"lagged_correlation\"].mean())\n",
    "        print(\"Volatility\")\n",
    "        print(lagged_corr(df_pillar_scores_10Y_EBITDA_volatility, col_, \"Volatility\", column_grop=\"Company Name\", p=1, time_column=\"Period\")[\"lagged_correlation\"].mean())\n",
    "        print(\"EBITDA Margin, Percent\")\n",
    "        print(lagged_corr(df_pillar_scores_10Y_EBITDA_volatility, col_, \"EBITDA Margin, Percent\", column_grop=\"Company Name\", p=1, time_column=\"Period\")[\"lagged_correlation\"].mean())\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Analyse Cluster high level categories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_individual_scores_10Y_clean_Cluster = df_individual_scores_10Y_clean.copy()\n",
    "\n",
    "#make dictionary with categories as values and whats in list as keys\n",
    "categories_dict = {}\n",
    "for category in healthcare_categories:\n",
    "    categories_dict[category] = \"Healthcare\"\n",
    "for category in consumer_categories:\n",
    "    categories_dict[category] = \"Consumer\"\n",
    "for category in tech_categories:\n",
    "    categories_dict[category] = \"Tech\"\n",
    "for category in finance_categories:\n",
    "    categories_dict[category] = \"Finance\"\n",
    "for category in energy_categories:\n",
    "    categories_dict[category] = \"Energy\"\n",
    "    \n",
    "#add the high level category to the dataframe\n",
    "df_individual_scores_10Y_clean_Cluster[\"Category\"] = df_individual_scores_10Y_clean_Cluster[\"GICS Industry Name\"].apply(lambda x: categories_dict[x] if x in categories_dict.keys() else \"Other\")\n",
    "\n",
    "#make mean of each company\n",
    "#get numeric columns\n",
    "numeric_columns = df_individual_scores_10Y_clean_Cluster.select_dtypes(include=np.number).columns.tolist()\n",
    "other_columns = [x for x in df_individual_scores_10Y_clean_Cluster.columns.tolist() if x not in numeric_columns]\n",
    "df_individual_scores_10Y_clean_Cluster = df_individual_scores_10Y_clean_Cluster.groupby('Company Name').agg({col: 'mean' for col in numeric_columns} | {col: lambda x: x.mode()[0] for col in other_columns})\n",
    "\n",
    "\n",
    "\n",
    "#print the composition of each cluster as an overview, \n",
    "for cluster in range(0, 5):\n",
    "    print(\"Cluster \" + str(cluster))\n",
    "    print(df_individual_scores_10Y_clean_Cluster[df_individual_scores_10Y_clean_Cluster[\"Cluster\"] == cluster][\"Category\"].value_counts())\n",
    "    print(\"\")\n",
    "    \n",
    "    #also return as pie chart\n",
    "    df_individual_scores_10Y_clean_Cluster[df_individual_scores_10Y_clean_Cluster[\"Cluster\"] == cluster][\"Category\"].value_counts().plot.pie()\n",
    "    plt.show()\n",
    "    \n",
    "#get a pie chart of the composition of the data (wich cluster has how many companies)\n",
    "print(\"Composition of the data\")\n",
    "for cluster in range(0, 5):\n",
    "    print(\"Cluster \" + str(cluster) + \" contains \" + str(df_individual_scores_10Y_clean_Cluster[df_individual_scores_10Y_clean_Cluster[\"Cluster\"] == cluster][\"Category\"].value_counts().sum()) + \" companies\") \n",
    "    print(\"\")\n",
    "    #also return as pie chart\n",
    "df_individual_scores_10Y_clean_Cluster[\"Cluster\"].value_counts().plot.pie()\n",
    "plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Lets perform the Granger tests"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Granger tests simple"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ESG and Returns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "residuals_1, residuals_2, residuals_3, y,coeffs_1, coeffs_2, coeffs_3 = check_granger_simple(df_individual_scores_10Y_clean_transformed, p=5, column_A= \"Return\", column_B= \"ESG Score\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#perform the industy beta analysis for every cluster\n",
    "for int_cluster in range(5):\n",
    "    print(\"============= \\n\\n\")\n",
    "    print(f\"Cluster {int_cluster}\")\n",
    "    #Zeilenumbruch\n",
    "    check_granger_industry_beta(df_individual_scores_10Y_clean_transformed, p=2, column_A= \"Return\", column_B= \"ESG Score\",column_industry=\"Cluster\", industry_name=int_cluster)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Granger Test on clusters with Gradient Descent optimization"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ESG and Returns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for int_cluster in range(5):\n",
    "    print(\"============= \\n\\n\")\n",
    "    print(f\"Cluster {int_cluster}\")\n",
    "    #Zeilenumbruch\n",
    "    residuals_1, residuals_2, residuals_3, y,coeffs_1_industry, coeffs_2_industry, coeffs_3_industry = granger_industry_gradient_descent(df_individual_scores_10Y_clean,2,\"Return\",\"ESG Score\",\"Cluster\",[int_cluster],0.0001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "df_pillar_scores_10Y_EBITDA_volatility_clean = df_merged_pillar_volatility_EBITDA.fillna(0)\n",
    "\n",
    "for str_pillar_ in [\"Social Pillar Score\",\"Environmental Pillar Score\",\"Governance Pillar Score\"]:\n",
    "    for int_cluster in range(0,5):\n",
    "        print(\"============= \\n\\n\")\n",
    "        print(f\"Pillar: {str_pillar}\")\n",
    "        int_cluster_use = random.choice([0,1,2,3,4])\n",
    "        print(\"============= \\n\\n\")\n",
    "        print(f\"Cluster {int_cluster_use}\")\n",
    "        #Zeilenumbruch\n",
    "        residuals_1, residuals_2, residuals_3, y,coeffs_1_industry, coeffs_2_industry, coeffs_3_industry = granger_industry_gradient_descent(df_pillar_scores_10Y_EBITDA_volatility_clean,2,\"EBITDA Margin, Percent\",str_pillar,\"Cluster\",[int_cluster_use],0.000025)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ESG and EBITDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for int_cluster in range(0,5):\n",
    "    print(\"============= \\n\\n\")\n",
    "    print(f\"Cluster {int_cluster}\")\n",
    "    #Zeilenumbruch\n",
    "    residuals_1, residuals_2, residuals_3, y,coeffs_1_industry, coeffs_2_industry, coeffs_3_industry = granger_industry_gradient_descent(df_pillar_scores_10Y_EBITDA_volatility_clean,2,\"EBITDA Margin, Percent \",\"ESG Score\",\"Cluster\",[int_cluster],0.0000025)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ESG and Volatility"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for int_cluster in range(1,5):\n",
    "    print(\"============= \\n\\n\")\n",
    "    print(f\"Cluster {int_cluster}\")\n",
    "    #Zeilenumbruch\n",
    "    residuals_1, residuals_2, residuals_3, y,coeffs_1_industry, coeffs_2_industry, coeffs_3_industry = granger_industry_gradient_descent(df_pillar_scores_10Y_EBITDA_volatility,2,\"Volatility\",\"ESG Score\",\"Cluster\",[int_cluster],0.00005)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ESG Pillar Scores and EBITDA Margin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for str_pillar in [\"Social Pillar Score\",\"Environmental Pillar Score\",\"Governance Pillar Score\"]:\n",
    "    for int_cluster in range(0,5):\n",
    "        print(\"============= \\n\\n\")\n",
    "        print(f\"Pillar: {str_pillar}\")\n",
    "        print(\"============= \\n\\n\")\n",
    "        print(f\"Cluster {int_cluster}\")\n",
    "        #Zeilenumbruch\n",
    "        residuals_1, residuals_2, residuals_3, y,coeffs_1_industry, coeffs_2_industry, coeffs_3_industry = granger_industry_gradient_descent(df_pillar_scores_10Y_EBITDA_volatility,2,\"EBITDA Margin, Percent\",str_pillar,\"Cluster\",[int_cluster],FLOAT_LEARNING_RATE)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ESG Pillar Scores and Volatility"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for str_pillar in [\"Social Pillar Score\",\"Environmental Pillar Score\",\"Governance Pillar Score\"]:\n",
    "    for int_cluster in range(0,5):\n",
    "        #iterate over all clusters and pillars\n",
    "        print(\"============= \\n\\n\")\n",
    "        print(f\"Pillar: {str_pillar}\")\n",
    "        print(\"============= \\n\\n\")\n",
    "        print(f\"Cluster {int_cluster}\")\n",
    "        #Zeilenumbruch\n",
    "        residuals_1, residuals_2, residuals_3, y,coeffs_1_industry, coeffs_2_industry, coeffs_3_industry = granger_industry_gradient_descent(df_pillar_scores_10Y_EBITDA_volatility,2,\"EBITDA Margin, Percent\",str_pillar,\"Cluster\",[int_cluster],0.000025)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
