{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preparation"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Module imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import dropbox\n",
    "import config\n",
    "import requests\n",
    "import os\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "import pandas as pd\n",
    "import geopandas as gpd\n",
    "import plotly.express as px\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.cluster import KMeans\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.metrics import silhouette_score, calinski_harabasz_score, davies_bouldin_score\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#global variables\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Methods"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Lets gather the data"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First we import the data form our local folder or from dropbox"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def download_excel_file(url, output_file):\n",
    "    \"\"\"\n",
    "    Download an Excel file from a URL and save it locally.\n",
    "    \"\"\"\n",
    "    # Modify the Dropbox URL to force download\n",
    "    download_url = url.replace(\"?dl=0\", \"?dl=1\")\n",
    "\n",
    "    # Send an HTTP GET request to download the file\n",
    "    response = requests.get(download_url)\n",
    "\n",
    "    # Check if the request was successful (HTTP status code 200)\n",
    "    if response.status_code == 200:\n",
    "        # Create the output folder if it does not exist\n",
    "        output_folder = os.path.dirname(output_file)\n",
    "        if output_folder and not os.path.exists(output_folder):\n",
    "            os.makedirs(output_folder)\n",
    "\n",
    "        # Save the file in the output folder\n",
    "        with open(output_file, \"wb\") as file:\n",
    "            file.write(response.content)\n",
    "    else:\n",
    "        print(\"Error: Unable to download the file\")\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">> First lets import some Data from yahoo \n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">> Lets import some Data from refinitive"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">>> via the API"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### LetÂ´s clean the Data"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Make the data readable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def refinite_to_python(file_path):\n",
    "    \"\"\"\n",
    "    Make a file from the refinitive screener readable for python and useable for the analysis.\n",
    "    input: file_path\n",
    "    output: cleaned dataframe\n",
    "    \"\"\"\n",
    "    \n",
    "    # Read data from excel file\n",
    "    df_all_comp_all = pd.read_excel(file_path, header=[0, 1])\n",
    "\n",
    "    # Combine multilevel columns into a single level\n",
    "    df_all_comp_all.columns = ['_'.join(col).strip() for col in df_all_comp_all.columns.values]\n",
    "\n",
    "    # Rename columns for better understanding\n",
    "    df_all_comp_all.rename(columns={\"Company Name_Unnamed: 1_level_1\": \"Company Name\"}, inplace=True)\n",
    "    df_all_comp_all.rename(columns={\"Identifier (RIC)_Unnamed: 0_level_1\": \"RIC\"}, inplace=True)\n",
    "    df_all_comp_all.rename(columns={\"Country of Headquarters_Unnamed: 2_level_1\": \"Country of Headquarters\"}, inplace=True)\n",
    "    df_all_comp_all.rename(columns={\"NAICS Subsector Name_Unnamed: 5_level_1\": \"NAICS Subsector Name\"}, inplace=True)\n",
    "\n",
    "    # Remove unwanted characters from column names\n",
    "    df_all_comp_all.columns = df_all_comp_all.columns.str.replace('\\nIn the last 10 FY_FY', ' ')\n",
    "    df_all_comp_all.columns = df_all_comp_all.columns.str.replace(\"\\nIn the last 15 Y_Y\",\" \")\n",
    "    df_all_comp_all.columns = df_all_comp_all.columns.str.replace('-', '')\n",
    "    \n",
    "\n",
    "    # Create a copy of the DataFrame\n",
    "    df_all_comp = df_all_comp_all.copy()\n",
    "    #return df_all_comp\n",
    "    df_all_comp.drop(columns=[\"YTD Total Return 11\"], inplace=True)\n",
    "    df_all_comp.drop(columns=[\"YTD Total Return 12\"], inplace=True)\n",
    "    df_all_comp.drop(columns=[\"YTD Total Return 13\"], inplace=True)\n",
    "    df_all_comp.drop(columns=[\"YTD Total Return 14\"], inplace=True)\n",
    "\n",
    "    # Drop rows with all NaN values and fill remaining NaNs with 0\n",
    "    df_all_comp.dropna(inplace=True, how=\"all\")\n",
    "    df_all_comp.fillna(0, inplace=True)\n",
    "\n",
    "    # Extract unique column prefixes\n",
    "    list_columns = [col for col in df_all_comp.columns if col[-1].isdigit()]\n",
    "    list_columns = [col[:-1] for col in list_columns]\n",
    "    list_columns = [col[:-1] if col[-1].isdigit() else col for col in list_columns]\n",
    "    list_columns = list(set(list_columns))\n",
    "    list_columns = [col.replace(\"\\n\", \"\") for col in list_columns]\n",
    "\n",
    "    # Initialize an empty list to store DataFrames\n",
    "    dfs = []\n",
    "\n",
    "    # Iterate over unique column prefixes\n",
    "    for colum in tqdm(list_columns):\n",
    "        esg_cols = [col for col in df_all_comp.columns if col.startswith(colum)]\n",
    "        df = df_all_comp[[\"Company Name\"] + esg_cols].copy()\n",
    "\n",
    "        # Melt the DataFrame to transform it into the desired format\n",
    "        melted_df = df.melt(\n",
    "            id_vars=[\"Company Name\"],\n",
    "            var_name=\"Period\",\n",
    "            value_name=colum\n",
    "        )\n",
    "        melted_df[\"Period\"] = melted_df[\"Period\"].apply(lambda x: 1 if x[-1].isdigit() == False else 10 if x[-2:-1] == \"10\" else int(x[-1]))\n",
    "\n",
    "        dfs.append(melted_df)\n",
    "\n",
    "    # Concatenate the melted DataFrames\n",
    "    melted_df_all = pd.concat(dfs, axis=1)\n",
    "\n",
    "    # Remove duplicate columns\n",
    "    melted_df_all = melted_df_all.loc[:, ~melted_df_all.columns.duplicated()]\n",
    "\n",
    "    # Sort the DataFrame by Company Name and Period\n",
    "    melted_df_all = melted_df_all.sort_values(by=[\"Company Name\", \"Period\"])\n",
    "\n",
    "    # Create a copy of the final DataFrame\n",
    "    df_data_ = melted_df_all.copy()\n",
    "\n",
    "    #remove rows without any period (nan)\n",
    "    #df_data_ = df_data_[df_data_[\"Period\"].notna()]\n",
    "    \n",
    "    #replace all 0 with nan, except for the period column\n",
    "    #it is important to diffirentiate between 0 and nan, for example for the correlation\n",
    "    df_data_ = df_data_.replace(0, np.nan)\n",
    "    df_data_ = df_data_.replace(\"0\", np.nan)\n",
    "    df_data_[\"Period\"] = df_data_[\"Period\"].fillna(0)\n",
    "\n",
    "    #add RIC, Country of Headquarters and NAICS Subsector Name to the dataframe\n",
    "    df_data_[\"RIC\"] = df_all_comp_all[\"RIC\"]\n",
    "    df_data_[\"Country of Headquarters\"] = df_all_comp_all[\"Country of Headquarters\"]\n",
    "    df_data_[\"NAICS Subsector Name\"] = df_all_comp_all[\"NAICS Subsector Name\"]\n",
    "    #fill nan of RIC, Country of Headquarters and NAICS Subsector Name with the last value that is not nan\n",
    "    df_data_[\"RIC\"] = df_data_.groupby(\"Company Name\")[\"RIC\"].ffill()\n",
    "    df_data_[\"Country of Headquarters\"] = df_data_.groupby(\"Company Name\")[\"Country of Headquarters\"].ffill()\n",
    "    df_data_[\"NAICS Subsector Name\"] = df_data_.groupby(\"Company Name\")[\"NAICS Subsector Name\"].ffill()\n",
    "    \n",
    "\n",
    "    #remove all the whitespaces in the end of a column name\n",
    "    df_data_.columns = df_data_.columns.str.rstrip()\n",
    "        \n",
    "    if \"ESG Score\" not in df_data_.columns:\n",
    "        #make sure only one ESG Score column is in the dataframe\n",
    "        df_data_[\"ESG Score\"] = df_data_[\"ESG Score \"]\n",
    "\n",
    "    \n",
    "    return df_data_\n",
    "    "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Clean the data and add some clusters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_data(df, esg_col = \"ESG Score\", return_col = \"Return\"):\n",
    "    \"\"\"\n",
    "    Clean and preprocess a dataframe for clustering based on \"ESG Score\" and \"Return\" columns.\n",
    "\n",
    "    Args:\n",
    "        df (pd.DataFrame): The input dataframe to clean.\n",
    "        esg_col (str): The column name for the ESG Score.\n",
    "        return_col (str): The column name for the Return.\n",
    "\n",
    "    Returns:\n",
    "        pd.DataFrame: The cleaned and preprocessed dataframe.\n",
    "    \"\"\"\n",
    "\n",
    "    # Make a copy of the input dataframe to avoid modifying it\n",
    "    df_data = df.copy()\n",
    "\n",
    "    # Drop rows with NaN values in \"Return\" or \"ESG Score\" columns\n",
    "    df_data.dropna(subset=[return_col, esg_col], inplace=True)\n",
    "\n",
    "    # Remove outliers in the \"Return\" column (more than 2 std away from the mean)\n",
    "    return_mean = df_data[return_col].mean()\n",
    "    return_std = df_data[return_col].std()\n",
    "    df_data = df_data[(df_data[return_col] < return_mean + 2 * return_std) & (df_data[return_col] > return_mean - 2 * return_std)]\n",
    "\n",
    "    # Rescale the \"ESG Score\" and \"Return\" columns to be between 0 and 1\n",
    "    df_data[esg_col] = df_data[esg_col] / df_data[esg_col].max()\n",
    "    df_data[return_col] = df_data[return_col] / df_data[return_col].max()\n",
    "\n",
    "    # Cluster the companies based on \"ESG Score\" and \"Return\" columns using KMeans\n",
    "    model = KMeans(n_clusters=6)\n",
    "    model.fit(df_data[[esg_col, return_col]])\n",
    "    labels = model.predict(df_data[[esg_col, return_col]])\n",
    "\n",
    "    # Return the cleaned dataframe\n",
    "    return df_data"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Add Transformation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def transform_columns(df,column):\n",
    "    \"\"\"\n",
    "    takes a dataframe and a column name and transforms the column into:\n",
    "    log\n",
    "    log_2\n",
    "    sqrt\n",
    "    sqrt_2\n",
    "    _2\n",
    "    _3\n",
    "    _4\n",
    "    \"\"\"\n",
    "    \n",
    "    df[column+\"_log\"] = np.log(df[column])\n",
    "    df[column+\"_log_2\"] = np.log(df[column]**2)\n",
    "    df[column+\"_sqrt\"] = np.sqrt(df[column])\n",
    "    df[column+\"_sqrt_2\"] = np.sqrt(df[column]**2)\n",
    "    df[column+\"_2\"] = df[column]**2\n",
    "    df[column+\"_3\"] = df[column]**3\n",
    "    df[column+\"_4\"] = df[column]**4\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Scale the Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scale_data(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"Scale the numerical columns of the DataFrame.\n",
    "\n",
    "    Args:\n",
    "        df (pd.DataFrame): The input DataFrame.\n",
    "\n",
    "    Returns:\n",
    "        pd.DataFrame: The scaled DataFrame.\n",
    "    \"\"\"\n",
    "    scaler = StandardScaler()\n",
    "    numerical_columns = df.select_dtypes(include=['float64', 'int64']).columns\n",
    "    df_scaled = scaler.fit_transform(df[numerical_columns])\n",
    "    return pd.DataFrame(df_scaled, columns=numerical_columns, index=df.index)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lets dive deep in the data"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MATH Formulas"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### LetÂ´s define our basic math functions \n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Gradient Descent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def initialize_coefficients(n_features, n_targets):\n",
    "    \"\"\"\n",
    "    Initializes the coefficients for VAR.\n",
    "    \n",
    "    Parameters:\n",
    "        n_features (int): Number of input features.\n",
    "        n_targets (int): Number of output targets.\n",
    "        \n",
    "    Returns:\n",
    "        numpy.ndarray: An (n_features x n_targets) matrix of initialized coefficients.\n",
    "    \"\"\"\n",
    "    return np.random.rand(n_features, n_targets)\n",
    "\n",
    "def calculate_loss(X, Y, coefficients):\n",
    "    \"\"\"\n",
    "    Calculates the mean squared error (MSE) for the current coefficients.\n",
    "    \n",
    "    Parameters:\n",
    "        X (numpy.ndarray): An (n_samples x n_features) array of input data.\n",
    "        Y (numpy.ndarray): An (n_samples x n_targets) array of output data.\n",
    "        coefficients (numpy.ndarray): An (n_features x n_targets) matrix of current coefficients.\n",
    "        \n",
    "    Returns:\n",
    "        float: The mean squared error (MSE) for the current coefficients.\n",
    "    \"\"\"\n",
    "    predicted = np.dot(X, coefficients)\n",
    "    errors = Y - predicted\n",
    "    return np.sum(errors**2) / (2 * X.shape[0])\n",
    "\n",
    "def calculate_lasso_loss(_X, _Y, coefficients, lambda_):\n",
    "    \"\"\"\n",
    "    Calculates the LASSO loss for the current coefficients.\n",
    "\n",
    "    Parameters:\n",
    "        X (numpy.ndarray): An (n_samples x n_features) array of input data.\n",
    "        Y (numpy.ndarray): An (n_samples x n_targets) array of output data.\n",
    "        coefficients (numpy.ndarray): An (n_features x n_targets) matrix of current coefficients.\n",
    "        lambda_ (float): The regularization parameter.\n",
    "\n",
    "    Returns:\n",
    "        float: The LASSO loss for the current coefficients.\n",
    "    \"\"\"\n",
    "    predicted = np.dot(_X, coefficients)\n",
    "    errors = _Y - predicted\n",
    "    mse = np.sum(errors**2) / (2 * _X.shape[0])\n",
    "    l1_norm = np.sum(np.abs(coefficients))\n",
    "    lasso_loss = mse + lambda_ * l1_norm\n",
    "    return lasso_loss\n",
    "\n",
    "\n",
    "def outer(a, b):\n",
    "    \"\"\"\n",
    "    Computes the outer product of two 1-dimensional arrays.\n",
    "\n",
    "    Parameters:\n",
    "        a (array-like): 1-dimensional array.\n",
    "        b (array-like): 1-dimensional array.\n",
    "\n",
    "    Returns:\n",
    "        2-dimensional array where the element at position (i, j) is the\n",
    "        product of the i-th element of `a` and the j-th element of `b`.\n",
    "    \"\"\"\n",
    "    outer_product = np.zeros((len(a), len(b)))  # initialize the result with zeros\n",
    "    for i in range(len(a)):\n",
    "        for j in range(len(b)):\n",
    "            outer_product[i, j] = a[i] * b[j]  # compute the product of the i-th element of a and the j-th element of b\n",
    "    return outer_product\n",
    "\n",
    "\n",
    "\n",
    "def calculate_gradient(X, Y, coefficients):\n",
    "    \"\"\"\n",
    "    Calculates the gradient of the loss function with respect to the coefficients.\n",
    "    \n",
    "    Parameters:\n",
    "        X (numpy.ndarray): An (n_samples x n_features) array of input data.\n",
    "        Y (numpy.ndarray): An (n_samples x n_targets) array of output data.\n",
    "        coefficients (numpy.ndarray): An (n_features x n_targets) matrix of current coefficients.\n",
    "        \n",
    "    Returns:\n",
    "        numpy.ndarray: An (n_features x n_targets) matrix of gradient values.\n",
    "    \"\"\"\n",
    "    n_samples = X.shape[0]\n",
    "    #without Lasso\n",
    "    #grad = np.dot(X.T, np.dot(X, coefficients) - Y) / n_samples\n",
    "    grad = (1/n_samples) * np.dot(X.T, np.dot(X, coefficients) - Y) + 0.2 * np.sign(coefficients)  \n",
    "    return grad\n",
    "\n",
    "def update_coefficients(coefficients, gradients, lr):\n",
    "    \"\"\"\n",
    "    Updates the coefficients using the gradient and the learning rate.\n",
    "    \n",
    "    Parameters:\n",
    "        coefficients (numpy.ndarray): An (n_features x n_targets) matrix of current coefficients.\n",
    "        gradients (numpy.ndarray): An (n_features x n_targets) matrix of gradient values.\n",
    "        lr (float): Learning rate for the optimization algorithm.\n",
    "        \n",
    "    Returns:\n",
    "        numpy.ndarray: An (n_features x n_targets) matrix of updated coefficients.\n",
    "    \"\"\"\n",
    "    return coefficients - lr * gradients\n",
    "\n",
    "def gradient_descent_step(X__, Y__, coefficients__, lr):\n",
    "    \"\"\"\n",
    "    Performs a single step of gradient descent for VAR.\n",
    "    \n",
    "    Parameters:\n",
    "        X (numpy.ndarray): An (n_samples x n_features) array of input data.\n",
    "        Y (numpy.ndarray): An (n_samples x n_targets) array of output data.\n",
    "        coefficients (numpy.ndarray): An (n_features x n_targets) matrix of current coefficients.\n",
    "        lr (float): Learning rate for the optimization algorithm.\n",
    "        \n",
    "    Returns:\n",
    "        tuple: A tuple containing the updated coefficients and the loss for the current iteration.\n",
    "    \"\"\"\n",
    "    lambda_=1#variieren mit cosine similarity\n",
    "    gradients = calculate_gradient(X__, Y__, coefficients__)\n",
    "    updated_coefficients = update_coefficients(coefficients__, gradients, lr)\n",
    "    loss = calculate_lasso_loss(X__, Y__, coefficients__, lambda_)\n",
    "    return updated_coefficients, loss\n",
    "\n",
    "def calculate_gradient(X_, Y_, coefficients_):\n",
    "    \"\"\"\n",
    "    Calculates the gradient of the loss function with respect to the coefficients.\n",
    "    \n",
    "    Parameters:\n",
    "        X (numpy.ndarray): An (n_samples x n_features) array of input data.\n",
    "        Y (numpy.ndarray): An (n_samples x n_targets) array of output data.\n",
    "        coefficients (numpy.ndarray): An (n_features x n_targets) matrix of current coefficients.\n",
    "        \n",
    "    Returns:\n",
    "        numpy.ndarray: An (n_features x n_targets) matrix of gradient values.\n",
    "    \"\"\"\n",
    "    gradients = np.zeros_like(coefficients_)\n",
    "    for i in range(X_.shape[0]):\n",
    "        xi = X_[i, :]\n",
    "        yi = Y_[i, :]\n",
    "        predicted = np.dot(xi, coefficients_)\n",
    "        error = yi - predicted\n",
    "        gradients += outer(xi, error)\n",
    "    gradients /= X_.shape[0]\n",
    "    return gradients\n",
    "import numpy as np"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "F-Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_f_statistic(sample1, sample2):\n",
    "    \"\"\"\n",
    "    Calculates the F-statistic for two samples.\n",
    "    \n",
    "    Parameters:\n",
    "        sample1 (numpy.ndarray): An array of values for the first sample.\n",
    "        sample2 (numpy.ndarray): An array of values for the second sample.\n",
    "        \n",
    "    Returns:\n",
    "        float: The F-statistic for the two samples.\n",
    "    \"\"\"\n",
    "    var1 = np.var(sample1)\n",
    "    var2 = np.var(sample2)\n",
    "    return var1 / var2\n",
    "\n",
    "def calculate_critical_value(sample1, sample2, alpha):\n",
    "    \"\"\"\n",
    "    Calculates the critical value of the F-distribution for two samples.\n",
    "    \n",
    "    Parameters:\n",
    "        sample1 (numpy.ndarray): An array of values for the first sample.\n",
    "        sample2 (numpy.ndarray): An array of values for the second sample.\n",
    "        alpha (float): The significance level.\n",
    "        \n",
    "    Returns:\n",
    "        float: The critical value of the F-distribution for the two samples.\n",
    "    \"\"\"\n",
    "    df1 = len(sample1) - 1\n",
    "    df2 = len(sample2) - 1\n",
    "    return f.ppf(q=1-alpha, dfn=df1, dfd=df2)\n",
    "\n",
    "def compare_f_statistic_to_critical_value(f_statistic, critical_value):\n",
    "    \"\"\"\n",
    "    Compares the F-statistic to the critical value of the F-distribution.\n",
    "    \n",
    "    Parameters:\n",
    "        f_statistic (float): The F-statistic for the two samples.\n",
    "        critical_value (float): The critical value of the F-distribution for the two samples.\n",
    "    \"\"\"\n",
    "    if f_statistic > critical_value:\n",
    "        print('Reject the null hypothesis that the variances are equal')\n",
    "        return False\n",
    "    else:\n",
    "        print('Accept the null hypothesis that the variances are equal')\n",
    "        return True"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cosine similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cosine_similarity(v1, v2):\n",
    "    \"\"\"\n",
    "    calculates the cosine similarity between two vectors\n",
    "    input: v1, v2: numpy arrays\n",
    "    output: cosine similarity (float) \n",
    "    \"\"\"\n",
    "    return np.dot(v1, v2) / (np.linalg.norm(v1) * np.linalg.norm(v2))\n",
    "\n",
    "def cosine_similarity_matrix(A,B):\n",
    "    \"\"\"\n",
    "    calculates the cosine similarity similarity between the rows of A and B\n",
    "    \"\"\"\n",
    "    return np.array([cosine_similarity(A[i,:],B[i,:]) for i in range(A.shape[0])])\n",
    "\n",
    "#calculate the cosine similarity for every company to every other company\n",
    "#select only the numerical columns\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lag Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lag_matrix(X,p):\n",
    "    \"\"\"\n",
    "    lags matrix X by p\n",
    "    \"\"\"\n",
    "    X_lagged = np.zeros((X.shape[0]-p,X.shape[1]*p))\n",
    "    for i in range(p):\n",
    "        X_lagged[:,i*X.shape[1]:(i+1)*X.shape[1]] = X[p-i-1:-i-1,:]\n",
    "    return X_lagged"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "calculate optimal number cluster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_optimal_clusters(df_scaled: pd.DataFrame) -> int:\n",
    "    \"\"\"Find the optimal number of clusters using silhouette, calinski_harabasz,\n",
    "    and davies_bouldin scores.\n",
    "\n",
    "    Args:\n",
    "        df_scaled (pd.DataFrame): The scaled DataFrame.\n",
    "\n",
    "    Returns:\n",
    "        int: The optimal number of clusters.\n",
    "    \"\"\"\n",
    "    scores = {}\n",
    "    for n_clusters in range(3, 30, 3):\n",
    "        kmeans = KMeans(n_clusters=n_clusters, random_state=0).fit(df_scaled)\n",
    "        scores[n_clusters] = sum((\n",
    "            silhouette_score(df_scaled, kmeans.labels_),\n",
    "            calinski_harabasz_score(df_scaled, kmeans.labels_),\n",
    "            davies_bouldin_score(df_scaled, kmeans.labels_)\n",
    "        ))\n",
    "    return max(scores, key=scores.get)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cluster the Companies (KMeans)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cluster_companies(df_data: pd.DataFrame, min_size: int = 15) -> pd.DataFrame:\n",
    "    \"\"\"Cluster the companies based on all the data into the optimal number of\n",
    "    clusters, but every cluster has at least `min_size` companies.\n",
    "\n",
    "    Args:\n",
    "        df_data (pd.DataFrame): The input DataFrame.\n",
    "        min_size (int, optional): The minimum number of companies per cluster.\n",
    "            Defaults to 15.\n",
    "\n",
    "    Returns:\n",
    "        pd.DataFrame: The input DataFrame with an additional column 'cluster'\n",
    "        indicating the cluster number for each company.\n",
    "    \"\"\"\n",
    "    # Scale the data\n",
    "    df_scaled = scale_data(df_data)\n",
    "\n",
    "    # Find the optimal number of clusters\n",
    "    optimal_clusters = find_optimal_clusters(df_scaled)\n",
    "\n",
    "    # Perform the clustering\n",
    "    kmeans = KMeans(n_clusters=optimal_clusters, random_state=0).fit(df_scaled)\n",
    "    df_data['cluster'] = kmeans.labels_\n",
    "\n",
    "    # Filter out clusters with less than `min_size` companies\n",
    "    df_data['cluster_size'] = df_data.groupby('cluster').transform('count')['id']\n",
    "    df_data = df_data[df_data['cluster_size'] >= min_size]\n",
    "    return df_data\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### LetÂ´s define our main models\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "VAR (Vektor Auto Regression)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def VAR_model(X, p):\n",
    "    \"\"\"\n",
    "    This function takes a matrix X and a number of lags p and performs a VAR(p) model on the data.\n",
    "    It returns the mean squared error and the R2 score, the matrix anf the coefficients.\n",
    "    \n",
    "    \"\"\"\n",
    "    n_samples, n_features = X.shape\n",
    "\n",
    "    X_lagged = np.zeros((n_samples - p, p * n_features))\n",
    "    for i in range(p):\n",
    "        X_lagged[:, i*n_features:(i+1)*n_features] = X[p-i-1:-i-1, :]\n",
    "\n",
    "    # Split the data into training and testing sets\n",
    "    train_size = int(0.8 * n_samples)\n",
    "    X_train = X_lagged[:train_size, :]\n",
    "    Y_train = X[p:train_size+p, :]\n",
    "    X_test = X_lagged[train_size-p:-p, :]\n",
    "    Y_test = X[train_size+p:, :]\n",
    "    print(X_train.shape, Y_train.shape, X_test.shape, Y_test.shape)\n",
    "    # Compute the coefficients using the training set\n",
    "    coeffs = np.linalg.inv(X_train.T @ X_train) @ X_train.T @ Y_train\n",
    "\n",
    "    # make predictions for the test set\n",
    "    Y_pred = X_test @ coeffs\n",
    "\n",
    "    # calculate the mean squared error\n",
    "    mse = np.mean((Y_test - Y_pred)**2)\n",
    "    #calculate the R2 score\n",
    "    r2 = 1 - np.sum((Y_test - Y_pred)**2) / np.sum((Y_test - np.mean(Y_test))**2)\n",
    "\n",
    "    \n",
    "    #return the mean squared error and the R2 score, the matrix anf the coefficients\n",
    "    return mse, r2, X_lagged, coeffs"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Granger Test"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First a simple Granger test to analyze fast"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_granger_simple(df_data: pd.DataFrame, p: int, column_A: str, column_B: str) -> tuple:\n",
    "    \"\"\"\n",
    "    Check for Granger causality between two variables, given a pandas DataFrame with columns 'Company Name', 'Period',\n",
    "    'Return', and 'ESG Score'.\n",
    "\n",
    "    Parameters:\n",
    "    df_data (pd.DataFrame): A pandas DataFrame with columns 'Company Name', 'Period', 'Return', and 'ESG Score'.\n",
    "    p (int): The number of lags to include in the VAR models.\n",
    "    column_A (str): The name of the first column to test for Granger causality.\n",
    "    column_B (str): The name of the second column to test for Granger causality.\n",
    "\n",
    "    Returns:\n",
    "    A tuple containing the residuals, y, and coefficients for each of the three VAR models, as well as the R-squared\n",
    "    values, F-statistics, and critical values for the Granger causality tests.\n",
    "    \"\"\"\n",
    "\n",
    "    # Create a copy of the input dataframe to avoid modifying the original\n",
    "    df_data_old = df_data.copy()\n",
    "\n",
    "    # Initialize an empty dataframe with the same columns as the input dataframe\n",
    "    df_data = pd.DataFrame(columns=df_data.columns)\n",
    "\n",
    "    # Iterate through all unique company names in the input dataframe\n",
    "    for company in df_data_old[\"Company Name\"].unique():\n",
    "        X_company = df_data_old[df_data_old[\"Company Name\"] == company]\n",
    "\n",
    "        # Only process companies with more than 5 data points\n",
    "        if X_company.shape[0] > 5:\n",
    "            # Add 'p' rows of zeros to the beginning of each company's data\n",
    "            X_c = pd.DataFrame(columns=df_data.columns)\n",
    "            X_c = X_c.append([X_company.iloc[0]] * p, ignore_index=True)\n",
    "            X_c = X_c * 0\n",
    "            X_c[\"Period\"] = list(range(-p, 0))\n",
    "            X_c = X_c.append(X_company, ignore_index=True)\n",
    "            df_data = df_data.append(X_c, ignore_index=True)\n",
    "\n",
    "    # Initialize the VAR models using different predictors\n",
    "    # Model 1: column_A\n",
    "    # Model 2: column_A, random noise\n",
    "    # Model 3: column_A, column_B\n",
    "    X1 = df_data[[column_A]].copy()\n",
    "    X2 = df_data[[column_A]].copy()\n",
    "    X2[\"random\"] = np.random.rand(len(X2))\n",
    "    X3 = df_data[[column_A, column_B]].copy()\n",
    "\n",
    "    # Convert dataframes to numpy arrays\n",
    "    X1, X2, X3 = X1.values, X2.values, X3.values\n",
    "\n",
    "    # Fit the VAR models with 'p' lags\n",
    "    mse1, r21, X_lagged1, coeffs_1 = VAR_model(X1, p)\n",
    "    mse2, r22, X_lagged2, coeffs_2 = VAR_model(X2, p)\n",
    "    mse3, r23, X_lagged3, coeffs_3 = VAR_model(X3, p)\n",
    "\n",
    "    # Calculate the residuals for each model\n",
    "    y = df_data[column_A].values[p:]\n",
    "    residuals_1 = y - (X_lagged1 @ coeffs_1)[:, 0]\n",
    "    residuals_2 = y - (X_lagged2 @ coeffs_2)[:, 0]\n",
    "    residuals_3 = y - (X_lagged3 @ coeffs_3)[:, 0]\n",
    "\n",
    "    # Define a function to calculate R-squared from residuals\n",
    "    def r2_from_residuals(y,residuals):\n",
    "        \"\"\"\n",
    "        Calculate the R-squared value from the residuals of a model.\n",
    "\n",
    "        Parameters:\n",
    "        y (numpy.ndarray): The dependent variable.\n",
    "        residuals (numpy.ndarray): The residuals of the model.\n",
    "\n",
    "        Returns:\n",
    "        The R-squared value.\n",
    "        \"\"\"\n",
    "        SST = sum((y - np.mean(y)) ** 2)\n",
    "        SSR = sum(residuals ** 2)\n",
    "        R2 = 1 - (SSR / SST)\n",
    "        return R2\n",
    "\n",
    "    # Print R-squared values for each model\n",
    "    print(f\"R-squared (Model 1): {r2_from_residuals(y, residuals_1)}\")\n",
    "    print(f\"R-squared (Model 2): {r2_from_residuals(y, residuals_2)}\")\n",
    "    print(f\"R-squared (Model 3): {r2_from_residuals(y, residuals_3)}\")\n",
    "\n",
    "    # Perform F-tests to compare residuals between models\n",
    "    F12 = calculate_f_statistic(residuals_1, residuals_2)\n",
    "    F13 = calculate_f_statistic(residuals_1, residuals_3)\n",
    "    print(f\"F-statistic (Model 1 vs Model 2): {F12}\")\n",
    "    print(f\"F-statistic (Model 1 vs Model 3): {F13}\")\n",
    "\n",
    "    critical_value12 = calculate_critical_value(residuals_1, residuals_2, alpha=0.05)\n",
    "    critical_value13 = calculate_critical_value(residuals_1, residuals_3, alpha=0.05)\n",
    "    print(f\"Critical value (Model 1 vs Model 2): {critical_value12}\")\n",
    "    print(f\"Critical value (Model 1 vs Model 3): {critical_value13}\")\n",
    "\n",
    "    # Compare F-statistics to critical values\n",
    "    compare_f_statistic_to_critical_value(F12, critical_value12)\n",
    "    compare_f_statistic_to_critical_value(F13, critical_value13)\n",
    "\n",
    "    # Return the residuals, y, and coefficients for each model\n",
    "    return (residuals_1, residuals_2, residuals_3, y,\n",
    "            coeffs_1, coeffs_2, coeffs_3)\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analyze"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualize"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ESG by Country"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_esg_map(dataframe, country_col='Country of Headquarters', esg_score_col='ESG Score',download=True\n",
    "                   ):\n",
    "    \"\"\"\n",
    "    Create a map of the world with ESG scores for each country. The ESG scores are standardized to have a mean of 0 and a standard deviation of 1.\n",
    "    input:\n",
    "        dataframe: A dataframe with a column containing the country names and a column containing the ESG scores.\n",
    "        country_col: The name of the column containing the country names.\n",
    "        esg_score_col: The name of the column containing the ESG scores.\n",
    "        download: A boolean indicating whether to download the map as an HTML file or to display it in the notebook.\n",
    "    output:\n",
    "        A map of the world with ESG scores for each country.(HTML file or displayed in the notebook)\n",
    "    \"\"\"\n",
    "    \n",
    "    world = gpd.read_file(gpd.datasets.get_path('naturalearth_lowres'))\n",
    "    merged_data = world.merge(dataframe, left_on='name', right_on=country_col, how='left')\n",
    "    merged_data = merged_data[merged_data[esg_score_col].notna()]\n",
    "\n",
    "    # Standardize the ESG scores\n",
    "    scaler = StandardScaler()\n",
    "    merged_data['Standardized ESG Score'] = scaler.fit_transform(merged_data[[esg_score_col]])\n",
    "\n",
    "    fig = px.choropleth(merged_data, geojson=merged_data.geometry, locations=merged_data.index,\n",
    "                        color='Standardized ESG Score', \n",
    "                        range_color=(merged_data['Standardized ESG Score'].min(), merged_data['Standardized ESG Score'].max()),\n",
    "                        projection='natural earth', hover_name='name', hover_data=[esg_score_col],\n",
    "                        labels={'Standardized ESG Score': 'Standardized ESG Score'})\n",
    "\n",
    "    fig.update_geos(showcountries=True, countrywidth=0.5)\n",
    "    fig.update_layout(title_text='Standardized ESG Score of Companies by Country', title_x=0.5)\n",
    "    if download:\n",
    "        fig.write_html(\"esg_map.html\")\n",
    "    else:\n",
    "        fig.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plot the Clusters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_clusters(df, labels, esg_col='ESG Score', return_col='Return'\n",
    "                  ):\n",
    "    \"\"\"\n",
    "    Plot the clusters of a dataframe based on cluster labels.\n",
    "\n",
    "    Args:\n",
    "        df (pd.DataFrame): The dataframe to plot.\n",
    "        labels (np.ndarray): The cluster labels of each row in the dataframe.\n",
    "    \"\"\"\n",
    "\n",
    "    # Create a scatter plot of \"ESG Score\" vs \"Return\" and use cluster labels to define the colors\n",
    "    sns.scatterplot(x=esg_col, y=return_col, hue=labels, data=df)\n",
    "    plt.legend(bbox_to_anchor=(1.05, 1), loc=2, borderaxespad=0.)\n",
    "    plt.show()\n",
    "\n",
    "    # Print the histogram of the cluster labels\n",
    "    sns.histplot(labels)\n",
    "    plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plot the ESG distribution per Cluster"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TEST CODE"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RUN CODE"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### First, we get the Data and clean it"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get the Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|ââââââââââ| 228/228 [00:02<00:00, 89.74it/s]\n"
     ]
    }
   ],
   "source": [
    "#let's download the all files we need \n",
    "for key, value in config.DICT_URL.items():\n",
    "    #check if the file is already downloaded\n",
    "    if not os.path.exists(config.LOCAL_FOLDER + key):\n",
    "        download_excel_file(value, config.LOCAL_FOLDER + key)\n",
    "        print(\"Downloaded file: \" + key)\n",
    "    \n",
    "df_individual_scores_10Y = refinite_to_python(config.LOCAL_FOLDER + \"ESG-individual_scores_10Y.xlsx\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Calculate the optimal number of clusters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Calculate the optimal number of clusters\n",
    "optimal_n_clusters = calculate_optimal_n_clusters(df_individual_scores_10Y, config.ESG_SCORES, config.RETURN)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Clean it and add clusters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/sklearn/cluster/_kmeans.py:870: FutureWarning: The default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "#first, lets rename the Return column \n",
    "df_individual_scores_10Y.rename(columns={'YTD Total Return':'Return'}, inplace=True)\n",
    "df_individual_scores_10Y_clean = clean_data(df_individual_scores_10Y)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Now we start with some basic data exploration and visualization "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> ESG by country"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "create_esg_map(df_individual_scores_10Y_clean)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ESG by Cluster"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
