{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preparation"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Module imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import dropbox\n",
    "import config\n",
    "import requests\n",
    "import os\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "import pandas as pd\n",
    "import geopandas as gpd\n",
    "import plotly.express as px\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.cluster import KMeans\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.metrics import silhouette_score, calinski_harabasz_score, davies_bouldin_score\n",
    "import scipy\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#global variables\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Methods"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Lets gather the data"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First we import the data form our local folder or from dropbox"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def download_excel_file(url, output_file):\n",
    "    \"\"\"\n",
    "    Download an Excel file from a URL and save it locally.\n",
    "    \"\"\"\n",
    "    # Modify the Dropbox URL to force download\n",
    "    download_url = url.replace(\"?dl=0\", \"?dl=1\")\n",
    "\n",
    "    # Send an HTTP GET request to download the file\n",
    "    response = requests.get(download_url)\n",
    "\n",
    "    # Check if the request was successful (HTTP status code 200)\n",
    "    if response.status_code == 200:\n",
    "        # Create the output folder if it does not exist\n",
    "        output_folder = os.path.dirname(output_file)\n",
    "        if output_folder and not os.path.exists(output_folder):\n",
    "            os.makedirs(output_folder)\n",
    "\n",
    "        # Save the file in the output folder\n",
    "        with open(output_file, \"wb\") as file:\n",
    "            file.write(response.content)\n",
    "    else:\n",
    "        print(\"Error: Unable to download the file\")\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the data generation via yahoo please see the Deprecated folder\n",
    "!Warning! it is highly unstructured code, since it was not used for any insights"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets import some Data from refinitive. Since the data validity is not given with the beta version of the refinitive Python Api. Anyways, hence it may help some people, here is the function (to get data without the API-KEY). It is not called, but the data can be downloaded with the function call. It is important to have a open session of refinitive workspace running on the PC. More methods can be found in the Deprecated folder. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_how_to_get_data_from_refinitive():\n",
    "    \"\"\"\n",
    "    example function to show how to get data from refinitive. Just to help if needed, absolutely no guarantee that it still works\n",
    "    \"\"\"\n",
    "    \n",
    "    #wee need this module to get the data from refinitive (still, eikon is also possible, but does not work well)\n",
    "    import refinitiv.data as rd\n",
    "    #if you want to use eikon, import it like this\n",
    "    #import refinitiv.data.eikon as ek\n",
    "    \n",
    "    import Deprecated.refinitive_fields\n",
    "    #in the ressource_use_fields.py file, we lists with all the fields we can use\n",
    "    #each key represents the categorie\n",
    "    import Deprecated.refinitive_fields as refinitive_fields\n",
    "    #let us import all the reccources use fields of the ESG\n",
    "    list_ressource_use_fields = refinitive_fields.ressource_use_fields\n",
    "    \n",
    "    #first we have to open a session\n",
    "    rd.open_session()\n",
    "    #now we just makeour query\n",
    "    #universe is the ticker of the company(ies), fields are the fields we want to get, start and end are the start and end date of the data, interval is the interval of the data\n",
    "    #to use eikon\n",
    "    df = rd.get_history(\n",
    "        universe = ['MBGn.DE'],\n",
    "        fields = list_ressource_use_fields,\n",
    "        start = \"2000-01-01\",\n",
    "        end = \"2022-12-31\",\n",
    "        interval='yearly',\n",
    "    )\n",
    "    return df"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Let´s clean the Data"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Make the data readable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def refinite_to_python(file_path):\n",
    "    \"\"\"\n",
    "    Make a file from the refinitive screener readable for python and useable for the analysis.\n",
    "    input: file_path\n",
    "    output: cleaned dataframe\n",
    "    \"\"\"\n",
    "    \n",
    "    # Read data from excel file\n",
    "    df_all_comp_all = pd.read_excel(file_path, header=[0, 1])\n",
    "\n",
    "    # Combine multilevel columns into a single level\n",
    "    df_all_comp_all.columns = ['_'.join(col).strip() for col in df_all_comp_all.columns.values]\n",
    "\n",
    "    # Rename columns for better understanding\n",
    "    if \"Company Name_Unnamed: 1_level_1\" in df_all_comp_all.columns:\n",
    "        df_all_comp_all.rename(columns={\"Company Name_Unnamed: 1_level_1\": \"Company Name\"}, inplace=True)\n",
    "    if \"Identifier (RIC)_Unnamed: 0_level_1\" in df_all_comp_all.columns:\n",
    "        df_all_comp_all.rename(columns={\"Identifier (RIC)_Unnamed: 0_level_1\": \"RIC\"}, inplace=True)\n",
    "    if \"Country of Headquarters_Unnamed: 2_level_1\" in df_all_comp_all.columns:\n",
    "        df_all_comp_all.rename(columns={\"Country of Headquarters_Unnamed: 2_level_1\": \"Country of Headquarters\"}, inplace=True)\n",
    "    if \"Country of Incorporation_Unnamed: 3_level_1\" in df_all_comp_all.columns:\n",
    "        df_all_comp_all.rename(columns={\"NAICS Subsector Name_Unnamed: 5_level_1\": \"NAICS Subsector Name\"}, inplace=True)\n",
    "\n",
    "\n",
    "    # Remove unwanted characters from column names\n",
    "    df_all_comp_all.columns = df_all_comp_all.columns.str.replace('\\nIn the last 10 FY_FY', ' ')\n",
    "    df_all_comp_all.columns = df_all_comp_all.columns.str.replace(\"\\nIn the last 15 Y_Y\",\" \")\n",
    "    df_all_comp_all.columns = df_all_comp_all.columns.str.replace('-', '')\n",
    "    \n",
    "\n",
    "    # Create a copy of the DataFrame\n",
    "    df_all_comp = df_all_comp_all.copy()\n",
    "    #return df_all_comp\n",
    "    #check if the YTD Total Return 11, 12, 13, 14 are in the dataframe\n",
    "    if \"YTD Total Return 11\" in df_all_comp.columns:  \n",
    "        df_all_comp.drop(columns=[\"YTD Total Return 11\"], inplace=True)\n",
    "    if \"YTD Total Return 12\" in df_all_comp.columns:\n",
    "        df_all_comp.drop(columns=[\"YTD Total Return 12\"], inplace=True)  \n",
    "    if \"YTD Total Return 13\" in df_all_comp.columns:\n",
    "        df_all_comp.drop(columns=[\"YTD Total Return 13\"], inplace=True)    \n",
    "    if \"YTD Total Return 14\" in df_all_comp.columns:\n",
    "        df_all_comp.drop(columns=[\"YTD Total Return 14\"], inplace=True)\n",
    "\n",
    "\n",
    "    # Drop rows with all NaN values and fill remaining NaNs with 0\n",
    "    df_all_comp.dropna(inplace=True, how=\"all\")\n",
    "    df_all_comp.fillna(0, inplace=True)\n",
    "\n",
    "\n",
    "    # Extract unique column prefixes\n",
    "    list_columns = [col for col in df_all_comp.columns if col[-1].isdigit()]\n",
    "    list_columns = [col[:-1] for col in list_columns]\n",
    "    list_columns = [col[:-1] if col[-1].isdigit() else col for col in list_columns]\n",
    "    list_columns = list(set(list_columns))\n",
    "    list_columns = [col.replace(\"\\n\", \"\") for col in list_columns]\n",
    "\n",
    "    # Initialize an empty list to store DataFrames\n",
    "    dfs = []\n",
    "\n",
    "    # Iterate over unique column prefixes\n",
    "    for colum in tqdm(list_columns):\n",
    "        esg_cols = [col for col in df_all_comp.columns if col.startswith(colum)]\n",
    "        df = df_all_comp[[\"Company Name\"] + esg_cols].copy()\n",
    "\n",
    "        # Melt the DataFrame to transform it into the desired format\n",
    "        melted_df = df.melt(\n",
    "            id_vars=[\"Company Name\"],\n",
    "            var_name=\"Period\",\n",
    "            value_name=colum\n",
    "        )\n",
    "        melted_df[\"Period\"] = melted_df[\"Period\"].apply(lambda x: 1 if x[-1].isdigit() == False else 10 if x[-2:-1] == \"10\" else int(x[-1]))\n",
    "\n",
    "        dfs.append(melted_df)\n",
    "\n",
    "    # Concatenate the melted DataFrames\n",
    "    melted_df_all = pd.concat(dfs, axis=1)\n",
    "\n",
    "    # Remove duplicate columns\n",
    "    melted_df_all = melted_df_all.loc[:, ~melted_df_all.columns.duplicated()]\n",
    "\n",
    "    # Sort the DataFrame by Company Name and Period\n",
    "    melted_df_all = melted_df_all.sort_values(by=[\"Company Name\", \"Period\"])\n",
    "\n",
    "    # Create a copy of the final DataFrame\n",
    "    df_data_ = melted_df_all.copy()\n",
    "    \n",
    "    #replace all 0 with nan, except for the period column\n",
    "    #it is important to diffirentiate between 0 and nan, for example for the correlation\n",
    "    df_data_ = df_data_.replace(0, np.nan)\n",
    "    df_data_ = df_data_.replace(\"0\", np.nan)\n",
    "    df_data_[\"Period\"] = df_data_[\"Period\"].fillna(0)\n",
    "\n",
    "    #add RIC, Country of Headquarters and NAICS Subsector Name to the dataframe\n",
    "    #fill nan of RIC, Country of Headquarters and NAICS Subsector Name with the last value that is not nan\n",
    "    if \"RIC\" in df_data_.columns:\n",
    "        df_data_[\"RIC\"] = df_all_comp_all[\"RIC\"]\n",
    "        df_data_[\"RIC\"] = df_data_.groupby(\"Company Name\")[\"RIC\"].ffill()   \n",
    "    if \"Country of Headquarters\" in df_data_.columns:\n",
    "        df_data_[\"Country of Headquarters\"] = df_all_comp_all[\"Country of Headquarters\"]\n",
    "        df_data_[\"Country of Headquarters\"] = df_data_.groupby(\"Company Name\")[\"Country of Headquarters\"].ffill()            \n",
    "    if \"NAICS Subsector Name\" in df_data_.columns:\n",
    "        df_data_[\"NAICS Subsector Name\"] = df_all_comp_all[\"NAICS Subsector Name\"]\n",
    "        df_data_[\"NAICS Subsector Name\"] = df_data_.groupby(\"Company Name\")[\"NAICS Subsector Name\"].ffill()   \n",
    "\n",
    "    #remove all the whitespaces in the end of a column name\n",
    "    df_data_.columns = df_data_.columns.str.rstrip()\n",
    "        \n",
    "    if \"ESG Score \" in df_data_.columns:\n",
    "        #make sure only one ESG Score column is in the dataframe\n",
    "        df_data_[\"ESG Score\"] = df_data_[\"ESG Score \"]\n",
    "\n",
    "    \n",
    "    return df_data_\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Clean the data and add some clusters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_data(df, esg_col = \"ESG Score\", return_col = \"Return\"):\n",
    "    \"\"\"\n",
    "    Clean and preprocess a dataframe for clustering based on \"ESG Score\" and \"Return\" columns.\n",
    "\n",
    "    Args:\n",
    "        df (pd.DataFrame): The input dataframe to clean.\n",
    "        esg_col (str): The column name for the ESG Score.\n",
    "        return_col (str): The column name for the Return.\n",
    "\n",
    "    Returns:\n",
    "        pd.DataFrame: The cleaned and preprocessed dataframe.\n",
    "    \"\"\"\n",
    "\n",
    "    # Make a copy of the input dataframe to avoid modifying it\n",
    "    df_data = df.copy()\n",
    "\n",
    "    # Drop rows with NaN values in \"Return\" or \"ESG Score\" columns\n",
    "    df_data.dropna(subset=[return_col, esg_col], inplace=True)\n",
    "\n",
    "    # Remove outliers in the \"Return\" column (more than 2 std away from the mean)\n",
    "    return_mean = df_data[return_col].mean()\n",
    "    return_std = df_data[return_col].std()\n",
    "    df_data = df_data[(df_data[return_col] < return_mean + 2 * return_std) & (df_data[return_col] > return_mean - 2 * return_std)]\n",
    "\n",
    "    # Rescale the \"ESG Score\" and \"Return\" columns to be between 0 and 1\n",
    "    df_data[esg_col] = df_data[esg_col] / df_data[esg_col].max()\n",
    "    df_data[return_col] = df_data[return_col] / df_data[return_col].max()\n",
    "\n",
    "    # Cluster the companies based on \"ESG Score\" and \"Return\" columns using KMeans\n",
    "    model = KMeans(n_clusters=6)\n",
    "    model.fit(df_data[[esg_col, return_col]])\n",
    "    labels = model.predict(df_data[[esg_col, return_col]])\n",
    "\n",
    "    # Return the cleaned dataframe\n",
    "    return df_data"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Add Transformation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def transform_columns(df,column):\n",
    "    \"\"\"\n",
    "    takes a dataframe and a column name and transforms the column into:\n",
    "    log\n",
    "    log_2\n",
    "    sqrt\n",
    "    sqrt_2\n",
    "    _2\n",
    "    _3\n",
    "    _4\n",
    "    \"\"\"\n",
    "    \n",
    "    df[column+\"_log\"] = np.log(df[column])\n",
    "    df[column+\"_log_2\"] = np.log(df[column]**2)\n",
    "    df[column+\"_sqrt\"] = np.sqrt(df[column])\n",
    "    df[column+\"_sqrt_2\"] = np.sqrt(df[column]**2)\n",
    "    df[column+\"_2\"] = df[column]**2\n",
    "    df[column+\"_3\"] = df[column]**3\n",
    "    df[column+\"_4\"] = df[column]**4\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Scale the Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "def scale_data(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"Scale the numerical columns of the DataFrame.\n",
    "\n",
    "    Args:\n",
    "        df (pd.DataFrame): The input DataFrame.\n",
    "\n",
    "    Returns:\n",
    "        pd.DataFrame: The scaled DataFrame.\n",
    "    \"\"\"\n",
    "    numerical_columns = df.select_dtypes(include=['float64', 'int64']).columns\n",
    "    if len(numerical_columns) == 0:\n",
    "        raise ValueError(\"No numerical columns in DataFrame\")\n",
    "    scaler = StandardScaler()\n",
    "    df_scaled = scaler.fit_transform(df[numerical_columns])\n",
    "    df_scaled = pd.DataFrame(df_scaled, columns=numerical_columns, index=df.index)\n",
    "    return pd.concat([df.drop(columns=numerical_columns), df_scaled], axis=1)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lets dive deep in the data"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MATH Formulas"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Let´s define our basic math functions \n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Gradient Descent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def initialize_coefficients(n_features, n_targets):\n",
    "    \"\"\"\n",
    "    Initializes the coefficients for VAR.\n",
    "    \n",
    "    Parameters:\n",
    "        n_features (int): Number of input features.\n",
    "        n_targets (int): Number of output targets.\n",
    "        \n",
    "    Returns:\n",
    "        numpy.ndarray: An (n_features x n_targets) matrix of initialized coefficients.\n",
    "    \"\"\"\n",
    "    return np.random.rand(n_features, n_targets)\n",
    "\n",
    "def calculate_loss(X, Y, coefficients):\n",
    "    \"\"\"\n",
    "    Calculates the mean squared error (MSE) for the current coefficients.\n",
    "    \n",
    "    Parameters:\n",
    "        X (numpy.ndarray): An (n_samples x n_features) array of input data.\n",
    "        Y (numpy.ndarray): An (n_samples x n_targets) array of output data.\n",
    "        coefficients (numpy.ndarray): An (n_features x n_targets) matrix of current coefficients.\n",
    "        \n",
    "    Returns:\n",
    "        float: The mean squared error (MSE) for the current coefficients.\n",
    "    \"\"\"\n",
    "    predicted = np.dot(X, coefficients)\n",
    "    errors = Y - predicted\n",
    "    return np.sum(errors**2) / (2 * X.shape[0])\n",
    "\n",
    "def calculate_lasso_loss(_X, _Y, coefficients, lambda_):\n",
    "    \"\"\"\n",
    "    Calculates the LASSO loss for the current coefficients.\n",
    "\n",
    "    Parameters:\n",
    "        X (numpy.ndarray): An (n_samples x n_features) array of input data.\n",
    "        Y (numpy.ndarray): An (n_samples x n_targets) array of output data.\n",
    "        coefficients (numpy.ndarray): An (n_features x n_targets) matrix of current coefficients.\n",
    "        lambda_ (float): The regularization parameter.\n",
    "\n",
    "    Returns:\n",
    "        float: The LASSO loss for the current coefficients.\n",
    "    \"\"\"\n",
    "    predicted = np.dot(_X, coefficients)\n",
    "    errors = _Y - predicted\n",
    "    mse = np.sum(errors**2) / (2 * _X.shape[0])\n",
    "    l1_norm = np.sum(np.abs(coefficients))\n",
    "    lasso_loss = mse + lambda_ * l1_norm\n",
    "    return lasso_loss\n",
    "\n",
    "\n",
    "def outer(a, b):\n",
    "    \"\"\"\n",
    "    Computes the outer product of two 1-dimensional arrays.\n",
    "\n",
    "    Parameters:\n",
    "        a (array-like): 1-dimensional array.\n",
    "        b (array-like): 1-dimensional array.\n",
    "\n",
    "    Returns:\n",
    "        2-dimensional array where the element at position (i, j) is the\n",
    "        product of the i-th element of `a` and the j-th element of `b`.\n",
    "    \"\"\"\n",
    "    outer_product = np.zeros((len(a), len(b)))  # initialize the result with zeros\n",
    "    for i in range(len(a)):\n",
    "        for j in range(len(b)):\n",
    "            outer_product[i, j] = a[i] * b[j]  # compute the product of the i-th element of a and the j-th element of b\n",
    "    return outer_product\n",
    "\n",
    "\n",
    "\n",
    "def calculate_gradient(X, Y, coefficients):\n",
    "    \"\"\"\n",
    "    Calculates the gradient of the loss function with respect to the coefficients.\n",
    "    \n",
    "    Parameters:\n",
    "        X (numpy.ndarray): An (n_samples x n_features) array of input data.\n",
    "        Y (numpy.ndarray): An (n_samples x n_targets) array of output data.\n",
    "        coefficients (numpy.ndarray): An (n_features x n_targets) matrix of current coefficients.\n",
    "        \n",
    "    Returns:\n",
    "        numpy.ndarray: An (n_features x n_targets) matrix of gradient values.\n",
    "    \"\"\"\n",
    "    n_samples = X.shape[0]\n",
    "    #without Lasso\n",
    "    #grad = np.dot(X.T, np.dot(X, coefficients) - Y) / n_samples\n",
    "    grad = (1/n_samples) * np.dot(X.T, np.dot(X, coefficients) - Y) + 0.2 * np.sign(coefficients)  \n",
    "    return grad\n",
    "\n",
    "def update_coefficients(coefficients, gradients, lr):\n",
    "    \"\"\"\n",
    "    Updates the coefficients using the gradient and the learning rate.\n",
    "    \n",
    "    Parameters:\n",
    "        coefficients (numpy.ndarray): An (n_features x n_targets) matrix of current coefficients.\n",
    "        gradients (numpy.ndarray): An (n_features x n_targets) matrix of gradient values.\n",
    "        lr (float): Learning rate for the optimization algorithm.\n",
    "        \n",
    "    Returns:\n",
    "        numpy.ndarray: An (n_features x n_targets) matrix of updated coefficients.\n",
    "    \"\"\"\n",
    "    return coefficients - lr * gradients\n",
    "\n",
    "def gradient_descent_step(X__, Y__, coefficients__, lr):\n",
    "    \"\"\"\n",
    "    Performs a single step of gradient descent for VAR.\n",
    "    \n",
    "    Parameters:\n",
    "        X (numpy.ndarray): An (n_samples x n_features) array of input data.\n",
    "        Y (numpy.ndarray): An (n_samples x n_targets) array of output data.\n",
    "        coefficients (numpy.ndarray): An (n_features x n_targets) matrix of current coefficients.\n",
    "        lr (float): Learning rate for the optimization algorithm.\n",
    "        \n",
    "    Returns:\n",
    "        tuple: A tuple containing the updated coefficients and the loss for the current iteration.\n",
    "    \"\"\"\n",
    "    lambda_=1#variieren mit cosine similarity\n",
    "    gradients = calculate_gradient(X__, Y__, coefficients__)\n",
    "    updated_coefficients = update_coefficients(coefficients__, gradients, lr)\n",
    "    loss = calculate_lasso_loss(X__, Y__, coefficients__, lambda_)\n",
    "    return updated_coefficients, loss\n",
    "\n",
    "def calculate_gradient(X_, Y_, coefficients_):\n",
    "    \"\"\"\n",
    "    Calculates the gradient of the loss function with respect to the coefficients.\n",
    "    \n",
    "    Parameters:\n",
    "        X (numpy.ndarray): An (n_samples x n_features) array of input data.\n",
    "        Y (numpy.ndarray): An (n_samples x n_targets) array of output data.\n",
    "        coefficients (numpy.ndarray): An (n_features x n_targets) matrix of current coefficients.\n",
    "        \n",
    "    Returns:\n",
    "        numpy.ndarray: An (n_features x n_targets) matrix of gradient values.\n",
    "    \"\"\"\n",
    "    gradients = np.zeros_like(coefficients_)\n",
    "    for i in range(X_.shape[0]):\n",
    "        xi = X_[i, :]\n",
    "        yi = Y_[i, :]\n",
    "        predicted = np.dot(xi, coefficients_)\n",
    "        error = yi - predicted\n",
    "        gradients += outer(xi, error)\n",
    "    gradients /= X_.shape[0]\n",
    "    return gradients\n",
    "import numpy as np"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "F-Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_f_statistic(sample1, sample2):\n",
    "    \"\"\"\n",
    "    Calculates the F-statistic for two samples.\n",
    "    \n",
    "    Parameters:\n",
    "        sample1 (numpy.ndarray): An array of values for the first sample.\n",
    "        sample2 (numpy.ndarray): An array of values for the second sample.\n",
    "        \n",
    "    Returns:\n",
    "        float: The F-statistic for the two samples.\n",
    "    \"\"\"\n",
    "    var1 = np.var(sample1)\n",
    "    var2 = np.var(sample2)\n",
    "    return var1 / var2\n",
    "\n",
    "def calculate_critical_value(sample1, sample2, alpha):\n",
    "    \"\"\"\n",
    "    Calculates the critical value of the F-distribution for two samples.\n",
    "    \n",
    "    Parameters:\n",
    "        sample1 (numpy.ndarray): An array of values for the first sample.\n",
    "        sample2 (numpy.ndarray): An array of values for the second sample.\n",
    "        alpha (float): The significance level.\n",
    "        \n",
    "    Returns:\n",
    "        float: The critical value of the F-distribution for the two samples.\n",
    "    \"\"\"\n",
    "    df1 = len(sample1) - 1\n",
    "    df2 = len(sample2) - 1\n",
    "    return f.ppf(q=1-alpha, dfn=df1, dfd=df2)\n",
    "\n",
    "def compare_f_statistic_to_critical_value(f_statistic, critical_value):\n",
    "    \"\"\"\n",
    "    Compares the F-statistic to the critical value of the F-distribution.\n",
    "    \n",
    "    Parameters:\n",
    "        f_statistic (float): The F-statistic for the two samples.\n",
    "        critical_value (float): The critical value of the F-distribution for the two samples.\n",
    "    \"\"\"\n",
    "    if f_statistic > critical_value:\n",
    "        print('Reject the null hypothesis that the variances are equal')\n",
    "        return False\n",
    "    else:\n",
    "        print('Accept the null hypothesis that the variances are equal')\n",
    "        return True"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cosine similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cosine_similarity(v1, v2):\n",
    "    \"\"\"\n",
    "    calculates the cosine similarity between two vectors\n",
    "    input: v1, v2: numpy arrays\n",
    "    output: cosine similarity (float) \n",
    "    \"\"\"\n",
    "    return np.dot(v1, v2) / (np.linalg.norm(v1) * np.linalg.norm(v2))\n",
    "\n",
    "def cosine_similarity_matrix(A,B):\n",
    "    \"\"\"\n",
    "    calculates the cosine similarity similarity between the rows of A and B\n",
    "    \"\"\"\n",
    "    return np.array([cosine_similarity(A[i,:],B[i,:]) for i in range(A.shape[0])])\n",
    "\n",
    "#calculate the cosine similarity for every company to every other company\n",
    "#select only the numerical columns\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lag Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lag_matrix(X,p):\n",
    "    \"\"\"\n",
    "    lags matrix X by p\n",
    "    \"\"\"\n",
    "    X_lagged = np.zeros((X.shape[0]-p,X.shape[1]*p))\n",
    "    for i in range(p):\n",
    "        X_lagged[:,i*X.shape[1]:(i+1)*X.shape[1]] = X[p-i-1:-i-1,:]\n",
    "    return X_lagged"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "calculate optimal number cluster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_optimal_clusters(df_scaled: pd.DataFrame) -> int:\n",
    "    \"\"\"Find the optimal number of clusters using silhouette, calinski_harabasz,\n",
    "    and davies_bouldin scores.\n",
    "\n",
    "    Args:\n",
    "        df_scaled (pd.DataFrame): The scaled DataFrame.\n",
    "\n",
    "    Returns:\n",
    "        int: The optimal number of clusters.\n",
    "    \"\"\"\n",
    "    scores = {}\n",
    "    #fill missing values with 0\n",
    "    df_scaled = df_scaled.fillna(0)\n",
    "    #make sure only numeric columns are used\n",
    "    list_numeric_columns = df_scaled.select_dtypes(include=['float64', 'int64']).columns\n",
    "    df_scaled_num = df_scaled[list_numeric_columns]\n",
    "    \n",
    "    for n_clusters in tqdm(range(3, 100, 3)):\n",
    "        kmeans = KMeans(n_clusters=n_clusters, random_state=0).fit(df_scaled_num)\n",
    "        scores[n_clusters] = sum((\n",
    "            silhouette_score(df_scaled_num, kmeans.labels_),\n",
    "            calinski_harabasz_score(df_scaled_num, kmeans.labels_),\n",
    "            davies_bouldin_score(df_scaled_num, kmeans.labels_)\n",
    "        ))\n",
    "    return max(scores, key=scores.get)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cluster the Companies (KMeans)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cluster_companies(df_data: pd.DataFrame, min_size: int = 15,\n",
    "                      cluster_size: int = 0\n",
    "                      ) -> pd.DataFrame:\n",
    "    \"\"\"Cluster the companies based on all the data into the optimal number of\n",
    "    clusters, but every cluster has at least `min_size` companies.\n",
    "\n",
    "    Args:\n",
    "        df_data (pd.DataFrame): The input DataFrame.\n",
    "        min_size (int, optional): The minimum number of companies per cluster.\n",
    "            Defaults to 15.\n",
    "        cluster_size (int, optional): The number of clusters to use. If 0, the\n",
    "            \n",
    "\n",
    "    Returns:\n",
    "        pd.DataFrame: The input DataFrame with an additional column 'cluster'\n",
    "        indicating the cluster number for each company.\n",
    "    \"\"\"\n",
    "    # Scale the data\n",
    "    df_scaled = scale_data(df_data)\n",
    "\n",
    "    # Find the optimal number of clusters if not provided\n",
    "    if cluster_size == 0:\n",
    "        optimal_clusters = find_optimal_clusters(df_scaled)\n",
    "    else:\n",
    "        optimal_clusters = cluster_size\n",
    "\n",
    "    # Perform the clustering\n",
    "    kmeans = KMeans(n_clusters=optimal_clusters, random_state=0).fit(df_scaled)\n",
    "    df_data['cluster'] = kmeans.labels_\n",
    "\n",
    "    # Filter out clusters with less than `min_size` companies\n",
    "    df_data['cluster_size'] = df_data.groupby('cluster').transform('count')['id']\n",
    "    df_data = df_data[df_data['cluster_size'] >= min_size]\n",
    "    return df_data\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Let´s define our main models\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "VAR (Vektor Auto Regression)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def VAR_model(X, p):\n",
    "    \"\"\"\n",
    "    This function takes a matrix X and a number of lags p and performs a VAR(p) model on the data.\n",
    "    It returns the mean squared error and the R2 score, the matrix anf the coefficients.\n",
    "    \n",
    "    \"\"\"\n",
    "    n_samples, n_features = X.shape\n",
    "\n",
    "    X_lagged = np.zeros((n_samples - p, p * n_features))\n",
    "    for i in range(p):\n",
    "        X_lagged[:, i*n_features:(i+1)*n_features] = X[p-i-1:-i-1, :]\n",
    "\n",
    "    # Split the data into training and testing sets\n",
    "    train_size = int(0.8 * n_samples)\n",
    "    X_train = X_lagged[:train_size, :]\n",
    "    Y_train = X[p:train_size+p, :]\n",
    "    X_test = X_lagged[train_size-p:-p, :]\n",
    "    Y_test = X[train_size+p:, :]\n",
    "    print(X_train.shape, Y_train.shape, X_test.shape, Y_test.shape)\n",
    "    # Compute the coefficients using the training set\n",
    "    coeffs = np.linalg.inv(X_train.T @ X_train) @ X_train.T @ Y_train\n",
    "\n",
    "    # make predictions for the test set\n",
    "    Y_pred = X_test @ coeffs\n",
    "\n",
    "    # calculate the mean squared error\n",
    "    mse = np.mean((Y_test - Y_pred)**2)\n",
    "    #calculate the R2 score\n",
    "    r2 = 1 - np.sum((Y_test - Y_pred)**2) / np.sum((Y_test - np.mean(Y_test))**2)\n",
    "\n",
    "    \n",
    "    #return the mean squared error and the R2 score, the matrix anf the coefficients\n",
    "    return mse, r2, X_lagged, coeffs"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Gradient Descent stepwise approach"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Industry beta on VAR Model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def industry_beta(df_data, coeffs_1, p, cluster_group, column_name = \"cluster\"):\n",
    "    \"\"\"    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    df_data : pd.DataFrame\n",
    "        The unscaled data to which the cluster will be added.\n",
    "    coeffs_1 : np.array\n",
    "        The coefficients of the VAR model.\n",
    "    p : int\n",
    "        The number of lags used in the VAR model.\n",
    "    cluster_group : int or string\n",
    "        The cluster number or the name of the industry.\n",
    "    column_name : string\n",
    "        The name of the column from wich the group is selected.\n",
    "    \n",
    "    Returns:\n",
    "    --------\n",
    "    coeffs_1_new : np.array\n",
    "        The updated coefficients of the VAR model.\n",
    "    \"\"\"\n",
    "    # Add the cluster to the unscaled data\n",
    "    df_data.sort_values(by=[\"Company Name\", \"Period\"], inplace=True)\n",
    "    \n",
    "    # Make a prediction for the cluster based on the VAR model\n",
    "    X_cluster = df_data[df_data[column_name] == cluster_group][[\"Return\", \"ESG Score\",\"Company Name\"]]\n",
    "    \n",
    "    # Add p rows with 0 for every company, so the model can predict the next two values\n",
    "    for company_name in X_cluster[\"Company Name\"].unique():\n",
    "        # Insert two rows with 0 at the beginning of the dataframe\n",
    "        for p_ in range(p):\n",
    "            X_cluster = pd.concat([pd.DataFrame([[0,0,company_name]], columns=[\"Return\", \"ESG Score\",\"Company Name\"]), X_cluster], ignore_index=True)\n",
    "    \n",
    "    # Make the prediction\n",
    "    X_cluster_1 = X_cluster[[\"Return\"]].values\n",
    "    X_cluster_1_lagged = lag_matrix(X_cluster_1, p)\n",
    "    y_pred = X_cluster_1_lagged @ coeffs_1\n",
    "    \n",
    "    # Linear Regression for y = a*(VAR Model) + b\n",
    "    # Create a matrix with y_pred and a column with 1\n",
    "    X = np.column_stack((np.ones(y_pred.shape[0]), y_pred))\n",
    "    \n",
    "    # Fit the model\n",
    "    industry_beta = np.linalg.inv(X.T @ X) @ X.T @ X_cluster_1[p:]\n",
    "    \n",
    "    # Make the prediction\n",
    "    coeffs_1_new = coeffs_1 * industry_beta[0]\n",
    "    \n",
    "    #print the industry beta\n",
    "    print(\"Used the folowing column: {}\".format(column_name))\n",
    "    print(\"The industry beta for cluster {} is {}\".format(cluster_group, industry_beta[1]))\n",
    "    \n",
    "    return coeffs_1_new\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Granger Test"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Granger test based on VAR (simple)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_granger_simple(df_data: pd.DataFrame, p: int, column_A: str, column_B: str) -> tuple:\n",
    "    \"\"\"\n",
    "    Check for Granger causality between two variables, given a pandas DataFrame with columns 'Company Name', 'Period',\n",
    "    'Return', and 'ESG Score'.\n",
    "\n",
    "    Parameters:\n",
    "    df_data (pd.DataFrame): A pandas DataFrame with columns 'Company Name', 'Period', 'Return', and 'ESG Score'.\n",
    "    p (int): The number of lags to include in the VAR models.\n",
    "    column_A (str): The name of the first column to test for Granger causality.\n",
    "    column_B (str): The name of the second column to test for Granger causality.\n",
    "\n",
    "    Returns:\n",
    "    A tuple containing the residuals, y, and coefficients for each of the three VAR models, as well as the R-squared\n",
    "    values, F-statistics, and critical values for the Granger causality tests.\n",
    "    \"\"\"\n",
    "\n",
    "    # Create a copy of the input dataframe to avoid modifying the original\n",
    "    df_data_old = df_data.copy()\n",
    "\n",
    "    # Initialize an empty dataframe with the same columns as the input dataframe\n",
    "    df_data = pd.DataFrame(columns=df_data.columns)\n",
    "\n",
    "    # Iterate through all unique company names in the input dataframe\n",
    "    for company in df_data_old[\"Company Name\"].unique():\n",
    "        X_company = df_data_old[df_data_old[\"Company Name\"] == company]\n",
    "\n",
    "        # Only process companies with more than 5 data points\n",
    "        if X_company.shape[0] > 5:\n",
    "            # Add 'p' rows of zeros to the beginning of each company's data\n",
    "            X_c = pd.DataFrame(columns=df_data.columns)\n",
    "            X_c = X_c.append([X_company.iloc[0]] * p, ignore_index=True)\n",
    "            X_c = X_c * 0\n",
    "            X_c[\"Period\"] = list(range(-p, 0))\n",
    "            X_c = X_c.append(X_company, ignore_index=True)\n",
    "            df_data = df_data.append(X_c, ignore_index=True)\n",
    "\n",
    "    # Initialize the VAR models using different predictors\n",
    "    # Model 1: column_A\n",
    "    # Model 2: column_A, random noise\n",
    "    # Model 3: column_A, column_B\n",
    "    X1 = df_data[[column_A]].copy()\n",
    "    X2 = df_data[[column_A]].copy()\n",
    "    X2[\"random\"] = np.random.rand(len(X2))\n",
    "    X3 = df_data[[column_A, column_B]].copy()\n",
    "\n",
    "    # Convert dataframes to numpy arrays\n",
    "    X1, X2, X3 = X1.values, X2.values, X3.values\n",
    "\n",
    "    # Fit the VAR models with 'p' lags\n",
    "    mse1, r21, X_lagged1, coeffs_1 = VAR_model(X1, p)\n",
    "    mse2, r22, X_lagged2, coeffs_2 = VAR_model(X2, p)\n",
    "    mse3, r23, X_lagged3, coeffs_3 = VAR_model(X3, p)\n",
    "\n",
    "    # Calculate the residuals for each model\n",
    "    y = df_data[column_A].values[p:]\n",
    "    residuals_1 = y - (X_lagged1 @ coeffs_1)[:, 0]\n",
    "    residuals_2 = y - (X_lagged2 @ coeffs_2)[:, 0]\n",
    "    residuals_3 = y - (X_lagged3 @ coeffs_3)[:, 0]\n",
    "\n",
    "    # Define a function to calculate R-squared from residuals\n",
    "    def r2_from_residuals(y,residuals):\n",
    "        \"\"\"\n",
    "        Calculate the R-squared value from the residuals of a model.\n",
    "\n",
    "        Parameters:\n",
    "        y (numpy.ndarray): The dependent variable.\n",
    "        residuals (numpy.ndarray): The residuals of the model.\n",
    "\n",
    "        Returns:\n",
    "        The R-squared value.\n",
    "        \"\"\"\n",
    "        SST = sum((y - np.mean(y)) ** 2)\n",
    "        SSR = sum(residuals ** 2)\n",
    "        R2 = 1 - (SSR / SST)\n",
    "        return R2\n",
    "\n",
    "    # Print R-squared values for each model\n",
    "    print(f\"R-squared (Model 1): {r2_from_residuals(y, residuals_1)}\")\n",
    "    print(f\"R-squared (Model 2): {r2_from_residuals(y, residuals_2)}\")\n",
    "    print(f\"R-squared (Model 3): {r2_from_residuals(y, residuals_3)}\")\n",
    "\n",
    "    # Perform F-tests to compare residuals between models\n",
    "    F12 = calculate_f_statistic(residuals_1, residuals_2)\n",
    "    F13 = calculate_f_statistic(residuals_1, residuals_3)\n",
    "    print(f\"F-statistic (Model 1 vs Model 2): {F12}\")\n",
    "    print(f\"F-statistic (Model 1 vs Model 3): {F13}\")\n",
    "\n",
    "    critical_value12 = calculate_critical_value(residuals_1, residuals_2, alpha=0.05)\n",
    "    critical_value13 = calculate_critical_value(residuals_1, residuals_3, alpha=0.05)\n",
    "    print(f\"Critical value (Model 1 vs Model 2): {critical_value12}\")\n",
    "    print(f\"Critical value (Model 1 vs Model 3): {critical_value13}\")\n",
    "\n",
    "    # Compare F-statistics to critical values\n",
    "    compare_f_statistic_to_critical_value(F12, critical_value12)\n",
    "    compare_f_statistic_to_critical_value(F13, critical_value13)\n",
    "\n",
    "    # Return the residuals, y, and coefficients for each model\n",
    "    return (residuals_1, residuals_2, residuals_3, y,\n",
    "            coeffs_1, coeffs_2, coeffs_3)\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Granger with industry beta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Granger test based on Multiple Linear Regression (Vanilla /Lasso /Ridge)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def granger_mlr(data, target, predictors, p, companys, alpha_lasso=0.05):\n",
    "    \"\"\"\n",
    "    This function performs Granger Causality tests using multivariate linear regression (MLR) models.\n",
    "    It fits three models:\n",
    "    1. Lasso Regression with the target and the lagged target as predictors\n",
    "    2. Lasso Regression with the target, the lagged target, and the lagged predictors as predictors\n",
    "    \n",
    "    Args:\n",
    "        data (DataFrame): The dataset containing the features and target.\n",
    "        target (str): The target variable.\n",
    "        predictors (list): The list of predictor variables.\n",
    "        p (int): The number of lags to include in the models.\n",
    "        companys (list): The list of companies to use for training, the rest will be used for testing.\n",
    "        alpha_lasso (float): The Lasso regularization parameter. Default is 0.05.\n",
    "        \n",
    "    Returns:\n",
    "        r2_residuals_1 (float): R-squared of the residuals of the first model.\n",
    "        r2_residuals_2 (float): R-squared of the residuals of the second model.\n",
    "        test_true (bool): The result of the F-test comparing the residuals of the two models.\n",
    "        data_shape (tuple): The shape of the processed data.\n",
    "    \"\"\"\n",
    "    # Preprocess data\n",
    "    data = data.copy()\n",
    "    data.dropna(inplace=True)\n",
    "    data = data[np.isfinite(data[target])]\n",
    "    data = data[np.isfinite(data[predictors])]\n",
    "\n",
    "    # Split data into training and testing sets\n",
    "    list_X_col = [target] + predictors + [\"Company Name\"]\n",
    "    X_train = data[~data[\"Company Name\"].isin(companys)][list_X_col].copy()\n",
    "    X_test = data[data[\"Company Name\"].isin(companys)][list_X_col].copy()\n",
    "    Y_train = data[~data[\"Company Name\"].isin(companys)][target].copy()\n",
    "    Y_test = data[data[\"Company Name\"].isin(companys)][target].copy()\n",
    "\n",
    "    # Add lagged predictors and target variables\n",
    "    for i in range(1, p + 1):\n",
    "        lag = -i\n",
    "        for pred in predictors:\n",
    "            col_name = f\"{pred}_shifted_{lag}\"\n",
    "            X_train[col_name] = X_train.groupby(\"Company Name\")[pred].shift(lag)\n",
    "            X_test[col_name] = X_test.groupby(\"Company Name\")[pred].shift(lag)\n",
    "        target_col_name = f\"{target}_shifted{lag}\"\n",
    "        X_train[target_col_name] = X_train.groupby(\"Company Name\")[target].shift(lag)\n",
    "        X_test[target_col_name] = X_test.groupby(\"Company Name\")[target].shift(lag)\n",
    "\n",
    "    # Remove the last p rows for every company and keep only shifted columns\n",
    "    X_train.dropna(inplace=True)\n",
    "    X_test.dropna(inplace=True)\n",
    "    Y_train = Y_train[X_train.index]\n",
    "    Y_test = Y_test[X_test.index]\n",
    "    X_train = X_train[[col for col in X_train.columns if \"shifted\" in col]]\n",
    "    X_test = X_test[[col for col in X_test.columns if \"shifted\" in col]]\n",
    "\n",
    "    # Train first model on the train data\n",
    "    model_1 = sklearn.linear_model.Lasso(alpha=alpha_lasso)\n",
    "    target_columns = [col for col in X_train.columns if str(target) in col]\n",
    "    model_1.fit(X_train[target_columns], Y_train)\n",
    "\n",
    "    # Train second model on the train data\n",
    "    model_2 = sklearn.linear_model.Lasso(alpha=alpha_lasso)\n",
    "    model_2.fit(X_train, Y_train)\n",
    "\n",
    "    # Calculate residuals\n",
    "    residuals_1 = Y_test - model_1.predict(X_test[target_columns])\n",
    "    residuals_2 = Y_test - model_2.predict(X_test)\n",
    "\n",
    "    # Calculate R-squared from residuals\n",
    "    def r2_from_residuals(y, residuals):\n",
    "        SST = sum((y - np.mean(y)) ** 2)  # Sum of Squares Total\n",
    "        SSR = sum(residuals ** 2)  # Sum of Squares Residuals\n",
    "        R2 = 1 - (SSR/SST)  # R-squared\n",
    "        return R2\n",
    "\n",
    "    r2_residuals_1 = r2_from_residuals(Y_test, residuals_1)\n",
    "    r2_residuals_2 = r2_from_residuals(Y_test, residuals_2)\n",
    "\n",
    "    # Perform F Test to check if the residuals are different\n",
    "    F = metrics.f_regression(residuals_1.reshape(-1, 1), residuals_2)[0][0]\n",
    "    critical_value = scipy.stats.f.ppf(1 - 0.05, len(residuals_1) - 1, len(residuals_2) - 1)\n",
    "    test_true = F > critical_value\n",
    "\n",
    "    # Print results\n",
    "    print(f\"F-statistic: {F}\")\n",
    "    print(f\"Critical value: {critical_value}\")\n",
    "    print(f\"Test result: {test_true}\")\n",
    "    print(\"R2 of the models\")\n",
    "    print(f\"R2 of the first model: {model_1.score(X_test[target_columns], Y_test)}\")\n",
    "    print(f\"R2 of the second model: {model_2.score(X_test, Y_test)}\")\n",
    "    print(\"R2 of the residuals\")\n",
    "\n",
    "    # Return R-squared of the residuals and the shape of the processed data\n",
    "    return r2_residuals_1, r2_residuals_2, test_true, data.shape\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analyze"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualize"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ESG by Country"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_esg_map(dataframe, country_col='Country of Headquarters', esg_score_col='ESG Score',download=True):\n",
    "    \"\"\"\n",
    "    Create a map of the world with ESG scores for each country. The ESG scores are standardized to have a mean of 0 and a standard deviation of 1.\n",
    "    input:\n",
    "        dataframe: A dataframe with a column containing the country names and a column containing the ESG scores.\n",
    "        country_col: The name of the column containing the country names.\n",
    "        esg_score_col: The name of the column containing the ESG scores.\n",
    "        download: A boolean indicating whether to download the map as an HTML file or to display it in the notebook.\n",
    "    output:\n",
    "        A map of the world with ESG scores for each country.(HTML file or displayed in the notebook)\n",
    "    \"\"\"\n",
    "    \n",
    "    world = gpd.read_file(gpd.datasets.get_path('naturalearth_lowres'))\n",
    "    merged_data = world.merge(dataframe, left_on='name', right_on=country_col, how='left')\n",
    "    merged_data = merged_data[merged_data[esg_score_col].notna()]\n",
    "\n",
    "    # Standardize the ESG scores\n",
    "    scaler = StandardScaler()\n",
    "    merged_data['Standardized ESG Score'] = scaler.fit_transform(merged_data[[esg_score_col]])\n",
    "\n",
    "    fig = px.choropleth(merged_data, geojson=merged_data.geometry, locations=merged_data.index,\n",
    "                        color='Standardized ESG Score', \n",
    "                        range_color=(merged_data['Standardized ESG Score'].min(), merged_data['Standardized ESG Score'].max()),\n",
    "                        projection='natural earth', hover_name='name', hover_data=[esg_score_col],\n",
    "                        labels={'Standardized ESG Score': 'Standardized ESG Score'})\n",
    "\n",
    "    fig.update_geos(showcountries=True, countrywidth=0.5)\n",
    "    fig.update_layout(title_text='Standardized ESG Score of Companies by Country', title_x=0.5)\n",
    "    if download:\n",
    "        fig.write_html(\"esg_map.html\")\n",
    "    else:\n",
    "        fig.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plot the Clusters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_clusters(df, labels, esg_col='ESG Score', return_col='Return'\n",
    "                  ):\n",
    "    \"\"\"\n",
    "    Plot the clusters of a dataframe based on cluster labels.\n",
    "\n",
    "    Args:\n",
    "        df (pd.DataFrame): The dataframe to plot.\n",
    "        labels (np.ndarray): The cluster labels of each row in the dataframe.\n",
    "    \"\"\"\n",
    "\n",
    "    # Create a scatter plot of \"ESG Score\" vs \"Return\" and use cluster labels to define the colors\n",
    "    sns.scatterplot(x=esg_col, y=return_col, hue=labels, data=df)\n",
    "    plt.legend(bbox_to_anchor=(1.05, 1), loc=2, borderaxespad=0.)\n",
    "    plt.show()\n",
    "\n",
    "    # Print the histogram of the cluster labels\n",
    "    sns.histplot(labels)\n",
    "    plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plot the ESG distribution per Cluster"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TEST CODE"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RUN CODE"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### First, we get the Data and clean it"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get the Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 229/229 [00:05<00:00, 43.52it/s]\n",
      "100%|██████████| 17/17 [00:00<00:00, 41.87it/s]\n",
      "100%|██████████| 2/2 [00:00<00:00, 58.43it/s]\n"
     ]
    }
   ],
   "source": [
    "#let's download the all files we need \n",
    "for key, value in config.DICT_URL.items():\n",
    "    #check if the file is already downloaded\n",
    "    if not os.path.exists(config.LOCAL_FOLDER + key):\n",
    "        download_excel_file(value, config.LOCAL_FOLDER + key)\n",
    "        print(\"Downloaded file: \" + key)\n",
    "    \n",
    "#Scores of the ESG indicators on a company level\n",
    "df_individual_scores_10Y = refinite_to_python(config.LOCAL_FOLDER + \"ESG-individual_scores_10Y.xlsx\")\n",
    "#lets add the ESG Pillar scores\n",
    "df_pillar_scores_10Y = refinite_to_python(config.LOCAL_FOLDER + \"ESG-pillar_scores_10Y.xlsx\")\n",
    "#lets read the logged volatility data to\n",
    "#since it is already prepared, we can just read it\n",
    "df_volatility_10Y_logged = pd.read_excel(config.LOCAL_FOLDER + \"ESG-volatility_10Y_logged.xlsx\")\n",
    "#finally, lets add the EBIDTA data\n",
    "df_EBITDA_10Y = refinite_to_python(config.LOCAL_FOLDER + \"ESG-EBITDA_10Y.xlsx\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Clean it and add clusters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/sklearn/cluster/_kmeans.py:870: FutureWarning: The default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "#first, lets rename the Return column \n",
    "df_individual_scores_10Y.rename(columns={'YTD Total Return':'Return'}, inplace=True)\n",
    "df_individual_scores_10Y_clean = clean_data(df_individual_scores_10Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/sklearn/utils/extmath.py:1047: RuntimeWarning: invalid value encountered in divide\n",
      "  updated_mean = (last_sum + new_sum) / updated_sample_count\n",
      "/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/sklearn/utils/extmath.py:1052: RuntimeWarning: invalid value encountered in divide\n",
      "  T = new_sum / new_sample_count\n",
      "/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/sklearn/utils/extmath.py:1072: RuntimeWarning: invalid value encountered in divide\n",
      "  new_unnormalized_variance -= correction**2 / new_sample_count\n"
     ]
    }
   ],
   "source": [
    "#lets calculate the optimal number of clusters\n",
    "df_individual_scores_10Y_clean_scaled = scale_data(df_individual_scores_10Y_clean)\n",
    "#takes a bit time, if interested, remove the # sign\n",
    "#optimal_n_clusters = find_optimal_clusters(df_individual_scores_10Y_clean_scaled)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets count the Nans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Company Name                                                                   1.000000\n",
       "Period                                                                         1.000000\n",
       "Day Care Services Score                                                        0.250275\n",
       "Highest Remuneration Package Score                                             0.922133\n",
       "Water Recycled Score                                                           0.133554\n",
       "Green Buildings Score                                                          0.388076\n",
       "Policy Water Efficiency Score 1.                                               0.081957\n",
       "Policy Energy Efficiency Score                                                 0.931571\n",
       "Cement Energy Use Score                                                        0.004719\n",
       "Resource Reduction Targets Score                                               1.000000\n",
       "Flexible Working Hours Score                                                   0.535945\n",
       "Average Training Hours Score                                                   0.426931\n",
       "State Owned Enterprise SOE Score                                               0.904043\n",
       "Gender Pay Gap Percentage Score                                                0.138745\n",
       "Lost Days To Total Days Score                                                  0.292119\n",
       "Audit Committee Independence Score                                             0.994966\n",
       "Hybrid Vehicles Score                                                          0.033349\n",
       "CSR Sustainability Committee Score                                             0.858109\n",
       "Litigation Expenses Score                                                      0.164228\n",
       "Agrochemical Products Score                                                    0.003775\n",
       "Corporate Responsibility Awards Score                                          0.593991\n",
       "Women Managers Score                                                           0.718578\n",
       "Internal Audit Department Reporting Score                                      0.937235\n",
       "Environmental Products Score                                                   0.641498\n",
       "Improvement Tools Business Ethics Score                                        0.834828\n",
       "UNPRI Signatory Score                                                          0.012585\n",
       "SelfReported Environmental Fines Score                                         0.264748\n",
       "External Consultants Score                                                     0.907032\n",
       "Policy Forced Labor Score                                                      0.558440\n",
       "Product Access Low Price Score                                                 0.109171\n",
       "Staff Transportation Impact Reduction Score                                    0.333805\n",
       "Land Environmental Impact Reduction Score                                      0.125846\n",
       "Anti Takeover Devices Above Two Score                                          1.000000\n",
       "Policy Data Privacy Score                                                      0.818940\n",
       "Product Quality Controversies Score                                            1.000000\n",
       "Environment Management Team Score                                              0.743590\n",
       "Bribery, Corruption and Fraud Controversies Score                              1.000000\n",
       "Board Gender Diversity, Percent Score                                          0.997955\n",
       "Health & Safety Policy Score                                                   0.971999\n",
       "Human Rights Policy Score                                                      1.000000\n",
       "Employee Satisfaction Score                                                    0.221488\n",
       "Total Donations To Revenues USD Score                                          0.623250\n",
       "NOx and SOx Emissions Reduction Score                                          0.201353\n",
       "Sustainability Compensation Incentives Score                                   0.438257\n",
       "Total Hazardous Waste To Revenues USD Score                                    0.456505\n",
       "Policy Sustainable Packaging Score                                             0.266793\n",
       "Env R&D Expenditures To Revenues USD Score                                     0.033192\n",
       "Board Background and Skills Score                                              0.973258\n",
       "Community Lending and Investments Score                                        0.020607\n",
       "Policy Water Efficiency Score 8.                                               0.067957\n",
       "Policy Customer Health & Safety Score                                          0.459651\n",
       "Environmental Controversies Score                                              0.965235\n",
       "Nomination Committee Independence Score                                        0.879188\n",
       "Board Size More Ten Less Eight Score                                           1.000000\n",
       "Integrated Strategy in MD&A Score                                              0.373447\n",
       "CO2 Equivalent Emissions Indirect, Scope 3 To Revenues USD in million Score    0.614126\n",
       "Environmental Partnerships Score                                               0.706465\n",
       "Trade Union Representation Score                                               0.461696\n",
       "Real Estate Sustainability Certifications Score                                0.068743\n",
       "Training and Development Policy Score                                          0.967752\n",
       "Animal Testing Score                                                           0.098631\n",
       "Customer Satisfaction Score                                                    0.121126\n",
       "Product Sales at Discount to Emerging Markets Score                            0.015259\n",
       "Environmental Restoration Initiatives Score                                    0.316816\n",
       "ESG Reporting Scope                                                            1.000000\n",
       "Drug Delay Score                                                               0.003146\n",
       "Training Costs Per Employee Score                                              0.189240\n",
       "Policy Bribery and Corruption Score                                            0.915369\n",
       "Employees Health & Safety Team Score                                           0.774579\n",
       "Policy Human Rights Score                                                      0.666195\n",
       "Environmental Expenditures Investments Score                                   0.440617\n",
       "Environmental Supply Chain Management Score                                    0.807456\n",
       "Resource Reduction Policy Score                                                1.000000\n",
       "Flaring Gases To Revenues USD in million Score                                 0.022338\n",
       "OECD Guidelines for Multinational Enterprises Score                            0.132452\n",
       "Accounting Controversies Score                                                 0.991663\n",
       "ESG Score                                                                      1.000000\n",
       "Shareholders Vote on Executive Pay Score                                       0.841592\n",
       "Waste Recycled To Total Waste Score                                            0.563473\n",
       "Nuclear Production Score                                                       0.030203\n",
       "Quality Mgt Systems Score                                                      0.502124\n",
       "Turnover of Employees Score                                                    0.555136\n",
       "Consumer Complaints Controversies Score                                        0.975303\n",
       "Environmental Supply Chain Monitoring Score                                    0.554035\n",
       "Policy Water Efficiency Score 5.                                               0.072047\n",
       "Policy Water Efficiency Score                                                  0.736196\n",
       "Fundamental Human Rights ILO UN Score                                          0.567878\n",
       "Occupational Diseases Score                                                    0.141104\n",
       "Injuries To Million Hours Score                                                0.592732\n",
       "Board Structure Policy Score                                                   0.991977\n",
       "Employee Health & Safety Training Hours Score                                  0.066226\n",
       "OzoneDepleting Substances To Revenues USD in million Score                     0.056630\n",
       "VOC Emissions To Revenues USD in million Score                                 0.159667\n",
       "Policy Business Ethics Score                                                   0.912537\n",
       "Total Energy Use To Revenues USD Score                                         0.793928\n",
       "Board Cultural Diversity, Percent Score                                        0.535001\n",
       "Policy Water Efficiency Score 2.                                               0.079283\n",
       "Water Use To Revenues USD Score                                                0.720466\n",
       "GMO Products Score                                                             0.098631\n",
       "Anticompetition Controversies Score                                            1.000000\n",
       "Embryonic Stem Cell Research Score                                             0.098631\n",
       "Board Member Compensation Score                                                0.964763\n",
       "Sustainable Building Products Score                                            0.054428\n",
       "Stakeholder Engagement Score                                                   0.745635\n",
       "Toxic Chemicals Reduction Score                                                0.176656\n",
       "Water Technologies Score                                                       0.048293\n",
       "Policy Water Efficiency Score 0.                                               0.081485\n",
       "Director Election Majority Requirement Score                                   0.845682\n",
       "Internal Promotion Score                                                       0.516439\n",
       "CSR Sustainability External Audit Score                                        0.563473\n",
       "Policy Environmental Supply Chain Score                                        0.784961\n",
       "CSR Sustainability Report Global Activities Score                              0.999371\n",
       "GRI Report Guidelines Score                                                    0.682397\n",
       "SelfReported Environmental Fines To Revenues in million Score                  0.264748\n",
       "Executive Compensation LT Objectives Score                                     0.086991\n",
       "Audit Committee Mgt Independence Score                                         0.903885\n",
       "Whistleblower Protection Score                                                 0.801164\n",
       "CEO Compensation Link to TSR Score                                             0.731949\n",
       "Product Responsibility Monitoring Score                                        0.219758\n",
       "Shareholders Approval Stock Compensation Plan Score                            0.683970\n",
       "Noise Reduction Score                                                          0.053170\n",
       "Policy Water Efficiency Score 3.                                               0.077080\n",
       "CSR Sustainability Reporting Score                                             0.999213\n",
       "Total Renewable Energy To Energy Use in million Score                          0.298883\n",
       "Policy Community Involvement Score                                             0.974202\n",
       "Healthy Food or Products Score                                                 0.109958\n",
       "Employee Resource Groups Score                                                 0.070316\n",
       "Env Supply Chain Partnership Termination Score                                 0.368413\n",
       "Compensation Committee Mgt Independence Score                                  0.876829\n",
       "Voting Cap Percentage Score                                                    0.999371\n",
       "Policy Freedom of Association Score                                            0.367941\n",
       "Auditor Tenure Score                                                           1.000000\n",
       "Diseases of the Developing World Score                                         0.015259\n",
       "Return                                                                         1.000000\n",
       "Executive Individual Compensation Score                                        0.021866\n",
       "Global Compact Signatory Score                                                 0.401919\n",
       "Renewable Energy Use Ratio Score                                               0.309108\n",
       "Board Member Affiliations Score                                                0.997955\n",
       "Renewable Energy Supply Score                                                  0.035079\n",
       "Executives Cultural Diversity Score                                            0.123801\n",
       "Independent Board Members Score                                                0.997798\n",
       "EMS Certified Percent Score                                                    0.291332\n",
       "Water Pollutant Emissions To Revenues USD Score                                0.100519\n",
       "Organic Products Initiatives Score                                             0.063395\n",
       "Public Availability Corporate Statutes Score                                   0.910178\n",
       "Wages Working Condition Controversies Score                                    0.960516\n",
       "Veto Power or Golden share Score                                               0.888784\n",
       "Human Rights Breaches Contractor Score                                         0.399560\n",
       "Net Employment Creation Score                                                  0.962876\n",
       "Executive Members Gender Diversity, Percent Score                              0.997955\n",
       "Targets Emissions Score                                                        0.716061\n",
       "Total Waste To Revenues USD Score                                              0.651251\n",
       "Employees With Disabilities Score                                              0.117823\n",
       "Audit Committee Expertise Score                                                1.000000\n",
       "Environmental Assets Under Mgt Score                                           0.055372\n",
       "Nonaudit to Audit Fees Ratio Score                                             0.856851\n",
       "Fossil Fuel Divestment Policy Score                                            0.009596\n",
       "Board Attendance Score                                                         0.419380\n",
       "Estimated CO2 Equivalents Emission Total Score                                 0.080698\n",
       "Labeled Wood Percentage Score                                                  0.014630\n",
       "Renewable/Clean Energy Products Score                                          0.229196\n",
       "Strikes Score                                                                  0.100834\n",
       "Human Rights Contractor Score                                                  0.749253\n",
       "Policy Fair Trade Score                                                        0.010697\n",
       "Flaring Gases Score                                                            0.022338\n",
       "Cement CO2 Equivalents Emission Score                                          0.005034\n",
       "Supply Chain Health & Safety Improvements Score                                0.005663\n",
       "Succession Plan Score                                                          0.912380\n",
       "Equal Shareholder Rights Score                                                 0.868491\n",
       "Retailing Responsibility Score                                                 0.098631\n",
       "Emissions Trading Score                                                        0.181847\n",
       "Board Functions Policy Score                                                   0.411515\n",
       "Fleet CO2 Emissions Score                                                      0.008180\n",
       "Supplier ESG training Score                                                    0.327828\n",
       "Biodiversity Impact Reduction Score                                            0.409942\n",
       "NonExecutive Board Members Score                                               0.997955\n",
       "Board Individual Reelection Score                                              0.726758\n",
       "Policy Child Labor Score                                                       0.568193\n",
       "Executive Compensation Policy Score                                            0.988674\n",
       "SOx Emissions To Revenues USD in million Score                                 0.287400\n",
       "Executive Compensation Controversies Score                                     0.991820\n",
       "Targets Energy Efficiency Score                                                0.452415\n",
       "Targets Water Efficiency Score                                                 0.331918\n",
       "Environmental Materials Sourcing Score                                         0.618845\n",
       "Fleet Fuel Consumption Score                                                   0.005663\n",
       "Ethical Trading Initiative ETI Score                                           0.002674\n",
       "Compensation Committee Independence Score                                      0.933616\n",
       "Nomination Committee Mgt Independence Score                                    1.000000\n",
       "Policy Fair Competition Score                                                  0.764040\n",
       "Board Meeting Attendance Average Score                                         0.910178\n",
       "NAICS Subsector Name_Unnamed: 5_level_                                         0.099733\n",
       "Strictly Independent Board Members Score                                       0.624980\n",
       "Litigation Expenses To Revenues in million Score                               0.164228\n",
       "HRC Corporate Equality Index Score                                             0.257354\n",
       "Policy Water Efficiency Score 7.                                               0.069215\n",
       "Responsible Marketing Controversies Score                                      0.978449\n",
       "Average Board Tenure Score                                                     0.992607\n",
       "OzoneDepleting Substances Score                                                0.056316\n",
       "Equator Principles or Env Project Financing Score                              0.064810\n",
       "CEOChairman Separation Score                                                   0.660846\n",
       "HIVAIDS Program Score                                                          0.094227\n",
       "Employees Health & Safety OHSAS 18001 Score                                    0.639767\n",
       "Product Impact Minimization Score                                              0.243826\n",
       "NOx Emissions To Revenues USD in million Score                                 0.300928\n",
       "Policy Water Efficiency Score 9.                                               0.067327\n",
       "Policy Water Efficiency Score 4.                                               0.074249\n",
       "Announced Layoffs To Total Employees Score                                     1.000000\n",
       "Critical Country 1 Score                                                       0.000000\n",
       "Policy Responsible Marketing Score                                             0.065282\n",
       "Total Senior Executives Compensation To Revenues in million Score              0.970741\n",
       "Women Employees Score                                                          0.870379\n",
       "Nomination Committee Involvement Score                                         0.758849\n",
       "Policy Emissions Score                                                         0.924178\n",
       "Accidental Spills To Revenues USD in million Score                             0.051597\n",
       "Revenue from Environmental Products Score                                      0.001258\n",
       "Internal Carbon Price per Tonne Score Score                                    0.014472\n",
       "Policy Diversity and Opportunity Score                                         0.956111\n",
       "Targets Diversity and Opportunity Score                                        0.316973\n",
       "Salary Gap Score                                                               0.655341\n",
       "QMS Certified Percent Score                                                    0.111059\n",
       "Total CO2 Equivalent Emissions To Revenues USD Score                           0.939751\n",
       "Total Senior Executives Compensation Score                                     0.970741\n",
       "eWaste Reduction Score                                                         0.269624\n",
       "Board Specific Skills, Percent Score                                           0.997955\n",
       "VOC or Particulate Matter Emissions Reduction Score                            0.198049\n",
       "Extractive Industries Transparency Initiative Score                            0.044832\n",
       "Policy Water Efficiency Score 6.                                               0.071575\n",
       "Insider Dealings Controversies Score                                           0.995281\n",
       "Climate Change Commercial Risks Opportunities Score                            0.751612\n",
       "Compensation Improvement Tools Score                                           0.856851\n",
       "Shareholder Rights Policy Score                                                0.976561\n",
       "dtype: float64"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#percentage of non-null values in each column\n",
    "pd.set_option('display.max_rows', None)\n",
    "display(1 - df_individual_scores_10Y_clean.isna().sum()/len(df_individual_scores_10Y_clean))\n",
    "pd.reset_option('display.max_rows', 15)\n",
    "#we leave the Nans so we can distinguish between the companies that have scores and the ones that don't"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets add the transformed values as returns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/pandas/core/arraylike.py:402: RuntimeWarning: invalid value encountered in log\n",
      "  result = getattr(ufunc, method)(*inputs, **kwargs)\n",
      "/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/pandas/core/arraylike.py:402: RuntimeWarning: invalid value encountered in sqrt\n",
      "  result = getattr(ufunc, method)(*inputs, **kwargs)\n"
     ]
    }
   ],
   "source": [
    "df_individual_scores_10Y_clean_transformed = transform_columns(df_individual_scores_10Y_clean,\"Return\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Now we start with some basic data exploration and visualization "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> ESG by country"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'Country of Headquarters'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[24], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m create_esg_map(df_individual_scores_10Y_clean)\n",
      "Cell \u001b[0;32mIn[17], line 15\u001b[0m, in \u001b[0;36mcreate_esg_map\u001b[0;34m(dataframe, country_col, esg_score_col, download)\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[39m\u001b[39m\u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m      4\u001b[0m \u001b[39mCreate a map of the world with ESG scores for each country. The ESG scores are standardized to have a mean of 0 and a standard deviation of 1.\u001b[39;00m\n\u001b[1;32m      5\u001b[0m \u001b[39minput:\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[39m    A map of the world with ESG scores for each country.(HTML file or displayed in the notebook)\u001b[39;00m\n\u001b[1;32m     12\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m     14\u001b[0m world \u001b[39m=\u001b[39m gpd\u001b[39m.\u001b[39mread_file(gpd\u001b[39m.\u001b[39mdatasets\u001b[39m.\u001b[39mget_path(\u001b[39m'\u001b[39m\u001b[39mnaturalearth_lowres\u001b[39m\u001b[39m'\u001b[39m))\n\u001b[0;32m---> 15\u001b[0m merged_data \u001b[39m=\u001b[39m world\u001b[39m.\u001b[39;49mmerge(dataframe, left_on\u001b[39m=\u001b[39;49m\u001b[39m'\u001b[39;49m\u001b[39mname\u001b[39;49m\u001b[39m'\u001b[39;49m, right_on\u001b[39m=\u001b[39;49mcountry_col, how\u001b[39m=\u001b[39;49m\u001b[39m'\u001b[39;49m\u001b[39mleft\u001b[39;49m\u001b[39m'\u001b[39;49m)\n\u001b[1;32m     16\u001b[0m merged_data \u001b[39m=\u001b[39m merged_data[merged_data[esg_score_col]\u001b[39m.\u001b[39mnotna()]\n\u001b[1;32m     18\u001b[0m \u001b[39m# Standardize the ESG scores\u001b[39;00m\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/geopandas/geodataframe.py:1470\u001b[0m, in \u001b[0;36mGeoDataFrame.merge\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1449\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mmerge\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs):\n\u001b[1;32m   1450\u001b[0m \u001b[39m    \u001b[39m\u001b[39mr\u001b[39m\u001b[39m\"\"\"Merge two ``GeoDataFrame`` objects with a database-style join.\u001b[39;00m\n\u001b[1;32m   1451\u001b[0m \n\u001b[1;32m   1452\u001b[0m \u001b[39m    Returns a ``GeoDataFrame`` if a geometry column is present; otherwise,\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1468\u001b[0m \n\u001b[1;32m   1469\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[0;32m-> 1470\u001b[0m     result \u001b[39m=\u001b[39m DataFrame\u001b[39m.\u001b[39;49mmerge(\u001b[39mself\u001b[39;49m, \u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1471\u001b[0m     geo_col \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_geometry_column_name\n\u001b[1;32m   1472\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(result, DataFrame) \u001b[39mand\u001b[39;00m geo_col \u001b[39min\u001b[39;00m result:\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/pandas/core/frame.py:10090\u001b[0m, in \u001b[0;36mDataFrame.merge\u001b[0;34m(self, right, how, on, left_on, right_on, left_index, right_index, sort, suffixes, copy, indicator, validate)\u001b[0m\n\u001b[1;32m  10071\u001b[0m \u001b[39m@Substitution\u001b[39m(\u001b[39m\"\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m  10072\u001b[0m \u001b[39m@Appender\u001b[39m(_merge_doc, indents\u001b[39m=\u001b[39m\u001b[39m2\u001b[39m)\n\u001b[1;32m  10073\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mmerge\u001b[39m(\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m  10086\u001b[0m     validate: \u001b[39mstr\u001b[39m \u001b[39m|\u001b[39m \u001b[39mNone\u001b[39;00m \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m,\n\u001b[1;32m  10087\u001b[0m ) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m DataFrame:\n\u001b[1;32m  10088\u001b[0m     \u001b[39mfrom\u001b[39;00m \u001b[39mpandas\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mcore\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mreshape\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mmerge\u001b[39;00m \u001b[39mimport\u001b[39;00m merge\n\u001b[0;32m> 10090\u001b[0m     \u001b[39mreturn\u001b[39;00m merge(\n\u001b[1;32m  10091\u001b[0m         \u001b[39mself\u001b[39;49m,\n\u001b[1;32m  10092\u001b[0m         right,\n\u001b[1;32m  10093\u001b[0m         how\u001b[39m=\u001b[39;49mhow,\n\u001b[1;32m  10094\u001b[0m         on\u001b[39m=\u001b[39;49mon,\n\u001b[1;32m  10095\u001b[0m         left_on\u001b[39m=\u001b[39;49mleft_on,\n\u001b[1;32m  10096\u001b[0m         right_on\u001b[39m=\u001b[39;49mright_on,\n\u001b[1;32m  10097\u001b[0m         left_index\u001b[39m=\u001b[39;49mleft_index,\n\u001b[1;32m  10098\u001b[0m         right_index\u001b[39m=\u001b[39;49mright_index,\n\u001b[1;32m  10099\u001b[0m         sort\u001b[39m=\u001b[39;49msort,\n\u001b[1;32m  10100\u001b[0m         suffixes\u001b[39m=\u001b[39;49msuffixes,\n\u001b[1;32m  10101\u001b[0m         copy\u001b[39m=\u001b[39;49mcopy,\n\u001b[1;32m  10102\u001b[0m         indicator\u001b[39m=\u001b[39;49mindicator,\n\u001b[1;32m  10103\u001b[0m         validate\u001b[39m=\u001b[39;49mvalidate,\n\u001b[1;32m  10104\u001b[0m     )\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/pandas/core/reshape/merge.py:110\u001b[0m, in \u001b[0;36mmerge\u001b[0;34m(left, right, how, on, left_on, right_on, left_index, right_index, sort, suffixes, copy, indicator, validate)\u001b[0m\n\u001b[1;32m     93\u001b[0m \u001b[39m@Substitution\u001b[39m(\u001b[39m\"\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39mleft : DataFrame or named Series\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m     94\u001b[0m \u001b[39m@Appender\u001b[39m(_merge_doc, indents\u001b[39m=\u001b[39m\u001b[39m0\u001b[39m)\n\u001b[1;32m     95\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mmerge\u001b[39m(\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    108\u001b[0m     validate: \u001b[39mstr\u001b[39m \u001b[39m|\u001b[39m \u001b[39mNone\u001b[39;00m \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m,\n\u001b[1;32m    109\u001b[0m ) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m DataFrame:\n\u001b[0;32m--> 110\u001b[0m     op \u001b[39m=\u001b[39m _MergeOperation(\n\u001b[1;32m    111\u001b[0m         left,\n\u001b[1;32m    112\u001b[0m         right,\n\u001b[1;32m    113\u001b[0m         how\u001b[39m=\u001b[39;49mhow,\n\u001b[1;32m    114\u001b[0m         on\u001b[39m=\u001b[39;49mon,\n\u001b[1;32m    115\u001b[0m         left_on\u001b[39m=\u001b[39;49mleft_on,\n\u001b[1;32m    116\u001b[0m         right_on\u001b[39m=\u001b[39;49mright_on,\n\u001b[1;32m    117\u001b[0m         left_index\u001b[39m=\u001b[39;49mleft_index,\n\u001b[1;32m    118\u001b[0m         right_index\u001b[39m=\u001b[39;49mright_index,\n\u001b[1;32m    119\u001b[0m         sort\u001b[39m=\u001b[39;49msort,\n\u001b[1;32m    120\u001b[0m         suffixes\u001b[39m=\u001b[39;49msuffixes,\n\u001b[1;32m    121\u001b[0m         indicator\u001b[39m=\u001b[39;49mindicator,\n\u001b[1;32m    122\u001b[0m         validate\u001b[39m=\u001b[39;49mvalidate,\n\u001b[1;32m    123\u001b[0m     )\n\u001b[1;32m    124\u001b[0m     \u001b[39mreturn\u001b[39;00m op\u001b[39m.\u001b[39mget_result(copy\u001b[39m=\u001b[39mcopy)\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/pandas/core/reshape/merge.py:703\u001b[0m, in \u001b[0;36m_MergeOperation.__init__\u001b[0;34m(self, left, right, how, on, left_on, right_on, axis, left_index, right_index, sort, suffixes, indicator, validate)\u001b[0m\n\u001b[1;32m    696\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_cross \u001b[39m=\u001b[39m cross_col\n\u001b[1;32m    698\u001b[0m \u001b[39m# note this function has side effects\u001b[39;00m\n\u001b[1;32m    699\u001b[0m (\n\u001b[1;32m    700\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mleft_join_keys,\n\u001b[1;32m    701\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mright_join_keys,\n\u001b[1;32m    702\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mjoin_names,\n\u001b[0;32m--> 703\u001b[0m ) \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_get_merge_keys()\n\u001b[1;32m    705\u001b[0m \u001b[39m# validate the merge keys dtypes. We may need to coerce\u001b[39;00m\n\u001b[1;32m    706\u001b[0m \u001b[39m# to avoid incompatible dtypes\u001b[39;00m\n\u001b[1;32m    707\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_maybe_coerce_merge_keys()\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/pandas/core/reshape/merge.py:1162\u001b[0m, in \u001b[0;36m_MergeOperation._get_merge_keys\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1160\u001b[0m rk \u001b[39m=\u001b[39m cast(Hashable, rk)\n\u001b[1;32m   1161\u001b[0m \u001b[39mif\u001b[39;00m rk \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m-> 1162\u001b[0m     right_keys\u001b[39m.\u001b[39mappend(right\u001b[39m.\u001b[39;49m_get_label_or_level_values(rk))\n\u001b[1;32m   1163\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m   1164\u001b[0m     \u001b[39m# work-around for merge_asof(right_index=True)\u001b[39;00m\n\u001b[1;32m   1165\u001b[0m     right_keys\u001b[39m.\u001b[39mappend(right\u001b[39m.\u001b[39mindex)\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/pandas/core/generic.py:1850\u001b[0m, in \u001b[0;36mNDFrame._get_label_or_level_values\u001b[0;34m(self, key, axis)\u001b[0m\n\u001b[1;32m   1844\u001b[0m     values \u001b[39m=\u001b[39m (\n\u001b[1;32m   1845\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39maxes[axis]\n\u001b[1;32m   1846\u001b[0m         \u001b[39m.\u001b[39mget_level_values(key)  \u001b[39m# type: ignore[assignment]\u001b[39;00m\n\u001b[1;32m   1847\u001b[0m         \u001b[39m.\u001b[39m_values\n\u001b[1;32m   1848\u001b[0m     )\n\u001b[1;32m   1849\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m-> 1850\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mKeyError\u001b[39;00m(key)\n\u001b[1;32m   1852\u001b[0m \u001b[39m# Check for duplicates\u001b[39;00m\n\u001b[1;32m   1853\u001b[0m \u001b[39mif\u001b[39;00m values\u001b[39m.\u001b[39mndim \u001b[39m>\u001b[39m \u001b[39m1\u001b[39m:\n",
      "\u001b[0;31mKeyError\u001b[0m: 'Country of Headquarters'"
     ]
    }
   ],
   "source": [
    "create_esg_map(df_individual_scores_10Y_clean)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ESG by Cluster"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Correlation"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets create a big Correlation matrix of all pill"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
